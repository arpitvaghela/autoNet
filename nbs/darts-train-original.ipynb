{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/arpitvaghela/autoNet.git#subdirectory=autonet-kafka/worker/darts","metadata":{"execution":{"iopub.status.busy":"2021-11-23T20:00:16.347471Z","iopub.execute_input":"2021-11-23T20:00:16.347790Z","iopub.status.idle":"2021-11-23T20:00:31.647985Z","shell.execute_reply.started":"2021-11-23T20:00:16.347707Z","shell.execute_reply":"2021-11-23T20:00:31.647136Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/arpitvaghela/autoNet.git#subdirectory=autonet-kafka/worker/darts\n  Cloning https://github.com/arpitvaghela/autoNet.git to /tmp/pip-req-build-0jgwagsh\n  Running command git clone -q https://github.com/arpitvaghela/autoNet.git /tmp/pip-req-build-0jgwagsh\n  Resolved https://github.com/arpitvaghela/autoNet.git to commit a1cbbeb966693067502ef94263e16baaf8309d82\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (1.19.5)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (0.8.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (1.9.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (0.10.1)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (2.6.0)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (2.2)\nCollecting aiokafka\n  Downloading aiokafka-0.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[K     |████████████████████████████████| 1.1 MB 522 kB/s eta 0:00:01     |█████████████████████████▍      | 860 kB 522 kB/s eta 0:00:01\n\u001b[?25hCollecting loguru\n  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n\u001b[K     |████████████████████████████████| 57 kB 3.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (6.2.5)\nRequirement already satisfied: pytest-cov in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (3.0.0)\nCollecting pytest-asyncio\n  Downloading pytest_asyncio-0.16.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (0.15.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (2.25.1)\nCollecting asyncio\n  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n\u001b[K     |████████████████████████████████| 101 kB 7.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: starlette in /opt/conda/lib/python3.7/site-packages (from darts==0.0.0) (0.16.0)\nCollecting kafka-logging-handler\n  Downloading kafka_logging_handler-0.2.5-py3-none-any.whl (18 kB)\nCollecting kafka-python>=2.0.0\n  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n\u001b[K     |████████████████████████████████| 246 kB 17.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from kafka-logging-handler->darts==0.0.0) (5.8.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (1.1.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (21.2.0)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (1.10.0)\nRequirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (4.8.1)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (1.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (21.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from pytest->darts==0.0.0) (0.10.2)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->darts==0.0.0) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->darts==0.0.0) (3.5.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest->darts==0.0.0) (2.4.7)\nRequirement already satisfied: coverage[toml]>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pytest-cov->darts==0.0.0) (6.0.2)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.7/site-packages (from coverage[toml]>=5.2.1->pytest-cov->darts==0.0.0) (1.2.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->darts==0.0.0) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->darts==0.0.0) (1.26.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->darts==0.0.0) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->darts==0.0.0) (2.10)\nRequirement already satisfied: anyio<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from starlette->darts==0.0.0) (3.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.0.0->starlette->darts==0.0.0) (1.2.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (1.35.0)\nRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (3.19.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (0.4.6)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (2.0.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (0.6.1)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (1.38.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (1.8.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (0.37.0)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (0.14.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (58.0.4)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->darts==0.0.0) (3.3.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard->darts==0.0.0) (1.16.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->darts==0.0.0) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->darts==0.0.0) (4.7.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->darts==0.0.0) (4.2.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->darts==0.0.0) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->darts==0.0.0) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->darts==0.0.0) (3.1.1)\nRequirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->darts==0.0.0) (8.2.0)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn->darts==0.0.0) (0.12.0)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from uvicorn->darts==0.0.0) (8.0.1)\nRequirement already satisfied: asgiref>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from uvicorn->darts==0.0.0) (3.4.1)\nBuilding wheels for collected packages: darts\n  Building wheel for darts (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for darts: filename=darts-0.0.0-py3-none-any.whl size=22078 sha256=b492926cd6619be507c79e18285fe65860d7072db6887e5e82505e5e85261205\n  Stored in directory: /tmp/pip-ephem-wheel-cache-707vwl96/wheels/c1/f8/20/181569b8633d001ec38dc063aab2cb13108b95e2816d3c5555\nSuccessfully built darts\nInstalling collected packages: kafka-python, pytest-asyncio, loguru, kafka-logging-handler, asyncio, aiokafka, darts\nSuccessfully installed aiokafka-0.7.2 asyncio-3.4.3 darts-0.0.0 kafka-logging-handler-0.2.5 kafka-python-2.0.2 loguru-0.5.3 pytest-asyncio-0.16.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import darts\nfrom darts import search","metadata":{"execution":{"iopub.status.busy":"2021-11-23T20:00:31.650148Z","iopub.execute_input":"2021-11-23T20:00:31.650460Z","iopub.status.idle":"2021-11-23T20:00:33.614362Z","shell.execute_reply.started":"2021-11-23T20:00:31.650421Z","shell.execute_reply":"2021-11-23T20:00:33.613542Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"search.search(\"cifar4_darts_org\", dataset=\"cifar4\")","metadata":{"execution":{"iopub.status.busy":"2021-11-23T20:00:33.615970Z","iopub.execute_input":"2021-11-23T20:00:33.616263Z","iopub.status.idle":"2021-11-24T00:51:52.346374Z","shell.execute_reply.started":"2021-11-23T20:00:33.616216Z","shell.execute_reply":"2021-11-24T00:51:52.345584Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"11/23 08:00:33 PM | Logger is set - training start\n","output_type":"stream"},{"name":"stdout","text":"\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170498071 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cdfd0b7a484cde881fafe125294618"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data/\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1111, 0.1110, 0.1111, 0.1111, 0.1111, 0.1109, 0.1113, 0.1113, 0.1111],\n        [0.1111, 0.1113, 0.1111, 0.1110, 0.1109, 0.1112, 0.1109, 0.1113, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1113, 0.1111, 0.1111, 0.1110, 0.1112, 0.1112, 0.1110, 0.1111, 0.1110],\n        [0.1109, 0.1111, 0.1113, 0.1110, 0.1112, 0.1111, 0.1111, 0.1112, 0.1111],\n        [0.1113, 0.1111, 0.1109, 0.1112, 0.1109, 0.1112, 0.1111, 0.1111, 0.1110]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1111, 0.1111, 0.1112, 0.1110, 0.1112, 0.1111, 0.1112, 0.1112, 0.1109],\n        [0.1112, 0.1112, 0.1111, 0.1112, 0.1109, 0.1111, 0.1111, 0.1111, 0.1110],\n        [0.1110, 0.1111, 0.1113, 0.1111, 0.1112, 0.1110, 0.1111, 0.1110, 0.1111],\n        [0.1110, 0.1112, 0.1113, 0.1110, 0.1111, 0.1112, 0.1111, 0.1111, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1111, 0.1112, 0.1111, 0.1111, 0.1110, 0.1111, 0.1111, 0.1110, 0.1112],\n        [0.1112, 0.1112, 0.1111, 0.1110, 0.1112, 0.1110, 0.1111, 0.1109, 0.1112],\n        [0.1110, 0.1112, 0.1110, 0.1112, 0.1112, 0.1110, 0.1110, 0.1113, 0.1111],\n        [0.1110, 0.1111, 0.1113, 0.1112, 0.1111, 0.1112, 0.1109, 0.1111, 0.1109],\n        [0.1110, 0.1110, 0.1111, 0.1111, 0.1111, 0.1113, 0.1111, 0.1112, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1112, 0.1109, 0.1112, 0.1111, 0.1111, 0.1112, 0.1112, 0.1110, 0.1111],\n        [0.1111, 0.1110, 0.1110, 0.1111, 0.1113, 0.1113, 0.1110, 0.1112, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1109, 0.1111, 0.1112, 0.1112, 0.1111, 0.1110, 0.1112, 0.1112, 0.1111],\n        [0.1109, 0.1112, 0.1111, 0.1112, 0.1110, 0.1112, 0.1113, 0.1111, 0.1111],\n        [0.1111, 0.1112, 0.1112, 0.1112, 0.1111, 0.1110, 0.1111, 0.1110, 0.1112]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1109, 0.1110, 0.1112, 0.1112, 0.1111, 0.1112, 0.1112, 0.1111, 0.1111],\n        [0.1111, 0.1110, 0.1112, 0.1111, 0.1111, 0.1111, 0.1113, 0.1111, 0.1111],\n        [0.1111, 0.1112, 0.1110, 0.1111, 0.1112, 0.1111, 0.1111, 0.1111, 0.1111],\n        [0.1113, 0.1111, 0.1111, 0.1110, 0.1112, 0.1111, 0.1110, 0.1112, 0.1110]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1112, 0.1112, 0.1110, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111],\n        [0.1109, 0.1111, 0.1111, 0.1112, 0.1110, 0.1112, 0.1113, 0.1112, 0.1111],\n        [0.1110, 0.1112, 0.1111, 0.1114, 0.1111, 0.1112, 0.1110, 0.1110, 0.1111],\n        [0.1109, 0.1112, 0.1113, 0.1111, 0.1111, 0.1112, 0.1110, 0.1110, 0.1111],\n        [0.1111, 0.1109, 0.1111, 0.1111, 0.1112, 0.1111, 0.1112, 0.1113, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n11/23 08:01:01 PM | Train: [ 1/50] Step 000/062 Loss 1.503 Prec@(1,5) (14.1%, 70.3%)\n11/23 08:05:25 PM | Train: [ 1/50] Step 050/062 Loss 0.996 Prec@(1,5) (57.8%, 94.2%)\n11/23 08:06:28 PM | Train: [ 1/50] Step 062/062 Loss 0.981 Prec@(1,5) (58.6%, 94.6%)\n11/23 08:06:28 PM | Train: [ 1/50] Final Prec@1 58.6250%\n11/23 08:06:29 PM | Valid: [ 1/50] Step 000/062 Loss 1.036 Prec@(1,5) (57.8%, 98.4%)\n11/23 08:06:42 PM | Valid: [ 1/50] Step 050/062 Loss 1.027 Prec@(1,5) (62.0%, 97.0%)\n11/23 08:06:45 PM | Valid: [ 1/50] Step 062/062 Loss 1.038 Prec@(1,5) (62.1%, 96.6%)\n11/23 08:06:45 PM | Valid: [ 1/50] Final Prec@1 62.1250%\n11/23 08:06:45 PM | genotype = Genotype(normal=[[('dil_conv_5x5', 1), ('dil_conv_5x5', 0)], [('dil_conv_5x5', 1), ('sep_conv_5x5', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 2), ('dil_conv_3x3', 1)], [('sep_conv_5x5', 2), ('sep_conv_5x5', 1)], [('max_pool_3x3', 0), ('sep_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1108, 0.1104, 0.1107, 0.1115, 0.1114, 0.1111, 0.1114, 0.1119, 0.1108],\n        [0.1104, 0.1102, 0.1103, 0.1112, 0.1112, 0.1117, 0.1114, 0.1120, 0.1116]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1116, 0.1107, 0.1110, 0.1110, 0.1116, 0.1109, 0.1113, 0.1108, 0.1112],\n        [0.1106, 0.1103, 0.1105, 0.1112, 0.1111, 0.1117, 0.1113, 0.1118, 0.1115],\n        [0.1112, 0.1106, 0.1108, 0.1115, 0.1111, 0.1111, 0.1112, 0.1111, 0.1114]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1114, 0.1109, 0.1112, 0.1107, 0.1112, 0.1112, 0.1110, 0.1114, 0.1110],\n        [0.1106, 0.1103, 0.1103, 0.1114, 0.1110, 0.1117, 0.1113, 0.1118, 0.1116],\n        [0.1108, 0.1106, 0.1110, 0.1113, 0.1109, 0.1112, 0.1112, 0.1117, 0.1113],\n        [0.1107, 0.1108, 0.1111, 0.1108, 0.1108, 0.1113, 0.1112, 0.1114, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1102, 0.1100, 0.1101, 0.1108, 0.1117, 0.1118, 0.1113, 0.1121, 0.1120],\n        [0.1104, 0.1101, 0.1103, 0.1113, 0.1114, 0.1116, 0.1113, 0.1117, 0.1118],\n        [0.1099, 0.1100, 0.1102, 0.1118, 0.1119, 0.1119, 0.1110, 0.1112, 0.1122],\n        [0.1099, 0.1098, 0.1101, 0.1118, 0.1115, 0.1119, 0.1110, 0.1118, 0.1123],\n        [0.1097, 0.1096, 0.1097, 0.1118, 0.1116, 0.1120, 0.1118, 0.1115, 0.1122]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1116, 0.1112, 0.1111, 0.1109, 0.1113, 0.1112, 0.1108, 0.1112, 0.1107],\n        [0.1111, 0.1106, 0.1115, 0.1114, 0.1112, 0.1110, 0.1107, 0.1110, 0.1116]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1113, 0.1114, 0.1114, 0.1111, 0.1106, 0.1111, 0.1113, 0.1112, 0.1107],\n        [0.1112, 0.1112, 0.1109, 0.1113, 0.1109, 0.1111, 0.1114, 0.1109, 0.1113],\n        [0.1109, 0.1110, 0.1115, 0.1110, 0.1109, 0.1111, 0.1112, 0.1108, 0.1115]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1115, 0.1113, 0.1113, 0.1108, 0.1109, 0.1108, 0.1112, 0.1114, 0.1109],\n        [0.1111, 0.1107, 0.1109, 0.1114, 0.1115, 0.1107, 0.1114, 0.1111, 0.1111],\n        [0.1108, 0.1108, 0.1106, 0.1107, 0.1119, 0.1111, 0.1117, 0.1113, 0.1111],\n        [0.1109, 0.1108, 0.1111, 0.1113, 0.1112, 0.1111, 0.1106, 0.1115, 0.1114]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1119, 0.1116, 0.1110, 0.1109, 0.1111, 0.1110, 0.1113, 0.1106, 0.1106],\n        [0.1110, 0.1109, 0.1112, 0.1110, 0.1113, 0.1111, 0.1111, 0.1110, 0.1114],\n        [0.1109, 0.1110, 0.1114, 0.1110, 0.1116, 0.1110, 0.1108, 0.1110, 0.1113],\n        [0.1107, 0.1110, 0.1114, 0.1113, 0.1117, 0.1110, 0.1111, 0.1107, 0.1111],\n        [0.1108, 0.1106, 0.1109, 0.1115, 0.1117, 0.1109, 0.1111, 0.1118, 0.1107]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:06:53 PM | Train: [ 2/50] Step 000/062 Loss 0.938 Prec@(1,5) (64.1%, 95.3%)\n11/23 08:11:17 PM | Train: [ 2/50] Step 050/062 Loss 0.821 Prec@(1,5) (67.5%, 96.9%)\n11/23 08:12:19 PM | Train: [ 2/50] Step 062/062 Loss 0.800 Prec@(1,5) (68.5%, 97.0%)\n11/23 08:12:19 PM | Train: [ 2/50] Final Prec@1 68.5250%\n11/23 08:12:20 PM | Valid: [ 2/50] Step 000/062 Loss 0.812 Prec@(1,5) (71.9%, 96.9%)\n11/23 08:12:32 PM | Valid: [ 2/50] Step 050/062 Loss 0.655 Prec@(1,5) (73.4%, 98.6%)\n11/23 08:12:35 PM | Valid: [ 2/50] Step 062/062 Loss 0.659 Prec@(1,5) (73.2%, 98.5%)\n11/23 08:12:36 PM | Valid: [ 2/50] Final Prec@1 73.1500%\n11/23 08:12:36 PM | genotype = Genotype(normal=[[('dil_conv_5x5', 0), ('dil_conv_5x5', 1)], [('max_pool_3x3', 0), ('sep_conv_7x7', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 3)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('skip_connect', 0)], [('sep_conv_5x5', 4), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1106, 0.1100, 0.1105, 0.1117, 0.1118, 0.1110, 0.1113, 0.1121, 0.1111],\n        [0.1105, 0.1100, 0.1103, 0.1115, 0.1113, 0.1118, 0.1114, 0.1118, 0.1115]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1118, 0.1105, 0.1111, 0.1113, 0.1114, 0.1107, 0.1109, 0.1106, 0.1117],\n        [0.1109, 0.1102, 0.1107, 0.1108, 0.1112, 0.1120, 0.1113, 0.1117, 0.1113],\n        [0.1111, 0.1103, 0.1111, 0.1115, 0.1113, 0.1104, 0.1115, 0.1110, 0.1119]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1114, 0.1106, 0.1111, 0.1107, 0.1112, 0.1111, 0.1109, 0.1118, 0.1111],\n        [0.1106, 0.1100, 0.1101, 0.1113, 0.1111, 0.1118, 0.1115, 0.1119, 0.1116],\n        [0.1106, 0.1103, 0.1109, 0.1108, 0.1110, 0.1116, 0.1113, 0.1122, 0.1113],\n        [0.1103, 0.1102, 0.1106, 0.1110, 0.1114, 0.1114, 0.1114, 0.1116, 0.1121]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1096, 0.1094, 0.1095, 0.1107, 0.1121, 0.1117, 0.1113, 0.1131, 0.1126],\n        [0.1102, 0.1099, 0.1101, 0.1110, 0.1114, 0.1119, 0.1116, 0.1117, 0.1121],\n        [0.1091, 0.1094, 0.1099, 0.1117, 0.1121, 0.1123, 0.1115, 0.1111, 0.1130],\n        [0.1090, 0.1089, 0.1093, 0.1122, 0.1117, 0.1125, 0.1114, 0.1120, 0.1130],\n        [0.1087, 0.1087, 0.1089, 0.1123, 0.1119, 0.1124, 0.1121, 0.1122, 0.1128]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1116, 0.1110, 0.1111, 0.1107, 0.1111, 0.1115, 0.1112, 0.1112, 0.1105],\n        [0.1115, 0.1109, 0.1113, 0.1115, 0.1110, 0.1108, 0.1104, 0.1110, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1114, 0.1112, 0.1112, 0.1112, 0.1107, 0.1108, 0.1117, 0.1112, 0.1105],\n        [0.1117, 0.1116, 0.1104, 0.1115, 0.1107, 0.1108, 0.1109, 0.1109, 0.1113],\n        [0.1106, 0.1105, 0.1112, 0.1110, 0.1107, 0.1119, 0.1114, 0.1111, 0.1116]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1116, 0.1113, 0.1117, 0.1107, 0.1111, 0.1108, 0.1107, 0.1111, 0.1110],\n        [0.1116, 0.1110, 0.1104, 0.1115, 0.1111, 0.1108, 0.1112, 0.1112, 0.1112],\n        [0.1103, 0.1104, 0.1101, 0.1107, 0.1124, 0.1113, 0.1119, 0.1120, 0.1111],\n        [0.1108, 0.1107, 0.1111, 0.1114, 0.1113, 0.1111, 0.1105, 0.1113, 0.1116]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1120, 0.1115, 0.1110, 0.1105, 0.1111, 0.1111, 0.1114, 0.1108, 0.1106],\n        [0.1113, 0.1112, 0.1111, 0.1109, 0.1113, 0.1112, 0.1112, 0.1104, 0.1113],\n        [0.1109, 0.1110, 0.1114, 0.1107, 0.1117, 0.1111, 0.1107, 0.1112, 0.1113],\n        [0.1107, 0.1109, 0.1114, 0.1117, 0.1121, 0.1108, 0.1106, 0.1107, 0.1111],\n        [0.1106, 0.1105, 0.1110, 0.1117, 0.1124, 0.1106, 0.1114, 0.1115, 0.1103]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:12:43 PM | Train: [ 3/50] Step 000/062 Loss 0.813 Prec@(1,5) (70.3%, 93.8%)\n11/23 08:17:06 PM | Train: [ 3/50] Step 050/062 Loss 0.692 Prec@(1,5) (72.2%, 97.9%)\n11/23 08:18:08 PM | Train: [ 3/50] Step 062/062 Loss 0.679 Prec@(1,5) (72.8%, 98.0%)\n11/23 08:18:08 PM | Train: [ 3/50] Final Prec@1 72.7750%\n11/23 08:18:09 PM | Valid: [ 3/50] Step 000/062 Loss 0.942 Prec@(1,5) (62.5%, 98.4%)\n11/23 08:18:21 PM | Valid: [ 3/50] Step 050/062 Loss 0.724 Prec@(1,5) (71.9%, 97.9%)\n11/23 08:18:24 PM | Valid: [ 3/50] Step 062/062 Loss 0.724 Prec@(1,5) (71.7%, 97.8%)\n11/23 08:18:24 PM | Valid: [ 3/50] Final Prec@1 71.6500%\n11/23 08:18:24 PM | genotype = Genotype(normal=[[('sep_conv_7x7', 1), ('sep_conv_3x3', 0)], [('max_pool_3x3', 0), ('sep_conv_7x7', 1)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 1)], [('dil_conv_5x5', 0), ('sep_conv_3x3', 3)]], normal_concat=range(2, 6), reduce=[[('sep_conv_7x7', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 4), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1111, 0.1101, 0.1109, 0.1121, 0.1112, 0.1108, 0.1106, 0.1121, 0.1110],\n        [0.1102, 0.1091, 0.1097, 0.1116, 0.1118, 0.1122, 0.1112, 0.1119, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1126, 0.1107, 0.1116, 0.1112, 0.1109, 0.1106, 0.1109, 0.1099, 0.1115],\n        [0.1105, 0.1094, 0.1101, 0.1111, 0.1113, 0.1123, 0.1116, 0.1120, 0.1116],\n        [0.1106, 0.1097, 0.1107, 0.1120, 0.1115, 0.1100, 0.1119, 0.1115, 0.1120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1118, 0.1106, 0.1112, 0.1104, 0.1111, 0.1114, 0.1106, 0.1120, 0.1110],\n        [0.1099, 0.1090, 0.1092, 0.1118, 0.1120, 0.1119, 0.1115, 0.1122, 0.1125],\n        [0.1100, 0.1096, 0.1103, 0.1108, 0.1111, 0.1117, 0.1119, 0.1131, 0.1115],\n        [0.1098, 0.1096, 0.1101, 0.1113, 0.1115, 0.1117, 0.1120, 0.1118, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1093, 0.1088, 0.1091, 0.1105, 0.1129, 0.1117, 0.1108, 0.1141, 0.1127],\n        [0.1098, 0.1092, 0.1098, 0.1112, 0.1110, 0.1124, 0.1119, 0.1117, 0.1130],\n        [0.1084, 0.1086, 0.1093, 0.1118, 0.1124, 0.1129, 0.1115, 0.1116, 0.1135],\n        [0.1084, 0.1081, 0.1087, 0.1131, 0.1116, 0.1125, 0.1120, 0.1121, 0.1135],\n        [0.1081, 0.1081, 0.1084, 0.1125, 0.1123, 0.1128, 0.1123, 0.1122, 0.1133]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1115, 0.1106, 0.1108, 0.1106, 0.1114, 0.1123, 0.1111, 0.1111, 0.1107],\n        [0.1115, 0.1106, 0.1110, 0.1121, 0.1105, 0.1105, 0.1103, 0.1116, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1114, 0.1109, 0.1114, 0.1110, 0.1106, 0.1104, 0.1117, 0.1117, 0.1108],\n        [0.1119, 0.1117, 0.1102, 0.1112, 0.1111, 0.1110, 0.1105, 0.1109, 0.1115],\n        [0.1106, 0.1100, 0.1107, 0.1110, 0.1111, 0.1121, 0.1122, 0.1111, 0.1112]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1117, 0.1111, 0.1121, 0.1105, 0.1113, 0.1107, 0.1105, 0.1111, 0.1111],\n        [0.1121, 0.1113, 0.1105, 0.1118, 0.1112, 0.1110, 0.1105, 0.1107, 0.1108],\n        [0.1104, 0.1099, 0.1100, 0.1105, 0.1131, 0.1116, 0.1117, 0.1118, 0.1110],\n        [0.1109, 0.1105, 0.1112, 0.1117, 0.1112, 0.1112, 0.1104, 0.1114, 0.1115]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1121, 0.1114, 0.1103, 0.1106, 0.1116, 0.1116, 0.1112, 0.1107, 0.1105],\n        [0.1118, 0.1114, 0.1107, 0.1109, 0.1111, 0.1118, 0.1112, 0.1096, 0.1116],\n        [0.1112, 0.1108, 0.1116, 0.1105, 0.1117, 0.1108, 0.1106, 0.1116, 0.1113],\n        [0.1107, 0.1106, 0.1117, 0.1113, 0.1124, 0.1112, 0.1100, 0.1109, 0.1112],\n        [0.1104, 0.1101, 0.1109, 0.1123, 0.1128, 0.1109, 0.1110, 0.1117, 0.1100]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:18:32 PM | Train: [ 4/50] Step 000/062 Loss 0.504 Prec@(1,5) (82.8%, 98.4%)\n11/23 08:22:56 PM | Train: [ 4/50] Step 050/062 Loss 0.616 Prec@(1,5) (75.9%, 98.4%)\n11/23 08:23:58 PM | Train: [ 4/50] Step 062/062 Loss 0.635 Prec@(1,5) (74.7%, 98.2%)\n11/23 08:23:58 PM | Train: [ 4/50] Final Prec@1 74.7000%\n11/23 08:23:59 PM | Valid: [ 4/50] Step 000/062 Loss 0.530 Prec@(1,5) (76.6%, 98.4%)\n11/23 08:24:12 PM | Valid: [ 4/50] Step 050/062 Loss 0.639 Prec@(1,5) (73.4%, 98.7%)\n11/23 08:24:14 PM | Valid: [ 4/50] Step 062/062 Loss 0.637 Prec@(1,5) (73.4%, 98.7%)\n11/23 08:24:14 PM | Valid: [ 4/50] Final Prec@1 73.4000%\n11/23 08:24:14 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 2), ('sep_conv_7x7', 1)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 1)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1106, 0.1095, 0.1105, 0.1129, 0.1112, 0.1113, 0.1103, 0.1120, 0.1118],\n        [0.1098, 0.1084, 0.1093, 0.1115, 0.1126, 0.1120, 0.1119, 0.1124, 0.1122]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1122, 0.1101, 0.1112, 0.1122, 0.1114, 0.1107, 0.1107, 0.1099, 0.1117],\n        [0.1101, 0.1089, 0.1100, 0.1111, 0.1115, 0.1126, 0.1117, 0.1121, 0.1120],\n        [0.1099, 0.1090, 0.1103, 0.1125, 0.1121, 0.1100, 0.1121, 0.1120, 0.1121]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1113, 0.1100, 0.1108, 0.1103, 0.1115, 0.1117, 0.1104, 0.1125, 0.1114],\n        [0.1095, 0.1084, 0.1088, 0.1123, 0.1126, 0.1119, 0.1113, 0.1122, 0.1129],\n        [0.1094, 0.1091, 0.1100, 0.1111, 0.1113, 0.1119, 0.1120, 0.1135, 0.1119],\n        [0.1091, 0.1088, 0.1097, 0.1118, 0.1117, 0.1120, 0.1124, 0.1120, 0.1126]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1084, 0.1079, 0.1084, 0.1104, 0.1137, 0.1120, 0.1108, 0.1149, 0.1135],\n        [0.1092, 0.1085, 0.1092, 0.1114, 0.1111, 0.1129, 0.1122, 0.1122, 0.1133],\n        [0.1076, 0.1077, 0.1089, 0.1117, 0.1130, 0.1133, 0.1118, 0.1119, 0.1141],\n        [0.1074, 0.1071, 0.1081, 0.1136, 0.1119, 0.1129, 0.1123, 0.1123, 0.1142],\n        [0.1070, 0.1071, 0.1076, 0.1125, 0.1131, 0.1136, 0.1125, 0.1127, 0.1138]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1116, 0.1105, 0.1109, 0.1107, 0.1115, 0.1125, 0.1105, 0.1111, 0.1107],\n        [0.1118, 0.1106, 0.1109, 0.1125, 0.1105, 0.1102, 0.1101, 0.1116, 0.1120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1117, 0.1108, 0.1116, 0.1111, 0.1106, 0.1098, 0.1115, 0.1117, 0.1112],\n        [0.1119, 0.1115, 0.1101, 0.1110, 0.1120, 0.1114, 0.1101, 0.1108, 0.1113],\n        [0.1102, 0.1094, 0.1102, 0.1111, 0.1114, 0.1125, 0.1127, 0.1114, 0.1111]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1121, 0.1112, 0.1121, 0.1102, 0.1119, 0.1103, 0.1100, 0.1110, 0.1112],\n        [0.1123, 0.1112, 0.1107, 0.1120, 0.1109, 0.1109, 0.1107, 0.1111, 0.1103],\n        [0.1101, 0.1093, 0.1099, 0.1107, 0.1136, 0.1117, 0.1116, 0.1119, 0.1113],\n        [0.1110, 0.1105, 0.1116, 0.1117, 0.1114, 0.1109, 0.1102, 0.1114, 0.1114]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1123, 0.1114, 0.1103, 0.1107, 0.1116, 0.1119, 0.1108, 0.1106, 0.1105],\n        [0.1119, 0.1112, 0.1102, 0.1109, 0.1112, 0.1122, 0.1117, 0.1092, 0.1114],\n        [0.1109, 0.1101, 0.1113, 0.1109, 0.1121, 0.1107, 0.1106, 0.1117, 0.1117],\n        [0.1105, 0.1102, 0.1119, 0.1111, 0.1132, 0.1115, 0.1094, 0.1110, 0.1111],\n        [0.1100, 0.1097, 0.1109, 0.1129, 0.1128, 0.1108, 0.1110, 0.1120, 0.1099]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:24:22 PM | Train: [ 5/50] Step 000/062 Loss 0.503 Prec@(1,5) (82.8%, 98.4%)\n11/23 08:28:46 PM | Train: [ 5/50] Step 050/062 Loss 0.596 Prec@(1,5) (76.5%, 98.5%)\n11/23 08:29:47 PM | Train: [ 5/50] Step 062/062 Loss 0.584 Prec@(1,5) (77.0%, 98.5%)\n11/23 08:29:47 PM | Train: [ 5/50] Final Prec@1 76.9500%\n11/23 08:29:48 PM | Valid: [ 5/50] Step 000/062 Loss 0.666 Prec@(1,5) (70.3%, 96.9%)\n11/23 08:30:00 PM | Valid: [ 5/50] Step 050/062 Loss 0.608 Prec@(1,5) (75.3%, 98.7%)\n11/23 08:30:04 PM | Valid: [ 5/50] Step 062/062 Loss 0.605 Prec@(1,5) (75.4%, 98.7%)\n11/23 08:30:04 PM | Valid: [ 5/50] Final Prec@1 75.4500%\n11/23 08:30:04 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 2), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1105, 0.1088, 0.1101, 0.1133, 0.1109, 0.1118, 0.1101, 0.1120, 0.1124],\n        [0.1097, 0.1079, 0.1089, 0.1116, 0.1131, 0.1121, 0.1119, 0.1128, 0.1120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1123, 0.1096, 0.1110, 0.1128, 0.1115, 0.1103, 0.1109, 0.1096, 0.1120],\n        [0.1101, 0.1086, 0.1100, 0.1113, 0.1117, 0.1126, 0.1117, 0.1119, 0.1121],\n        [0.1098, 0.1086, 0.1102, 0.1127, 0.1128, 0.1095, 0.1120, 0.1124, 0.1120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1111, 0.1094, 0.1103, 0.1106, 0.1119, 0.1119, 0.1099, 0.1130, 0.1119],\n        [0.1092, 0.1078, 0.1084, 0.1122, 0.1127, 0.1126, 0.1113, 0.1124, 0.1133],\n        [0.1089, 0.1084, 0.1095, 0.1111, 0.1116, 0.1126, 0.1120, 0.1138, 0.1121],\n        [0.1085, 0.1080, 0.1091, 0.1119, 0.1117, 0.1122, 0.1130, 0.1124, 0.1133]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1079, 0.1070, 0.1075, 0.1102, 0.1146, 0.1123, 0.1107, 0.1159, 0.1140],\n        [0.1089, 0.1079, 0.1087, 0.1113, 0.1114, 0.1130, 0.1125, 0.1124, 0.1138],\n        [0.1071, 0.1071, 0.1085, 0.1116, 0.1129, 0.1136, 0.1122, 0.1125, 0.1146],\n        [0.1067, 0.1062, 0.1073, 0.1139, 0.1121, 0.1133, 0.1131, 0.1123, 0.1150],\n        [0.1063, 0.1062, 0.1068, 0.1125, 0.1137, 0.1143, 0.1130, 0.1130, 0.1143]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1117, 0.1103, 0.1106, 0.1104, 0.1116, 0.1130, 0.1101, 0.1113, 0.1111],\n        [0.1120, 0.1106, 0.1109, 0.1131, 0.1101, 0.1105, 0.1097, 0.1113, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1116, 0.1101, 0.1120, 0.1112, 0.1101, 0.1097, 0.1118, 0.1121, 0.1113],\n        [0.1121, 0.1115, 0.1101, 0.1107, 0.1123, 0.1115, 0.1097, 0.1109, 0.1111],\n        [0.1102, 0.1088, 0.1099, 0.1111, 0.1118, 0.1128, 0.1129, 0.1116, 0.1109]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1123, 0.1110, 0.1122, 0.1101, 0.1119, 0.1102, 0.1095, 0.1112, 0.1117],\n        [0.1127, 0.1114, 0.1105, 0.1122, 0.1109, 0.1107, 0.1106, 0.1112, 0.1099],\n        [0.1098, 0.1084, 0.1095, 0.1111, 0.1145, 0.1120, 0.1111, 0.1121, 0.1116],\n        [0.1111, 0.1103, 0.1119, 0.1114, 0.1114, 0.1109, 0.1101, 0.1114, 0.1114]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1123, 0.1110, 0.1104, 0.1105, 0.1119, 0.1119, 0.1107, 0.1104, 0.1110],\n        [0.1123, 0.1113, 0.1098, 0.1114, 0.1109, 0.1122, 0.1118, 0.1091, 0.1112],\n        [0.1107, 0.1092, 0.1108, 0.1111, 0.1126, 0.1111, 0.1106, 0.1122, 0.1116],\n        [0.1105, 0.1098, 0.1119, 0.1112, 0.1137, 0.1119, 0.1090, 0.1112, 0.1107],\n        [0.1097, 0.1091, 0.1107, 0.1133, 0.1127, 0.1107, 0.1115, 0.1125, 0.1098]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:30:11 PM | Train: [ 6/50] Step 000/062 Loss 0.559 Prec@(1,5) (76.6%, 100.0%)\n11/23 08:34:35 PM | Train: [ 6/50] Step 050/062 Loss 0.530 Prec@(1,5) (78.9%, 98.9%)\n11/23 08:40:23 PM | Train: [ 7/50] Step 050/062 Loss 0.519 Prec@(1,5) (80.0%, 98.9%)\n11/23 08:41:25 PM | Train: [ 7/50] Step 062/062 Loss 0.509 Prec@(1,5) (80.4%, 98.9%)\n11/23 08:41:25 PM | Train: [ 7/50] Final Prec@1 80.4250%\n11/23 08:41:26 PM | Valid: [ 7/50] Step 000/062 Loss 0.390 Prec@(1,5) (85.9%, 98.4%)\n11/23 08:41:38 PM | Valid: [ 7/50] Step 050/062 Loss 0.638 Prec@(1,5) (75.2%, 98.7%)\n11/23 08:41:41 PM | Valid: [ 7/50] Step 062/062 Loss 0.632 Prec@(1,5) (75.2%, 98.7%)\n11/23 08:41:41 PM | Valid: [ 7/50] Final Prec@1 75.1750%\n11/23 08:41:41 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 0)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('skip_connect', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1100, 0.1076, 0.1096, 0.1146, 0.1107, 0.1120, 0.1098, 0.1117, 0.1139],\n        [0.1093, 0.1067, 0.1085, 0.1122, 0.1141, 0.1126, 0.1125, 0.1124, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1126, 0.1088, 0.1107, 0.1141, 0.1116, 0.1103, 0.1105, 0.1088, 0.1126],\n        [0.1099, 0.1077, 0.1098, 0.1115, 0.1121, 0.1125, 0.1120, 0.1120, 0.1125],\n        [0.1092, 0.1075, 0.1101, 0.1130, 0.1134, 0.1085, 0.1125, 0.1134, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1110, 0.1084, 0.1097, 0.1107, 0.1126, 0.1119, 0.1089, 0.1141, 0.1126],\n        [0.1083, 0.1065, 0.1075, 0.1132, 0.1134, 0.1133, 0.1107, 0.1124, 0.1146],\n        [0.1074, 0.1069, 0.1088, 0.1116, 0.1120, 0.1126, 0.1130, 0.1148, 0.1129],\n        [0.1070, 0.1064, 0.1081, 0.1127, 0.1116, 0.1136, 0.1136, 0.1122, 0.1148]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1064, 0.1050, 0.1058, 0.1098, 0.1164, 0.1126, 0.1108, 0.1179, 0.1153],\n        [0.1080, 0.1065, 0.1077, 0.1125, 0.1113, 0.1132, 0.1132, 0.1131, 0.1144],\n        [0.1056, 0.1056, 0.1080, 0.1121, 0.1126, 0.1135, 0.1125, 0.1133, 0.1168],\n        [0.1048, 0.1040, 0.1056, 0.1148, 0.1134, 0.1135, 0.1140, 0.1128, 0.1170],\n        [0.1043, 0.1040, 0.1049, 0.1126, 0.1147, 0.1162, 0.1139, 0.1141, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1127, 0.1107, 0.1101, 0.1087, 0.1121, 0.1137, 0.1092, 0.1117, 0.1110],\n        [0.1118, 0.1099, 0.1110, 0.1143, 0.1103, 0.1104, 0.1091, 0.1109, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1118, 0.1096, 0.1129, 0.1109, 0.1097, 0.1095, 0.1115, 0.1126, 0.1114],\n        [0.1124, 0.1114, 0.1099, 0.1106, 0.1128, 0.1116, 0.1090, 0.1112, 0.1110],\n        [0.1095, 0.1076, 0.1091, 0.1114, 0.1121, 0.1143, 0.1139, 0.1117, 0.1104]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1133, 0.1116, 0.1128, 0.1101, 0.1121, 0.1100, 0.1080, 0.1107, 0.1114],\n        [0.1135, 0.1119, 0.1104, 0.1117, 0.1108, 0.1107, 0.1102, 0.1117, 0.1091],\n        [0.1092, 0.1076, 0.1096, 0.1111, 0.1160, 0.1126, 0.1101, 0.1126, 0.1113],\n        [0.1114, 0.1106, 0.1131, 0.1106, 0.1107, 0.1110, 0.1097, 0.1115, 0.1113]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1128, 0.1111, 0.1103, 0.1106, 0.1122, 0.1116, 0.1104, 0.1094, 0.1115],\n        [0.1126, 0.1113, 0.1099, 0.1117, 0.1105, 0.1121, 0.1121, 0.1086, 0.1112],\n        [0.1095, 0.1079, 0.1105, 0.1109, 0.1133, 0.1116, 0.1113, 0.1129, 0.1120],\n        [0.1097, 0.1090, 0.1122, 0.1113, 0.1151, 0.1124, 0.1085, 0.1112, 0.1106],\n        [0.1088, 0.1084, 0.1113, 0.1144, 0.1129, 0.1105, 0.1117, 0.1118, 0.1102]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:41:48 PM | Train: [ 8/50] Step 000/062 Loss 0.438 Prec@(1,5) (85.9%, 98.4%)\n11/23 08:46:11 PM | Train: [ 8/50] Step 050/062 Loss 0.477 Prec@(1,5) (81.7%, 99.1%)\n11/23 08:47:13 PM | Train: [ 8/50] Step 062/062 Loss 0.475 Prec@(1,5) (81.8%, 99.2%)\n11/23 08:47:13 PM | Train: [ 8/50] Final Prec@1 81.7750%\n11/23 08:47:14 PM | Valid: [ 8/50] Step 000/062 Loss 0.564 Prec@(1,5) (81.2%, 100.0%)\n11/23 08:47:26 PM | Valid: [ 8/50] Step 050/062 Loss 0.602 Prec@(1,5) (77.5%, 98.9%)\n11/23 08:47:29 PM | Valid: [ 8/50] Step 062/062 Loss 0.603 Prec@(1,5) (77.0%, 98.8%)\n11/23 08:47:29 PM | Valid: [ 8/50] Final Prec@1 77.0500%\n11/23 08:47:29 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 2), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1102, 0.1068, 0.1093, 0.1151, 0.1102, 0.1124, 0.1097, 0.1117, 0.1145],\n        [0.1092, 0.1061, 0.1081, 0.1120, 0.1139, 0.1132, 0.1133, 0.1123, 0.1119]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1129, 0.1082, 0.1106, 0.1148, 0.1115, 0.1101, 0.1103, 0.1086, 0.1131],\n        [0.1097, 0.1070, 0.1094, 0.1121, 0.1120, 0.1129, 0.1119, 0.1123, 0.1128],\n        [0.1088, 0.1065, 0.1097, 0.1129, 0.1139, 0.1087, 0.1126, 0.1140, 0.1129]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1108, 0.1075, 0.1091, 0.1113, 0.1133, 0.1124, 0.1080, 0.1144, 0.1132],\n        [0.1076, 0.1055, 0.1066, 0.1142, 0.1135, 0.1142, 0.1105, 0.1127, 0.1152],\n        [0.1065, 0.1057, 0.1078, 0.1121, 0.1131, 0.1127, 0.1139, 0.1150, 0.1132],\n        [0.1060, 0.1051, 0.1070, 0.1134, 0.1118, 0.1144, 0.1144, 0.1123, 0.1155]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.1039, 0.1049, 0.1094, 0.1174, 0.1127, 0.1107, 0.1191, 0.1160],\n        [0.1078, 0.1059, 0.1071, 0.1132, 0.1104, 0.1132, 0.1138, 0.1139, 0.1146],\n        [0.1051, 0.1047, 0.1073, 0.1127, 0.1121, 0.1143, 0.1125, 0.1138, 0.1175],\n        [0.1041, 0.1030, 0.1049, 0.1148, 0.1140, 0.1135, 0.1145, 0.1129, 0.1182],\n        [0.1038, 0.1033, 0.1044, 0.1125, 0.1147, 0.1167, 0.1144, 0.1146, 0.1156]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1130, 0.1104, 0.1102, 0.1082, 0.1123, 0.1141, 0.1085, 0.1120, 0.1112],\n        [0.1123, 0.1099, 0.1110, 0.1149, 0.1100, 0.1104, 0.1089, 0.1103, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1120, 0.1090, 0.1129, 0.1108, 0.1092, 0.1099, 0.1118, 0.1130, 0.1115],\n        [0.1128, 0.1116, 0.1099, 0.1107, 0.1134, 0.1114, 0.1082, 0.1115, 0.1106],\n        [0.1092, 0.1067, 0.1085, 0.1112, 0.1122, 0.1152, 0.1145, 0.1123, 0.1103]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1138, 0.1116, 0.1128, 0.1097, 0.1121, 0.1100, 0.1080, 0.1108, 0.1113],\n        [0.1133, 0.1114, 0.1104, 0.1120, 0.1110, 0.1106, 0.1104, 0.1118, 0.1090],\n        [0.1084, 0.1063, 0.1089, 0.1112, 0.1167, 0.1131, 0.1103, 0.1133, 0.1117],\n        [0.1113, 0.1100, 0.1131, 0.1106, 0.1106, 0.1116, 0.1096, 0.1118, 0.1115]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1128, 0.1107, 0.1105, 0.1104, 0.1129, 0.1118, 0.1100, 0.1093, 0.1116],\n        [0.1131, 0.1115, 0.1093, 0.1126, 0.1098, 0.1126, 0.1116, 0.1085, 0.1110],\n        [0.1089, 0.1067, 0.1100, 0.1117, 0.1135, 0.1115, 0.1121, 0.1132, 0.1124],\n        [0.1092, 0.1080, 0.1120, 0.1117, 0.1162, 0.1127, 0.1087, 0.1112, 0.1104],\n        [0.1084, 0.1079, 0.1115, 0.1149, 0.1130, 0.1103, 0.1117, 0.1118, 0.1105]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:47:37 PM | Train: [ 9/50] Step 000/062 Loss 0.362 Prec@(1,5) (81.2%, 100.0%)\n11/23 08:52:00 PM | Train: [ 9/50] Step 050/062 Loss 0.443 Prec@(1,5) (82.2%, 99.1%)\n11/23 08:53:01 PM | Train: [ 9/50] Step 062/062 Loss 0.450 Prec@(1,5) (81.9%, 99.0%)\n11/23 08:53:01 PM | Train: [ 9/50] Final Prec@1 81.9000%\n11/23 08:53:02 PM | Valid: [ 9/50] Step 000/062 Loss 0.578 Prec@(1,5) (79.7%, 100.0%)\n11/23 08:53:14 PM | Valid: [ 9/50] Step 050/062 Loss 0.557 Prec@(1,5) (77.8%, 99.0%)\n11/23 08:53:17 PM | Valid: [ 9/50] Step 062/062 Loss 0.560 Prec@(1,5) (77.8%, 99.1%)\n11/23 08:53:17 PM | Valid: [ 9/50] Final Prec@1 77.7500%\n11/23 08:53:17 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 2)], [('sep_conv_3x3', 1), ('dil_conv_5x5', 2)], [('dil_conv_5x5', 0), ('sep_conv_7x7', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('skip_connect', 0)], [('sep_conv_5x5', 2), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1099, 0.1060, 0.1088, 0.1155, 0.1102, 0.1126, 0.1096, 0.1120, 0.1153],\n        [0.1088, 0.1052, 0.1077, 0.1119, 0.1144, 0.1138, 0.1140, 0.1124, 0.1118]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1123, 0.1072, 0.1098, 0.1155, 0.1118, 0.1100, 0.1110, 0.1084, 0.1139],\n        [0.1093, 0.1063, 0.1092, 0.1123, 0.1124, 0.1134, 0.1120, 0.1122, 0.1130],\n        [0.1082, 0.1054, 0.1094, 0.1130, 0.1144, 0.1086, 0.1135, 0.1141, 0.1134]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1104, 0.1066, 0.1086, 0.1116, 0.1137, 0.1126, 0.1076, 0.1150, 0.1139],\n        [0.1067, 0.1044, 0.1057, 0.1152, 0.1140, 0.1149, 0.1103, 0.1128, 0.1160],\n        [0.1059, 0.1048, 0.1073, 0.1122, 0.1136, 0.1130, 0.1142, 0.1154, 0.1136],\n        [0.1052, 0.1039, 0.1063, 0.1141, 0.1119, 0.1150, 0.1145, 0.1129, 0.1161]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1052, 0.1028, 0.1040, 0.1087, 0.1189, 0.1133, 0.1105, 0.1199, 0.1167],\n        [0.1073, 0.1053, 0.1069, 0.1138, 0.1100, 0.1133, 0.1138, 0.1144, 0.1152],\n        [0.1046, 0.1038, 0.1071, 0.1130, 0.1117, 0.1145, 0.1128, 0.1141, 0.1184],\n        [0.1035, 0.1020, 0.1045, 0.1148, 0.1142, 0.1139, 0.1144, 0.1134, 0.1193],\n        [0.1031, 0.1024, 0.1038, 0.1123, 0.1150, 0.1169, 0.1147, 0.1157, 0.1161]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1132, 0.1103, 0.1101, 0.1076, 0.1128, 0.1140, 0.1085, 0.1122, 0.1113],\n        [0.1122, 0.1096, 0.1105, 0.1155, 0.1100, 0.1109, 0.1084, 0.1107, 0.1121]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1125, 0.1092, 0.1134, 0.1102, 0.1091, 0.1096, 0.1115, 0.1130, 0.1115],\n        [0.1131, 0.1118, 0.1097, 0.1102, 0.1135, 0.1116, 0.1074, 0.1124, 0.1102],\n        [0.1092, 0.1060, 0.1086, 0.1113, 0.1123, 0.1155, 0.1146, 0.1124, 0.1100]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1142, 0.1118, 0.1131, 0.1100, 0.1124, 0.1094, 0.1075, 0.1108, 0.1109],\n        [0.1137, 0.1119, 0.1104, 0.1123, 0.1113, 0.1097, 0.1104, 0.1118, 0.1085],\n        [0.1081, 0.1055, 0.1092, 0.1119, 0.1174, 0.1127, 0.1101, 0.1134, 0.1117],\n        [0.1110, 0.1096, 0.1138, 0.1107, 0.1103, 0.1116, 0.1096, 0.1114, 0.1120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1128, 0.1106, 0.1110, 0.1107, 0.1130, 0.1119, 0.1092, 0.1092, 0.1116],\n        [0.1130, 0.1114, 0.1091, 0.1129, 0.1093, 0.1129, 0.1118, 0.1086, 0.1109],\n        [0.1081, 0.1055, 0.1099, 0.1124, 0.1137, 0.1112, 0.1125, 0.1137, 0.1130],\n        [0.1084, 0.1070, 0.1119, 0.1123, 0.1171, 0.1126, 0.1086, 0.1114, 0.1107],\n        [0.1077, 0.1071, 0.1116, 0.1157, 0.1124, 0.1106, 0.1115, 0.1118, 0.1114]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:53:24 PM | Train: [10/50] Step 000/062 Loss 0.364 Prec@(1,5) (87.5%, 100.0%)\n11/23 08:57:47 PM | Train: [10/50] Step 050/062 Loss 0.418 Prec@(1,5) (83.9%, 99.4%)\n11/23 08:58:49 PM | Train: [10/50] Step 062/062 Loss 0.421 Prec@(1,5) (83.7%, 99.4%)\n11/23 08:58:49 PM | Train: [10/50] Final Prec@1 83.7250%\n11/23 08:58:50 PM | Valid: [10/50] Step 000/062 Loss 0.476 Prec@(1,5) (79.7%, 98.4%)\n11/23 08:59:02 PM | Valid: [10/50] Step 050/062 Loss 0.508 Prec@(1,5) (80.8%, 99.1%)\n11/23 08:59:05 PM | Valid: [10/50] Step 062/062 Loss 0.517 Prec@(1,5) (80.7%, 99.0%)\n11/23 08:59:05 PM | Valid: [10/50] Final Prec@1 80.6500%\n11/23 08:59:05 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 2)], [('sep_conv_3x3', 1), ('dil_conv_5x5', 0)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('skip_connect', 0)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1091, 0.1048, 0.1080, 0.1159, 0.1104, 0.1126, 0.1101, 0.1129, 0.1162],\n        [0.1088, 0.1046, 0.1076, 0.1125, 0.1146, 0.1141, 0.1140, 0.1122, 0.1116]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1113, 0.1059, 0.1088, 0.1166, 0.1126, 0.1099, 0.1113, 0.1086, 0.1149],\n        [0.1090, 0.1055, 0.1089, 0.1122, 0.1127, 0.1137, 0.1121, 0.1132, 0.1127],\n        [0.1073, 0.1043, 0.1090, 0.1126, 0.1148, 0.1087, 0.1144, 0.1145, 0.1144]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1097, 0.1057, 0.1079, 0.1122, 0.1148, 0.1129, 0.1067, 0.1155, 0.1147],\n        [0.1064, 0.1037, 0.1052, 0.1160, 0.1144, 0.1152, 0.1102, 0.1122, 0.1166],\n        [0.1051, 0.1039, 0.1069, 0.1122, 0.1141, 0.1131, 0.1149, 0.1157, 0.1142],\n        [0.1044, 0.1028, 0.1055, 0.1142, 0.1121, 0.1152, 0.1154, 0.1135, 0.1170]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1040, 0.1015, 0.1029, 0.1086, 0.1196, 0.1138, 0.1100, 0.1214, 0.1183],\n        [0.1071, 0.1045, 0.1064, 0.1147, 0.1100, 0.1135, 0.1138, 0.1146, 0.1154],\n        [0.1036, 0.1025, 0.1063, 0.1138, 0.1122, 0.1149, 0.1130, 0.1143, 0.1193],\n        [0.1026, 0.1007, 0.1037, 0.1156, 0.1146, 0.1138, 0.1147, 0.1135, 0.1208],\n        [0.1019, 0.1009, 0.1026, 0.1122, 0.1155, 0.1173, 0.1153, 0.1173, 0.1170]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1131, 0.1100, 0.1108, 0.1074, 0.1133, 0.1147, 0.1080, 0.1116, 0.1112],\n        [0.1121, 0.1094, 0.1099, 0.1157, 0.1104, 0.1120, 0.1078, 0.1107, 0.1121]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1125, 0.1089, 0.1140, 0.1102, 0.1090, 0.1097, 0.1109, 0.1128, 0.1120],\n        [0.1131, 0.1119, 0.1098, 0.1100, 0.1140, 0.1112, 0.1075, 0.1129, 0.1097],\n        [0.1091, 0.1055, 0.1086, 0.1112, 0.1125, 0.1160, 0.1148, 0.1126, 0.1097]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1140, 0.1114, 0.1136, 0.1095, 0.1129, 0.1095, 0.1072, 0.1113, 0.1106],\n        [0.1141, 0.1124, 0.1101, 0.1119, 0.1112, 0.1092, 0.1109, 0.1122, 0.1081],\n        [0.1078, 0.1051, 0.1093, 0.1123, 0.1183, 0.1127, 0.1097, 0.1134, 0.1113],\n        [0.1110, 0.1092, 0.1144, 0.1104, 0.1102, 0.1115, 0.1090, 0.1117, 0.1127]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1127, 0.1104, 0.1111, 0.1105, 0.1127, 0.1124, 0.1093, 0.1089, 0.1119],\n        [0.1126, 0.1110, 0.1089, 0.1138, 0.1091, 0.1131, 0.1123, 0.1086, 0.1107],\n        [0.1074, 0.1044, 0.1094, 0.1131, 0.1144, 0.1116, 0.1130, 0.1139, 0.1129],\n        [0.1077, 0.1057, 0.1114, 0.1128, 0.1184, 0.1127, 0.1091, 0.1108, 0.1112],\n        [0.1069, 0.1060, 0.1113, 0.1162, 0.1128, 0.1103, 0.1117, 0.1125, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 08:59:13 PM | Train: [11/50] Step 000/062 Loss 0.463 Prec@(1,5) (82.8%, 100.0%)\n11/23 09:03:35 PM | Train: [11/50] Step 050/062 Loss 0.412 Prec@(1,5) (83.4%, 99.4%)\n11/23 09:04:37 PM | Train: [11/50] Step 062/062 Loss 0.402 Prec@(1,5) (83.9%, 99.4%)\n11/23 09:04:37 PM | Train: [11/50] Final Prec@1 83.8500%\n11/23 09:04:38 PM | Valid: [11/50] Step 000/062 Loss 0.695 Prec@(1,5) (73.4%, 100.0%)\n11/23 09:04:50 PM | Valid: [11/50] Step 050/062 Loss 0.571 Prec@(1,5) (79.3%, 98.9%)\n11/23 09:04:53 PM | Valid: [11/50] Step 062/062 Loss 0.555 Prec@(1,5) (80.2%, 98.9%)\n11/23 09:04:53 PM | Valid: [11/50] Final Prec@1 80.2250%\n11/23 09:04:53 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('sep_conv_5x5', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('skip_connect', 0)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1092, 0.1040, 0.1076, 0.1167, 0.1102, 0.1130, 0.1097, 0.1131, 0.1166],\n        [0.1086, 0.1036, 0.1070, 0.1126, 0.1155, 0.1140, 0.1144, 0.1125, 0.1119]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1113, 0.1050, 0.1084, 0.1177, 0.1131, 0.1095, 0.1110, 0.1083, 0.1156],\n        [0.1087, 0.1044, 0.1082, 0.1127, 0.1132, 0.1141, 0.1128, 0.1131, 0.1129],\n        [0.1069, 0.1032, 0.1085, 0.1126, 0.1151, 0.1095, 0.1150, 0.1141, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1099, 0.1049, 0.1075, 0.1127, 0.1153, 0.1130, 0.1056, 0.1159, 0.1151],\n        [0.1058, 0.1025, 0.1043, 0.1169, 0.1152, 0.1161, 0.1102, 0.1119, 0.1172],\n        [0.1047, 0.1029, 0.1064, 0.1124, 0.1151, 0.1133, 0.1150, 0.1156, 0.1147],\n        [0.1037, 0.1016, 0.1048, 0.1140, 0.1126, 0.1157, 0.1160, 0.1136, 0.1179]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1034, 0.1003, 0.1020, 0.1080, 0.1203, 0.1140, 0.1107, 0.1221, 0.1192],\n        [0.1066, 0.1034, 0.1057, 0.1153, 0.1100, 0.1134, 0.1145, 0.1151, 0.1162],\n        [0.1028, 0.1012, 0.1055, 0.1148, 0.1117, 0.1157, 0.1140, 0.1138, 0.1205],\n        [0.1017, 0.0993, 0.1027, 0.1161, 0.1153, 0.1142, 0.1154, 0.1134, 0.1220],\n        [0.1011, 0.0997, 0.1017, 0.1123, 0.1156, 0.1178, 0.1158, 0.1180, 0.1180]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1136, 0.1100, 0.1108, 0.1069, 0.1136, 0.1150, 0.1074, 0.1112, 0.1114],\n        [0.1123, 0.1092, 0.1096, 0.1165, 0.1103, 0.1125, 0.1072, 0.1104, 0.1121]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1128, 0.1085, 0.1143, 0.1097, 0.1088, 0.1103, 0.1107, 0.1130, 0.1118],\n        [0.1137, 0.1121, 0.1096, 0.1097, 0.1145, 0.1111, 0.1073, 0.1126, 0.1094],\n        [0.1091, 0.1046, 0.1080, 0.1114, 0.1127, 0.1165, 0.1151, 0.1133, 0.1093]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1144, 0.1112, 0.1133, 0.1097, 0.1126, 0.1097, 0.1067, 0.1116, 0.1107],\n        [0.1144, 0.1123, 0.1102, 0.1123, 0.1114, 0.1084, 0.1112, 0.1120, 0.1079],\n        [0.1076, 0.1041, 0.1093, 0.1131, 0.1186, 0.1128, 0.1093, 0.1133, 0.1119],\n        [0.1114, 0.1088, 0.1149, 0.1102, 0.1104, 0.1115, 0.1088, 0.1111, 0.1128]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1129, 0.1100, 0.1113, 0.1103, 0.1129, 0.1128, 0.1090, 0.1085, 0.1122],\n        [0.1129, 0.1109, 0.1087, 0.1143, 0.1087, 0.1132, 0.1121, 0.1084, 0.1106],\n        [0.1070, 0.1032, 0.1089, 0.1138, 0.1143, 0.1116, 0.1133, 0.1147, 0.1132],\n        [0.1075, 0.1048, 0.1113, 0.1135, 0.1194, 0.1131, 0.1090, 0.1105, 0.1109],\n        [0.1063, 0.1048, 0.1108, 0.1167, 0.1127, 0.1104, 0.1122, 0.1132, 0.1129]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:05:00 PM | Train: [12/50] Step 000/062 Loss 0.515 Prec@(1,5) (76.6%, 100.0%)\n11/23 09:09:24 PM | Train: [12/50] Step 050/062 Loss 0.376 Prec@(1,5) (85.5%, 99.4%)\n11/23 09:10:26 PM | Train: [12/50] Step 062/062 Loss 0.376 Prec@(1,5) (85.5%, 99.5%)\n11/23 09:10:26 PM | Train: [12/50] Final Prec@1 85.5000%\n11/23 09:10:27 PM | Valid: [12/50] Step 000/062 Loss 0.382 Prec@(1,5) (89.1%, 100.0%)\n11/23 09:10:40 PM | Valid: [12/50] Step 050/062 Loss 0.464 Prec@(1,5) (81.7%, 99.2%)\n11/23 09:10:43 PM | Valid: [12/50] Step 062/062 Loss 0.473 Prec@(1,5) (81.5%, 99.2%)\n11/23 09:10:43 PM | Valid: [12/50] Final Prec@1 81.5250%\n11/23 09:10:43 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('sep_conv_5x5', 1)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1091, 0.1035, 0.1076, 0.1170, 0.1100, 0.1130, 0.1095, 0.1130, 0.1173],\n        [0.1081, 0.1027, 0.1065, 0.1131, 0.1163, 0.1145, 0.1145, 0.1125, 0.1119]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1111, 0.1043, 0.1081, 0.1181, 0.1139, 0.1092, 0.1111, 0.1080, 0.1161],\n        [0.1079, 0.1031, 0.1073, 0.1135, 0.1139, 0.1143, 0.1130, 0.1134, 0.1136],\n        [0.1060, 0.1021, 0.1082, 0.1129, 0.1152, 0.1096, 0.1155, 0.1142, 0.1163]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1101, 0.1044, 0.1075, 0.1126, 0.1154, 0.1127, 0.1049, 0.1166, 0.1158],\n        [0.1051, 0.1013, 0.1034, 0.1174, 0.1157, 0.1163, 0.1107, 0.1117, 0.1183],\n        [0.1036, 0.1018, 0.1057, 0.1124, 0.1159, 0.1136, 0.1162, 0.1159, 0.1149],\n        [0.1027, 0.1003, 0.1041, 0.1142, 0.1125, 0.1165, 0.1166, 0.1138, 0.1193]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1027, 0.0992, 0.1011, 0.1075, 0.1218, 0.1146, 0.1104, 0.1228, 0.1199],\n        [0.1059, 0.1023, 0.1048, 0.1166, 0.1095, 0.1138, 0.1152, 0.1151, 0.1168],\n        [0.1014, 0.0999, 0.1045, 0.1154, 0.1123, 0.1161, 0.1145, 0.1146, 0.1213],\n        [0.1004, 0.0979, 0.1018, 0.1166, 0.1154, 0.1145, 0.1155, 0.1141, 0.1238],\n        [0.0997, 0.0983, 0.1005, 0.1126, 0.1162, 0.1179, 0.1167, 0.1189, 0.1193]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1147, 0.1106, 0.1110, 0.1060, 0.1135, 0.1148, 0.1071, 0.1112, 0.1111],\n        [0.1119, 0.1088, 0.1096, 0.1175, 0.1099, 0.1132, 0.1064, 0.1102, 0.1125]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1138, 0.1088, 0.1139, 0.1098, 0.1086, 0.1107, 0.1103, 0.1128, 0.1113],\n        [0.1138, 0.1121, 0.1097, 0.1095, 0.1143, 0.1115, 0.1071, 0.1125, 0.1095],\n        [0.1085, 0.1036, 0.1075, 0.1114, 0.1130, 0.1169, 0.1159, 0.1140, 0.1091]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1154, 0.1118, 0.1129, 0.1096, 0.1126, 0.1096, 0.1065, 0.1116, 0.1100],\n        [0.1141, 0.1121, 0.1101, 0.1126, 0.1117, 0.1084, 0.1110, 0.1122, 0.1080],\n        [0.1069, 0.1031, 0.1089, 0.1139, 0.1197, 0.1130, 0.1096, 0.1130, 0.1119],\n        [0.1116, 0.1087, 0.1157, 0.1099, 0.1104, 0.1110, 0.1084, 0.1109, 0.1135]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1139, 0.1105, 0.1109, 0.1100, 0.1126, 0.1131, 0.1086, 0.1081, 0.1125],\n        [0.1127, 0.1108, 0.1086, 0.1143, 0.1088, 0.1139, 0.1117, 0.1086, 0.1106],\n        [0.1066, 0.1025, 0.1091, 0.1133, 0.1143, 0.1112, 0.1139, 0.1155, 0.1136],\n        [0.1075, 0.1044, 0.1117, 0.1137, 0.1203, 0.1132, 0.1085, 0.1106, 0.1103],\n        [0.1060, 0.1043, 0.1113, 0.1164, 0.1124, 0.1104, 0.1117, 0.1140, 0.1135]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:10:51 PM | Train: [13/50] Step 000/062 Loss 0.365 Prec@(1,5) (87.5%, 100.0%)\n11/23 09:15:16 PM | Train: [13/50] Step 050/062 Loss 0.340 Prec@(1,5) (87.4%, 99.5%)\n11/23 09:16:17 PM | Train: [13/50] Step 062/062 Loss 0.341 Prec@(1,5) (87.1%, 99.5%)\n11/23 09:16:18 PM | Train: [13/50] Final Prec@1 87.1000%\n11/23 09:16:18 PM | Valid: [13/50] Step 000/062 Loss 0.461 Prec@(1,5) (82.8%, 100.0%)\n11/23 09:16:30 PM | Valid: [13/50] Step 050/062 Loss 0.488 Prec@(1,5) (81.4%, 98.9%)\n11/23 09:16:34 PM | Valid: [13/50] Step 062/062 Loss 0.482 Prec@(1,5) (81.8%, 99.0%)\n11/23 09:16:34 PM | Valid: [13/50] Final Prec@1 81.7750%\n11/23 09:16:34 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_7x7', 3)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1092, 0.1030, 0.1076, 0.1171, 0.1092, 0.1131, 0.1095, 0.1139, 0.1174],\n        [0.1080, 0.1020, 0.1065, 0.1132, 0.1165, 0.1145, 0.1145, 0.1124, 0.1124]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1111, 0.1038, 0.1079, 0.1184, 0.1145, 0.1088, 0.1108, 0.1080, 0.1168],\n        [0.1074, 0.1023, 0.1069, 0.1142, 0.1142, 0.1149, 0.1125, 0.1134, 0.1142],\n        [0.1049, 0.1009, 0.1075, 0.1124, 0.1149, 0.1101, 0.1174, 0.1148, 0.1170]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1097, 0.1036, 0.1070, 0.1134, 0.1156, 0.1129, 0.1047, 0.1166, 0.1164],\n        [0.1045, 0.1002, 0.1026, 0.1179, 0.1159, 0.1169, 0.1112, 0.1117, 0.1192],\n        [0.1027, 0.1008, 0.1055, 0.1126, 0.1164, 0.1140, 0.1163, 0.1157, 0.1160],\n        [0.1018, 0.0991, 0.1033, 0.1148, 0.1127, 0.1171, 0.1170, 0.1137, 0.1205]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1019, 0.0982, 0.1003, 0.1077, 0.1227, 0.1151, 0.1104, 0.1228, 0.1207],\n        [0.1053, 0.1015, 0.1042, 0.1171, 0.1091, 0.1144, 0.1156, 0.1158, 0.1171],\n        [0.1002, 0.0987, 0.1040, 0.1156, 0.1126, 0.1172, 0.1146, 0.1143, 0.1227],\n        [0.0995, 0.0968, 0.1012, 0.1168, 0.1164, 0.1140, 0.1161, 0.1142, 0.1250],\n        [0.0986, 0.0970, 0.0994, 0.1127, 0.1164, 0.1184, 0.1173, 0.1198, 0.1204]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1145, 0.1101, 0.1112, 0.1060, 0.1136, 0.1154, 0.1070, 0.1110, 0.1112],\n        [0.1123, 0.1089, 0.1096, 0.1177, 0.1097, 0.1133, 0.1061, 0.1100, 0.1124]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1135, 0.1081, 0.1139, 0.1097, 0.1081, 0.1113, 0.1106, 0.1133, 0.1114],\n        [0.1146, 0.1129, 0.1098, 0.1092, 0.1145, 0.1112, 0.1066, 0.1122, 0.1090],\n        [0.1083, 0.1031, 0.1071, 0.1110, 0.1134, 0.1177, 0.1163, 0.1145, 0.1085]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1153, 0.1114, 0.1129, 0.1087, 0.1128, 0.1098, 0.1067, 0.1121, 0.1103],\n        [0.1144, 0.1123, 0.1102, 0.1128, 0.1118, 0.1077, 0.1114, 0.1119, 0.1075],\n        [0.1068, 0.1028, 0.1090, 0.1139, 0.1202, 0.1128, 0.1099, 0.1129, 0.1118],\n        [0.1118, 0.1088, 0.1167, 0.1097, 0.1098, 0.1107, 0.1077, 0.1109, 0.1139]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1136, 0.1099, 0.1107, 0.1095, 0.1125, 0.1129, 0.1091, 0.1086, 0.1133],\n        [0.1132, 0.1112, 0.1083, 0.1150, 0.1082, 0.1140, 0.1119, 0.1079, 0.1102],\n        [0.1063, 0.1020, 0.1090, 0.1139, 0.1145, 0.1110, 0.1144, 0.1150, 0.1138],\n        [0.1073, 0.1039, 0.1116, 0.1143, 0.1210, 0.1133, 0.1082, 0.1102, 0.1102],\n        [0.1056, 0.1035, 0.1112, 0.1169, 0.1125, 0.1103, 0.1110, 0.1145, 0.1145]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:16:41 PM | Train: [14/50] Step 000/062 Loss 0.333 Prec@(1,5) (92.2%, 98.4%)\n11/23 09:21:04 PM | Train: [14/50] Step 050/062 Loss 0.327 Prec@(1,5) (87.7%, 99.4%)\n11/23 09:22:06 PM | Train: [14/50] Step 062/062 Loss 0.332 Prec@(1,5) (87.3%, 99.5%)\n11/23 09:22:06 PM | Train: [14/50] Final Prec@1 87.3000%\n11/23 09:22:07 PM | Valid: [14/50] Step 000/062 Loss 0.430 Prec@(1,5) (79.7%, 100.0%)\n11/23 09:22:19 PM | Valid: [14/50] Step 050/062 Loss 0.514 Prec@(1,5) (80.2%, 99.0%)\n11/23 09:22:22 PM | Valid: [14/50] Step 062/062 Loss 0.498 Prec@(1,5) (81.0%, 99.1%)\n11/23 09:22:22 PM | Valid: [14/50] Final Prec@1 81.0250%\n11/23 09:22:22 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('sep_conv_7x7', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1088, 0.1022, 0.1074, 0.1179, 0.1090, 0.1127, 0.1091, 0.1146, 0.1183],\n        [0.1078, 0.1014, 0.1065, 0.1134, 0.1164, 0.1150, 0.1143, 0.1125, 0.1127]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1107, 0.1031, 0.1076, 0.1189, 0.1153, 0.1083, 0.1107, 0.1078, 0.1176],\n        [0.1065, 0.1012, 0.1063, 0.1149, 0.1144, 0.1151, 0.1129, 0.1140, 0.1146],\n        [0.1035, 0.0993, 0.1068, 0.1130, 0.1159, 0.1097, 0.1180, 0.1155, 0.1183]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1092, 0.1027, 0.1064, 0.1133, 0.1161, 0.1128, 0.1048, 0.1175, 0.1173],\n        [0.1039, 0.0993, 0.1020, 0.1186, 0.1158, 0.1172, 0.1111, 0.1117, 0.1205],\n        [0.1014, 0.0993, 0.1048, 0.1129, 0.1174, 0.1144, 0.1169, 0.1155, 0.1173],\n        [0.1005, 0.0977, 0.1023, 0.1147, 0.1130, 0.1178, 0.1179, 0.1139, 0.1221]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1009, 0.0970, 0.0994, 0.1072, 0.1241, 0.1157, 0.1105, 0.1235, 0.1216],\n        [0.1044, 0.1002, 0.1034, 0.1180, 0.1094, 0.1145, 0.1164, 0.1160, 0.1177],\n        [0.0987, 0.0970, 0.1031, 0.1171, 0.1119, 0.1173, 0.1148, 0.1153, 0.1247],\n        [0.0983, 0.0954, 0.1003, 0.1174, 0.1171, 0.1138, 0.1161, 0.1146, 0.1272],\n        [0.0969, 0.0952, 0.0980, 0.1131, 0.1171, 0.1190, 0.1180, 0.1213, 0.1214]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1149, 0.1097, 0.1114, 0.1055, 0.1139, 0.1154, 0.1070, 0.1111, 0.1111],\n        [0.1128, 0.1090, 0.1099, 0.1179, 0.1087, 0.1134, 0.1060, 0.1097, 0.1126]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1139, 0.1076, 0.1141, 0.1093, 0.1084, 0.1115, 0.1102, 0.1134, 0.1116],\n        [0.1152, 0.1131, 0.1102, 0.1090, 0.1143, 0.1108, 0.1068, 0.1121, 0.1084],\n        [0.1085, 0.1021, 0.1068, 0.1110, 0.1136, 0.1180, 0.1169, 0.1150, 0.1081]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1159, 0.1112, 0.1125, 0.1087, 0.1128, 0.1096, 0.1067, 0.1120, 0.1104],\n        [0.1156, 0.1130, 0.1101, 0.1132, 0.1119, 0.1069, 0.1114, 0.1114, 0.1065],\n        [0.1070, 0.1018, 0.1090, 0.1142, 0.1207, 0.1130, 0.1097, 0.1127, 0.1119],\n        [0.1122, 0.1083, 0.1175, 0.1094, 0.1096, 0.1109, 0.1077, 0.1101, 0.1142]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1139, 0.1095, 0.1106, 0.1089, 0.1123, 0.1136, 0.1092, 0.1082, 0.1138],\n        [0.1136, 0.1113, 0.1082, 0.1154, 0.1085, 0.1145, 0.1113, 0.1072, 0.1100],\n        [0.1060, 0.1008, 0.1086, 0.1145, 0.1147, 0.1102, 0.1153, 0.1161, 0.1139],\n        [0.1070, 0.1029, 0.1114, 0.1152, 0.1217, 0.1136, 0.1080, 0.1100, 0.1103],\n        [0.1052, 0.1024, 0.1112, 0.1168, 0.1124, 0.1104, 0.1110, 0.1149, 0.1158]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:22:29 PM | Train: [15/50] Step 000/062 Loss 0.187 Prec@(1,5) (93.8%, 100.0%)\n11/23 09:26:52 PM | Train: [15/50] Step 050/062 Loss 0.288 Prec@(1,5) (89.6%, 99.5%)\n11/23 09:27:54 PM | Train: [15/50] Step 062/062 Loss 0.302 Prec@(1,5) (89.0%, 99.5%)\n11/23 09:27:54 PM | Train: [15/50] Final Prec@1 89.0250%\n11/23 09:27:55 PM | Valid: [15/50] Step 000/062 Loss 0.616 Prec@(1,5) (68.8%, 100.0%)\n11/23 09:28:08 PM | Valid: [15/50] Step 050/062 Loss 0.508 Prec@(1,5) (82.0%, 99.1%)\n11/23 09:28:10 PM | Valid: [15/50] Step 062/062 Loss 0.513 Prec@(1,5) (81.7%, 99.1%)\n11/23 09:28:10 PM | Valid: [15/50] Final Prec@1 81.6750%\n11/23 09:28:10 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1089, 0.1016, 0.1074, 0.1182, 0.1080, 0.1133, 0.1095, 0.1149, 0.1183],\n        [0.1067, 0.1000, 0.1057, 0.1141, 0.1170, 0.1151, 0.1156, 0.1124, 0.1134]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1105, 0.1026, 0.1077, 0.1195, 0.1153, 0.1082, 0.1105, 0.1079, 0.1179],\n        [0.1054, 0.1000, 0.1058, 0.1151, 0.1154, 0.1147, 0.1136, 0.1143, 0.1157],\n        [0.1027, 0.0983, 0.1068, 0.1126, 0.1163, 0.1104, 0.1188, 0.1149, 0.1192]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1088, 0.1020, 0.1061, 0.1134, 0.1167, 0.1131, 0.1041, 0.1180, 0.1179],\n        [0.1027, 0.0979, 0.1009, 0.1198, 0.1163, 0.1181, 0.1112, 0.1115, 0.1217],\n        [0.1002, 0.0980, 0.1042, 0.1133, 0.1185, 0.1140, 0.1174, 0.1158, 0.1185],\n        [0.0990, 0.0961, 0.1012, 0.1149, 0.1136, 0.1183, 0.1185, 0.1142, 0.1242]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0999, 0.0958, 0.0986, 0.1069, 0.1250, 0.1165, 0.1107, 0.1242, 0.1224],\n        [0.1031, 0.0987, 0.1021, 0.1187, 0.1095, 0.1151, 0.1171, 0.1163, 0.1193],\n        [0.0973, 0.0955, 0.1025, 0.1172, 0.1119, 0.1181, 0.1153, 0.1153, 0.1269],\n        [0.0967, 0.0936, 0.0992, 0.1177, 0.1181, 0.1140, 0.1166, 0.1143, 0.1298],\n        [0.0950, 0.0932, 0.0963, 0.1129, 0.1175, 0.1196, 0.1189, 0.1234, 0.1231]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1158, 0.1105, 0.1112, 0.1050, 0.1138, 0.1152, 0.1069, 0.1103, 0.1113],\n        [0.1131, 0.1086, 0.1102, 0.1181, 0.1089, 0.1134, 0.1055, 0.1098, 0.1123]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1141, 0.1076, 0.1147, 0.1090, 0.1078, 0.1123, 0.1094, 0.1137, 0.1113],\n        [0.1158, 0.1135, 0.1101, 0.1085, 0.1147, 0.1111, 0.1064, 0.1121, 0.1078],\n        [0.1076, 0.1010, 0.1063, 0.1111, 0.1136, 0.1187, 0.1178, 0.1158, 0.1081]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1166, 0.1118, 0.1119, 0.1083, 0.1129, 0.1098, 0.1064, 0.1122, 0.1101],\n        [0.1162, 0.1131, 0.1101, 0.1133, 0.1123, 0.1058, 0.1116, 0.1121, 0.1056],\n        [0.1062, 0.1009, 0.1092, 0.1154, 0.1206, 0.1125, 0.1103, 0.1125, 0.1125],\n        [0.1121, 0.1083, 0.1187, 0.1085, 0.1097, 0.1106, 0.1074, 0.1100, 0.1145]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1145, 0.1100, 0.1104, 0.1089, 0.1124, 0.1133, 0.1087, 0.1076, 0.1142],\n        [0.1145, 0.1120, 0.1074, 0.1159, 0.1082, 0.1142, 0.1108, 0.1070, 0.1099],\n        [0.1047, 0.0995, 0.1081, 0.1149, 0.1146, 0.1103, 0.1167, 0.1169, 0.1144],\n        [0.1062, 0.1023, 0.1118, 0.1154, 0.1223, 0.1132, 0.1080, 0.1101, 0.1106],\n        [0.1040, 0.1014, 0.1113, 0.1167, 0.1125, 0.1103, 0.1110, 0.1158, 0.1171]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:28:18 PM | Train: [16/50] Step 000/062 Loss 0.307 Prec@(1,5) (89.1%, 100.0%)\n11/23 09:32:40 PM | Train: [16/50] Step 050/062 Loss 0.273 Prec@(1,5) (89.4%, 99.7%)\n11/23 09:33:42 PM | Train: [16/50] Step 062/062 Loss 0.271 Prec@(1,5) (89.5%, 99.7%)\n11/23 09:33:42 PM | Train: [16/50] Final Prec@1 89.5250%\n11/23 09:33:43 PM | Valid: [16/50] Step 000/062 Loss 0.484 Prec@(1,5) (82.8%, 100.0%)\n11/23 09:33:55 PM | Valid: [16/50] Step 050/062 Loss 0.574 Prec@(1,5) (80.5%, 99.0%)\n11/23 09:33:58 PM | Valid: [16/50] Step 062/062 Loss 0.568 Prec@(1,5) (80.7%, 99.1%)\n11/23 09:33:58 PM | Valid: [16/50] Final Prec@1 80.6500%\n11/23 09:33:58 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('sep_conv_5x5', 2), ('skip_connect', 3)], [('sep_conv_5x5', 3), ('dil_conv_3x3', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1084, 0.1008, 0.1069, 0.1187, 0.1081, 0.1137, 0.1090, 0.1161, 0.1183],\n        [0.1058, 0.0989, 0.1050, 0.1146, 0.1172, 0.1157, 0.1158, 0.1129, 0.1142]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1100, 0.1019, 0.1073, 0.1201, 0.1159, 0.1080, 0.1102, 0.1078, 0.1188],\n        [0.1047, 0.0992, 0.1053, 0.1155, 0.1156, 0.1149, 0.1139, 0.1146, 0.1164],\n        [0.1023, 0.0976, 0.1065, 0.1118, 0.1168, 0.1108, 0.1191, 0.1154, 0.1196]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1081, 0.1010, 0.1053, 0.1138, 0.1173, 0.1134, 0.1036, 0.1188, 0.1187],\n        [0.1015, 0.0965, 0.0997, 0.1206, 0.1166, 0.1182, 0.1119, 0.1119, 0.1230],\n        [0.0993, 0.0970, 0.1035, 0.1129, 0.1198, 0.1143, 0.1179, 0.1159, 0.1193],\n        [0.0978, 0.0946, 0.1000, 0.1152, 0.1139, 0.1193, 0.1191, 0.1142, 0.1260]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0990, 0.0947, 0.0977, 0.1070, 0.1251, 0.1174, 0.1104, 0.1251, 0.1235],\n        [0.1024, 0.0976, 0.1014, 0.1192, 0.1086, 0.1156, 0.1180, 0.1166, 0.1207],\n        [0.0962, 0.0941, 0.1014, 0.1176, 0.1123, 0.1186, 0.1165, 0.1156, 0.1277],\n        [0.0954, 0.0920, 0.0980, 0.1183, 0.1178, 0.1150, 0.1169, 0.1150, 0.1315],\n        [0.0936, 0.0917, 0.0951, 0.1129, 0.1185, 0.1201, 0.1196, 0.1242, 0.1243]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1165, 0.1107, 0.1111, 0.1041, 0.1137, 0.1154, 0.1071, 0.1104, 0.1108],\n        [0.1134, 0.1084, 0.1107, 0.1182, 0.1080, 0.1135, 0.1052, 0.1098, 0.1128]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1146, 0.1074, 0.1153, 0.1085, 0.1071, 0.1128, 0.1089, 0.1143, 0.1111],\n        [0.1167, 0.1142, 0.1100, 0.1084, 0.1152, 0.1110, 0.1054, 0.1117, 0.1073],\n        [0.1079, 0.1005, 0.1060, 0.1111, 0.1136, 0.1198, 0.1176, 0.1161, 0.1075]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1173, 0.1121, 0.1117, 0.1078, 0.1130, 0.1102, 0.1053, 0.1127, 0.1099],\n        [0.1167, 0.1131, 0.1102, 0.1135, 0.1123, 0.1048, 0.1119, 0.1125, 0.1050],\n        [0.1057, 0.1000, 0.1093, 0.1162, 0.1208, 0.1130, 0.1100, 0.1122, 0.1127],\n        [0.1123, 0.1081, 0.1194, 0.1077, 0.1099, 0.1107, 0.1074, 0.1094, 0.1151]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1148, 0.1099, 0.1101, 0.1094, 0.1121, 0.1138, 0.1086, 0.1065, 0.1147],\n        [0.1149, 0.1121, 0.1071, 0.1159, 0.1082, 0.1143, 0.1105, 0.1070, 0.1100],\n        [0.1041, 0.0984, 0.1078, 0.1150, 0.1157, 0.1096, 0.1175, 0.1175, 0.1144],\n        [0.1058, 0.1016, 0.1118, 0.1158, 0.1234, 0.1140, 0.1071, 0.1099, 0.1106],\n        [0.1035, 0.1005, 0.1113, 0.1160, 0.1129, 0.1101, 0.1108, 0.1163, 0.1187]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:34:05 PM | Train: [17/50] Step 000/062 Loss 0.301 Prec@(1,5) (87.5%, 100.0%)\n11/23 09:38:29 PM | Train: [17/50] Step 050/062 Loss 0.272 Prec@(1,5) (90.0%, 99.7%)\n11/23 09:39:31 PM | Train: [17/50] Step 062/062 Loss 0.271 Prec@(1,5) (89.8%, 99.7%)\n11/23 09:39:32 PM | Train: [17/50] Final Prec@1 89.8250%\n11/23 09:39:32 PM | Valid: [17/50] Step 000/062 Loss 0.553 Prec@(1,5) (81.2%, 98.4%)\n11/23 09:39:45 PM | Valid: [17/50] Step 050/062 Loss 0.447 Prec@(1,5) (83.8%, 99.1%)\n11/23 09:39:48 PM | Valid: [17/50] Step 062/062 Loss 0.459 Prec@(1,5) (83.7%, 99.2%)\n11/23 09:39:48 PM | Valid: [17/50] Final Prec@1 83.6500%\n11/23 09:39:48 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_7x7', 3)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1085, 0.1004, 0.1071, 0.1191, 0.1073, 0.1139, 0.1087, 0.1164, 0.1185],\n        [0.1055, 0.0980, 0.1048, 0.1147, 0.1176, 0.1159, 0.1164, 0.1123, 0.1148]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1097, 0.1010, 0.1068, 0.1205, 0.1167, 0.1078, 0.1098, 0.1081, 0.1195],\n        [0.1045, 0.0982, 0.1051, 0.1154, 0.1158, 0.1152, 0.1139, 0.1152, 0.1167],\n        [0.1016, 0.0965, 0.1062, 0.1113, 0.1174, 0.1112, 0.1196, 0.1159, 0.1204]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1077, 0.1001, 0.1049, 0.1142, 0.1181, 0.1132, 0.1033, 0.1192, 0.1193],\n        [0.1008, 0.0953, 0.0989, 0.1219, 0.1165, 0.1184, 0.1124, 0.1117, 0.1243],\n        [0.0982, 0.0956, 0.1028, 0.1133, 0.1204, 0.1143, 0.1188, 0.1161, 0.1205],\n        [0.0972, 0.0934, 0.0993, 0.1154, 0.1136, 0.1202, 0.1193, 0.1142, 0.1273]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0984, 0.0938, 0.0971, 0.1066, 0.1255, 0.1173, 0.1108, 0.1257, 0.1247],\n        [0.1015, 0.0961, 0.1004, 0.1201, 0.1088, 0.1159, 0.1186, 0.1169, 0.1217],\n        [0.0950, 0.0926, 0.1005, 0.1182, 0.1121, 0.1193, 0.1169, 0.1161, 0.1294],\n        [0.0944, 0.0905, 0.0970, 0.1193, 0.1184, 0.1154, 0.1168, 0.1148, 0.1334],\n        [0.0922, 0.0901, 0.0937, 0.1134, 0.1196, 0.1206, 0.1197, 0.1250, 0.1257]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1171, 0.1107, 0.1115, 0.1037, 0.1136, 0.1153, 0.1072, 0.1100, 0.1108],\n        [0.1140, 0.1083, 0.1108, 0.1185, 0.1073, 0.1137, 0.1050, 0.1097, 0.1128]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1153, 0.1073, 0.1154, 0.1078, 0.1073, 0.1132, 0.1086, 0.1141, 0.1110],\n        [0.1177, 0.1148, 0.1101, 0.1082, 0.1154, 0.1103, 0.1051, 0.1115, 0.1069],\n        [0.1079, 0.0996, 0.1056, 0.1111, 0.1143, 0.1197, 0.1184, 0.1164, 0.1070]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1183, 0.1128, 0.1108, 0.1075, 0.1127, 0.1103, 0.1055, 0.1127, 0.1093],\n        [0.1177, 0.1135, 0.1106, 0.1132, 0.1123, 0.1041, 0.1115, 0.1128, 0.1043],\n        [0.1055, 0.0991, 0.1096, 0.1168, 0.1209, 0.1128, 0.1102, 0.1119, 0.1132],\n        [0.1122, 0.1077, 0.1203, 0.1070, 0.1101, 0.1106, 0.1072, 0.1092, 0.1158]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1157, 0.1104, 0.1097, 0.1088, 0.1118, 0.1144, 0.1081, 0.1063, 0.1147],\n        [0.1156, 0.1123, 0.1069, 0.1161, 0.1085, 0.1139, 0.1100, 0.1071, 0.1095],\n        [0.1038, 0.0973, 0.1077, 0.1150, 0.1162, 0.1085, 0.1180, 0.1184, 0.1151],\n        [0.1055, 0.1011, 0.1124, 0.1163, 0.1241, 0.1137, 0.1070, 0.1090, 0.1108],\n        [0.1030, 0.0997, 0.1118, 0.1160, 0.1120, 0.1098, 0.1106, 0.1167, 0.1203]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:39:55 PM | Train: [18/50] Step 000/062 Loss 0.283 Prec@(1,5) (89.1%, 100.0%)\n11/23 09:44:19 PM | Train: [18/50] Step 050/062 Loss 0.227 Prec@(1,5) (91.4%, 99.7%)\n11/23 09:45:21 PM | Train: [18/50] Step 062/062 Loss 0.234 Prec@(1,5) (91.2%, 99.6%)\n11/23 09:45:21 PM | Train: [18/50] Final Prec@1 91.2500%\n11/23 09:45:21 PM | Valid: [18/50] Step 000/062 Loss 0.227 Prec@(1,5) (93.8%, 100.0%)\n11/23 09:45:34 PM | Valid: [18/50] Step 050/062 Loss 0.560 Prec@(1,5) (81.4%, 99.1%)\n11/23 09:45:37 PM | Valid: [18/50] Step 062/062 Loss 0.550 Prec@(1,5) (81.7%, 99.1%)\n11/23 09:45:37 PM | Valid: [18/50] Final Prec@1 81.6500%\n11/23 09:45:37 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_7x7', 3)], [('sep_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1084, 0.0998, 0.1070, 0.1195, 0.1066, 0.1143, 0.1092, 0.1164, 0.1189],\n        [0.1054, 0.0974, 0.1045, 0.1145, 0.1172, 0.1167, 0.1160, 0.1126, 0.1157]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1087, 0.0997, 0.1058, 0.1214, 0.1179, 0.1080, 0.1098, 0.1087, 0.1202],\n        [0.1035, 0.0971, 0.1041, 0.1157, 0.1161, 0.1152, 0.1140, 0.1168, 0.1174],\n        [0.1002, 0.0951, 0.1053, 0.1118, 0.1174, 0.1119, 0.1201, 0.1166, 0.1215]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1075, 0.0993, 0.1045, 0.1148, 0.1188, 0.1131, 0.1024, 0.1194, 0.1202],\n        [0.1000, 0.0942, 0.0978, 0.1228, 0.1163, 0.1191, 0.1127, 0.1114, 0.1257],\n        [0.0971, 0.0945, 0.1020, 0.1125, 0.1218, 0.1148, 0.1196, 0.1164, 0.1213],\n        [0.0961, 0.0921, 0.0984, 0.1152, 0.1139, 0.1219, 0.1198, 0.1138, 0.1289]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0975, 0.0926, 0.0962, 0.1068, 0.1262, 0.1176, 0.1110, 0.1260, 0.1261],\n        [0.1011, 0.0953, 0.0998, 0.1209, 0.1084, 0.1159, 0.1191, 0.1171, 0.1225],\n        [0.0936, 0.0912, 0.0993, 0.1187, 0.1126, 0.1201, 0.1178, 0.1162, 0.1305],\n        [0.0933, 0.0892, 0.0961, 0.1200, 0.1183, 0.1155, 0.1172, 0.1153, 0.1353],\n        [0.0910, 0.0888, 0.0926, 0.1137, 0.1195, 0.1208, 0.1208, 0.1259, 0.1270]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1173, 0.1108, 0.1111, 0.1032, 0.1140, 0.1160, 0.1069, 0.1097, 0.1110],\n        [0.1147, 0.1086, 0.1106, 0.1186, 0.1065, 0.1134, 0.1051, 0.1098, 0.1126]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1152, 0.1072, 0.1150, 0.1074, 0.1068, 0.1142, 0.1088, 0.1149, 0.1106],\n        [0.1187, 0.1157, 0.1099, 0.1081, 0.1154, 0.1099, 0.1047, 0.1108, 0.1066],\n        [0.1078, 0.0988, 0.1050, 0.1107, 0.1150, 0.1202, 0.1188, 0.1175, 0.1064]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1186, 0.1131, 0.1107, 0.1070, 0.1128, 0.1103, 0.1052, 0.1132, 0.1092],\n        [0.1183, 0.1138, 0.1108, 0.1132, 0.1121, 0.1029, 0.1117, 0.1133, 0.1039],\n        [0.1050, 0.0981, 0.1091, 0.1176, 0.1209, 0.1135, 0.1103, 0.1125, 0.1128],\n        [0.1121, 0.1074, 0.1212, 0.1065, 0.1101, 0.1103, 0.1067, 0.1091, 0.1166]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1157, 0.1105, 0.1099, 0.1085, 0.1112, 0.1147, 0.1078, 0.1062, 0.1154],\n        [0.1163, 0.1130, 0.1065, 0.1172, 0.1073, 0.1135, 0.1102, 0.1068, 0.1093],\n        [0.1033, 0.0963, 0.1076, 0.1156, 0.1156, 0.1084, 0.1185, 0.1189, 0.1158],\n        [0.1049, 0.1007, 0.1126, 0.1163, 0.1247, 0.1140, 0.1070, 0.1091, 0.1109],\n        [0.1025, 0.0993, 0.1121, 0.1158, 0.1120, 0.1096, 0.1098, 0.1177, 0.1212]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:45:45 PM | Train: [19/50] Step 000/062 Loss 0.139 Prec@(1,5) (98.4%, 100.0%)\n11/23 09:50:08 PM | Train: [19/50] Step 050/062 Loss 0.221 Prec@(1,5) (91.9%, 99.8%)\n11/23 09:51:09 PM | Train: [19/50] Step 062/062 Loss 0.220 Prec@(1,5) (91.7%, 99.8%)\n11/23 09:51:10 PM | Train: [19/50] Final Prec@1 91.6750%\n11/23 09:51:10 PM | Valid: [19/50] Step 000/062 Loss 0.517 Prec@(1,5) (82.8%, 98.4%)\n11/23 09:51:23 PM | Valid: [19/50] Step 050/062 Loss 0.504 Prec@(1,5) (83.4%, 99.2%)\n11/23 09:51:25 PM | Valid: [19/50] Step 062/062 Loss 0.492 Prec@(1,5) (83.7%, 99.4%)\n11/23 09:51:26 PM | Valid: [19/50] Final Prec@1 83.6750%\n11/23 09:51:26 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('sep_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 1), ('max_pool_3x3', 0)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1090, 0.0997, 0.1075, 0.1195, 0.1057, 0.1139, 0.1094, 0.1166, 0.1187],\n        [0.1049, 0.0963, 0.1040, 0.1146, 0.1177, 0.1165, 0.1165, 0.1125, 0.1170]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1083, 0.0989, 0.1054, 0.1219, 0.1188, 0.1080, 0.1093, 0.1088, 0.1208],\n        [0.1025, 0.0958, 0.1034, 0.1155, 0.1165, 0.1162, 0.1140, 0.1175, 0.1187],\n        [0.0993, 0.0939, 0.1052, 0.1113, 0.1182, 0.1118, 0.1203, 0.1170, 0.1230]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1074, 0.0984, 0.1040, 0.1153, 0.1192, 0.1124, 0.1021, 0.1198, 0.1214],\n        [0.0993, 0.0930, 0.0970, 0.1238, 0.1164, 0.1193, 0.1134, 0.1106, 0.1272],\n        [0.0961, 0.0930, 0.1015, 0.1124, 0.1233, 0.1148, 0.1199, 0.1161, 0.1229],\n        [0.0951, 0.0907, 0.0976, 0.1150, 0.1145, 0.1226, 0.1197, 0.1135, 0.1313]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0971, 0.0917, 0.0955, 0.1066, 0.1268, 0.1181, 0.1111, 0.1261, 0.1271],\n        [0.1001, 0.0938, 0.0985, 0.1221, 0.1082, 0.1172, 0.1197, 0.1172, 0.1232],\n        [0.0923, 0.0896, 0.0984, 0.1180, 0.1129, 0.1204, 0.1191, 0.1169, 0.1324],\n        [0.0921, 0.0877, 0.0951, 0.1203, 0.1187, 0.1153, 0.1170, 0.1159, 0.1379],\n        [0.0895, 0.0869, 0.0909, 0.1140, 0.1207, 0.1208, 0.1216, 0.1267, 0.1288]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1187, 0.1115, 0.1110, 0.1019, 0.1140, 0.1163, 0.1066, 0.1097, 0.1104],\n        [0.1147, 0.1080, 0.1107, 0.1188, 0.1062, 0.1137, 0.1049, 0.1099, 0.1132]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1167, 0.1078, 0.1148, 0.1072, 0.1059, 0.1143, 0.1084, 0.1151, 0.1097],\n        [0.1196, 0.1162, 0.1106, 0.1078, 0.1148, 0.1096, 0.1042, 0.1105, 0.1067],\n        [0.1073, 0.0981, 0.1046, 0.1101, 0.1160, 0.1203, 0.1196, 0.1181, 0.1059]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1203, 0.1141, 0.1100, 0.1067, 0.1122, 0.1095, 0.1056, 0.1134, 0.1082],\n        [0.1188, 0.1137, 0.1111, 0.1135, 0.1122, 0.1023, 0.1114, 0.1137, 0.1033],\n        [0.1048, 0.0974, 0.1098, 0.1184, 0.1209, 0.1130, 0.1094, 0.1129, 0.1134],\n        [0.1119, 0.1072, 0.1221, 0.1062, 0.1099, 0.1099, 0.1069, 0.1089, 0.1169]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1171, 0.1112, 0.1093, 0.1081, 0.1109, 0.1153, 0.1075, 0.1054, 0.1153],\n        [0.1166, 0.1132, 0.1060, 0.1173, 0.1069, 0.1134, 0.1101, 0.1070, 0.1095],\n        [0.1029, 0.0956, 0.1082, 0.1159, 0.1155, 0.1074, 0.1182, 0.1200, 0.1165],\n        [0.1038, 0.0999, 0.1129, 0.1167, 0.1252, 0.1137, 0.1075, 0.1086, 0.1117],\n        [0.1018, 0.0986, 0.1127, 0.1152, 0.1115, 0.1093, 0.1096, 0.1184, 0.1229]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:51:33 PM | Train: [20/50] Step 000/062 Loss 0.199 Prec@(1,5) (95.3%, 100.0%)\n11/23 09:55:57 PM | Train: [20/50] Step 050/062 Loss 0.221 Prec@(1,5) (92.2%, 99.8%)\n11/23 09:56:58 PM | Train: [20/50] Step 062/062 Loss 0.211 Prec@(1,5) (92.7%, 99.9%)\n11/23 09:56:58 PM | Train: [20/50] Final Prec@1 92.7000%\n11/23 09:56:59 PM | Valid: [20/50] Step 000/062 Loss 0.373 Prec@(1,5) (81.2%, 98.4%)\n11/23 09:57:12 PM | Valid: [20/50] Step 050/062 Loss 0.489 Prec@(1,5) (84.4%, 98.8%)\n11/23 09:57:15 PM | Valid: [20/50] Step 062/062 Loss 0.487 Prec@(1,5) (84.5%, 98.9%)\n11/23 09:57:15 PM | Valid: [20/50] Final Prec@1 84.4500%\n11/23 09:57:15 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('sep_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1096, 0.0997, 0.1083, 0.1195, 0.1048, 0.1140, 0.1091, 0.1165, 0.1184],\n        [0.1039, 0.0951, 0.1033, 0.1149, 0.1184, 0.1163, 0.1167, 0.1132, 0.1182]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1082, 0.0982, 0.1053, 0.1225, 0.1191, 0.1079, 0.1087, 0.1089, 0.1212],\n        [0.1015, 0.0945, 0.1027, 0.1158, 0.1172, 0.1162, 0.1132, 0.1185, 0.1205],\n        [0.0982, 0.0927, 0.1051, 0.1116, 0.1185, 0.1112, 0.1208, 0.1172, 0.1247]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1074, 0.0977, 0.1039, 0.1153, 0.1198, 0.1111, 0.1018, 0.1206, 0.1224],\n        [0.0980, 0.0913, 0.0956, 0.1246, 0.1167, 0.1197, 0.1142, 0.1106, 0.1294],\n        [0.0949, 0.0916, 0.1005, 0.1117, 0.1247, 0.1147, 0.1216, 0.1164, 0.1240],\n        [0.0934, 0.0887, 0.0960, 0.1153, 0.1150, 0.1236, 0.1205, 0.1133, 0.1341]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0969, 0.0910, 0.0952, 0.1057, 0.1270, 0.1184, 0.1112, 0.1267, 0.1280],\n        [0.0988, 0.0922, 0.0972, 0.1235, 0.1085, 0.1175, 0.1204, 0.1174, 0.1244],\n        [0.0913, 0.0883, 0.0975, 0.1185, 0.1132, 0.1212, 0.1194, 0.1167, 0.1339],\n        [0.0908, 0.0861, 0.0940, 0.1207, 0.1193, 0.1151, 0.1175, 0.1161, 0.1404],\n        [0.0881, 0.0855, 0.0897, 0.1146, 0.1214, 0.1205, 0.1222, 0.1270, 0.1310]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1198, 0.1119, 0.1110, 0.1014, 0.1142, 0.1159, 0.1063, 0.1095, 0.1100],\n        [0.1151, 0.1080, 0.1105, 0.1188, 0.1059, 0.1138, 0.1045, 0.1096, 0.1137]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1179, 0.1081, 0.1157, 0.1069, 0.1049, 0.1145, 0.1079, 0.1151, 0.1090],\n        [0.1196, 0.1162, 0.1106, 0.1073, 0.1154, 0.1093, 0.1044, 0.1106, 0.1066],\n        [0.1066, 0.0971, 0.1039, 0.1098, 0.1171, 0.1210, 0.1202, 0.1191, 0.1051]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1214, 0.1147, 0.1100, 0.1059, 0.1120, 0.1093, 0.1050, 0.1142, 0.1076],\n        [0.1190, 0.1139, 0.1117, 0.1133, 0.1121, 0.1015, 0.1113, 0.1144, 0.1028],\n        [0.1039, 0.0966, 0.1096, 0.1191, 0.1216, 0.1130, 0.1098, 0.1129, 0.1135],\n        [0.1116, 0.1071, 0.1233, 0.1059, 0.1094, 0.1102, 0.1066, 0.1085, 0.1173]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1178, 0.1115, 0.1091, 0.1078, 0.1112, 0.1156, 0.1069, 0.1051, 0.1151],\n        [0.1170, 0.1134, 0.1054, 0.1180, 0.1071, 0.1133, 0.1096, 0.1064, 0.1098],\n        [0.1015, 0.0943, 0.1083, 0.1163, 0.1161, 0.1070, 0.1185, 0.1204, 0.1174],\n        [0.1030, 0.0994, 0.1133, 0.1175, 0.1250, 0.1141, 0.1078, 0.1081, 0.1118],\n        [0.1005, 0.0976, 0.1127, 0.1148, 0.1112, 0.1090, 0.1100, 0.1194, 0.1248]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 09:57:22 PM | Train: [21/50] Step 000/062 Loss 0.210 Prec@(1,5) (92.2%, 100.0%)\n11/23 10:01:46 PM | Train: [21/50] Step 050/062 Loss 0.180 Prec@(1,5) (93.8%, 99.8%)\n11/23 10:02:47 PM | Train: [21/50] Step 062/062 Loss 0.177 Prec@(1,5) (93.8%, 99.8%)\n11/23 10:02:47 PM | Train: [21/50] Final Prec@1 93.8000%\n11/23 10:02:48 PM | Valid: [21/50] Step 000/062 Loss 0.321 Prec@(1,5) (92.2%, 100.0%)\n11/23 10:03:00 PM | Valid: [21/50] Step 050/062 Loss 0.485 Prec@(1,5) (85.3%, 99.3%)\n11/23 10:03:03 PM | Valid: [21/50] Step 062/062 Loss 0.472 Prec@(1,5) (85.4%, 99.4%)\n11/23 10:03:03 PM | Valid: [21/50] Final Prec@1 85.3500%\n11/23 10:03:03 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_7x7', 3)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1099, 0.0997, 0.1090, 0.1202, 0.1040, 0.1141, 0.1090, 0.1164, 0.1178],\n        [0.1030, 0.0941, 0.1029, 0.1155, 0.1179, 0.1164, 0.1169, 0.1134, 0.1199]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1083, 0.0979, 0.1056, 0.1225, 0.1201, 0.1075, 0.1078, 0.1088, 0.1215],\n        [0.1002, 0.0934, 0.1020, 0.1159, 0.1176, 0.1162, 0.1140, 0.1185, 0.1223],\n        [0.0965, 0.0911, 0.1045, 0.1119, 0.1184, 0.1116, 0.1215, 0.1179, 0.1266]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1077, 0.0976, 0.1044, 0.1156, 0.1200, 0.1101, 0.1015, 0.1205, 0.1225],\n        [0.0967, 0.0901, 0.0947, 0.1255, 0.1171, 0.1197, 0.1147, 0.1107, 0.1309],\n        [0.0938, 0.0904, 0.0999, 0.1120, 0.1252, 0.1146, 0.1223, 0.1167, 0.1250],\n        [0.0925, 0.0876, 0.0956, 0.1157, 0.1145, 0.1244, 0.1207, 0.1127, 0.1364]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0966, 0.0905, 0.0950, 0.1051, 0.1273, 0.1186, 0.1112, 0.1273, 0.1285],\n        [0.0977, 0.0912, 0.0965, 0.1245, 0.1083, 0.1180, 0.1208, 0.1173, 0.1258],\n        [0.0896, 0.0867, 0.0965, 0.1192, 0.1135, 0.1215, 0.1202, 0.1170, 0.1357],\n        [0.0895, 0.0848, 0.0933, 0.1214, 0.1193, 0.1145, 0.1181, 0.1163, 0.1430],\n        [0.0868, 0.0842, 0.0888, 0.1141, 0.1220, 0.1201, 0.1227, 0.1280, 0.1333]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1206, 0.1123, 0.1113, 0.1005, 0.1142, 0.1157, 0.1062, 0.1094, 0.1098],\n        [0.1161, 0.1085, 0.1108, 0.1193, 0.1050, 0.1132, 0.1043, 0.1090, 0.1138]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1185, 0.1083, 0.1163, 0.1064, 0.1046, 0.1147, 0.1077, 0.1149, 0.1085],\n        [0.1203, 0.1169, 0.1109, 0.1073, 0.1156, 0.1086, 0.1040, 0.1103, 0.1061],\n        [0.1058, 0.0961, 0.1035, 0.1099, 0.1178, 0.1216, 0.1206, 0.1196, 0.1052]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1225, 0.1152, 0.1100, 0.1055, 0.1113, 0.1092, 0.1048, 0.1145, 0.1069],\n        [0.1202, 0.1148, 0.1119, 0.1124, 0.1117, 0.1007, 0.1111, 0.1148, 0.1022],\n        [0.1033, 0.0959, 0.1103, 0.1193, 0.1217, 0.1126, 0.1100, 0.1129, 0.1140],\n        [0.1114, 0.1072, 0.1250, 0.1051, 0.1087, 0.1102, 0.1062, 0.1082, 0.1180]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1189, 0.1121, 0.1084, 0.1077, 0.1110, 0.1158, 0.1065, 0.1045, 0.1151],\n        [0.1181, 0.1144, 0.1049, 0.1185, 0.1067, 0.1127, 0.1091, 0.1063, 0.1094],\n        [0.1007, 0.0935, 0.1087, 0.1165, 0.1159, 0.1064, 0.1191, 0.1209, 0.1182],\n        [0.1028, 0.0993, 0.1142, 0.1176, 0.1254, 0.1142, 0.1075, 0.1074, 0.1116],\n        [0.0999, 0.0975, 0.1137, 0.1147, 0.1114, 0.1079, 0.1094, 0.1194, 0.1261]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:03:10 PM | Train: [22/50] Step 000/062 Loss 0.164 Prec@(1,5) (92.2%, 100.0%)\n11/23 10:07:33 PM | Train: [22/50] Step 050/062 Loss 0.152 Prec@(1,5) (94.2%, 100.0%)\n11/23 10:08:35 PM | Train: [22/50] Step 062/062 Loss 0.159 Prec@(1,5) (94.1%, 99.9%)\n11/23 10:08:35 PM | Train: [22/50] Final Prec@1 94.0750%\n11/23 10:08:36 PM | Valid: [22/50] Step 000/062 Loss 0.731 Prec@(1,5) (87.5%, 100.0%)\n11/23 10:08:48 PM | Valid: [22/50] Step 050/062 Loss 0.511 Prec@(1,5) (85.3%, 99.1%)\n11/23 10:08:51 PM | Valid: [22/50] Step 062/062 Loss 0.519 Prec@(1,5) (85.1%, 98.9%)\n11/23 10:08:51 PM | Valid: [22/50] Final Prec@1 85.0750%\n11/23 10:08:51 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('sep_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_7x7', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1103, 0.0995, 0.1096, 0.1207, 0.1030, 0.1141, 0.1091, 0.1162, 0.1176],\n        [0.1025, 0.0931, 0.1025, 0.1159, 0.1181, 0.1159, 0.1166, 0.1139, 0.1214]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1082, 0.0971, 0.1053, 0.1228, 0.1207, 0.1070, 0.1076, 0.1095, 0.1217],\n        [0.0996, 0.0923, 0.1015, 0.1157, 0.1177, 0.1163, 0.1144, 0.1189, 0.1237],\n        [0.0954, 0.0897, 0.1042, 0.1110, 0.1194, 0.1123, 0.1218, 0.1178, 0.1284]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1078, 0.0968, 0.1040, 0.1158, 0.1209, 0.1093, 0.1012, 0.1215, 0.1227],\n        [0.0960, 0.0888, 0.0937, 0.1259, 0.1168, 0.1198, 0.1154, 0.1110, 0.1326],\n        [0.0927, 0.0890, 0.0993, 0.1117, 0.1263, 0.1152, 0.1231, 0.1163, 0.1265],\n        [0.0912, 0.0861, 0.0946, 0.1160, 0.1150, 0.1251, 0.1206, 0.1124, 0.1389]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0963, 0.0897, 0.0945, 0.1049, 0.1285, 0.1187, 0.1113, 0.1271, 0.1292],\n        [0.0968, 0.0900, 0.0957, 0.1250, 0.1084, 0.1177, 0.1214, 0.1178, 0.1272],\n        [0.0883, 0.0852, 0.0955, 0.1193, 0.1133, 0.1222, 0.1208, 0.1174, 0.1380],\n        [0.0881, 0.0835, 0.0925, 0.1216, 0.1196, 0.1140, 0.1189, 0.1162, 0.1456],\n        [0.0855, 0.0828, 0.0877, 0.1138, 0.1225, 0.1202, 0.1227, 0.1291, 0.1357]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1217, 0.1126, 0.1116, 0.1000, 0.1138, 0.1160, 0.1055, 0.1094, 0.1094],\n        [0.1166, 0.1085, 0.1108, 0.1194, 0.1046, 0.1129, 0.1043, 0.1087, 0.1142]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1193, 0.1084, 0.1170, 0.1058, 0.1039, 0.1152, 0.1073, 0.1155, 0.1077],\n        [0.1211, 0.1174, 0.1109, 0.1075, 0.1152, 0.1082, 0.1036, 0.1103, 0.1057],\n        [0.1053, 0.0955, 0.1037, 0.1090, 0.1175, 0.1220, 0.1208, 0.1208, 0.1052]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1234, 0.1155, 0.1103, 0.1055, 0.1107, 0.1092, 0.1045, 0.1146, 0.1063],\n        [0.1210, 0.1152, 0.1119, 0.1120, 0.1117, 0.0999, 0.1112, 0.1151, 0.1019],\n        [0.1026, 0.0950, 0.1103, 0.1202, 0.1223, 0.1122, 0.1098, 0.1132, 0.1143],\n        [0.1111, 0.1070, 0.1262, 0.1044, 0.1081, 0.1100, 0.1062, 0.1082, 0.1186]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1200, 0.1128, 0.1080, 0.1075, 0.1104, 0.1154, 0.1061, 0.1045, 0.1154],\n        [0.1187, 0.1149, 0.1044, 0.1191, 0.1068, 0.1128, 0.1090, 0.1055, 0.1089],\n        [0.0998, 0.0928, 0.1087, 0.1172, 0.1157, 0.1073, 0.1192, 0.1215, 0.1178],\n        [0.1020, 0.0990, 0.1151, 0.1183, 0.1255, 0.1140, 0.1070, 0.1069, 0.1122],\n        [0.0992, 0.0971, 0.1148, 0.1144, 0.1110, 0.1073, 0.1089, 0.1194, 0.1279]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:08:58 PM | Train: [23/50] Step 000/062 Loss 0.077 Prec@(1,5) (95.3%, 100.0%)\n11/23 10:13:22 PM | Train: [23/50] Step 050/062 Loss 0.166 Prec@(1,5) (93.6%, 99.9%)\n11/23 10:14:24 PM | Train: [23/50] Step 062/062 Loss 0.165 Prec@(1,5) (93.7%, 99.9%)\n11/23 10:14:24 PM | Train: [23/50] Final Prec@1 93.7000%\n11/23 10:14:25 PM | Valid: [23/50] Step 000/062 Loss 0.671 Prec@(1,5) (82.8%, 98.4%)\n11/23 10:14:37 PM | Valid: [23/50] Step 050/062 Loss 0.513 Prec@(1,5) (84.8%, 99.0%)\n11/23 10:14:40 PM | Valid: [23/50] Step 062/062 Loss 0.514 Prec@(1,5) (84.8%, 99.0%)\n11/23 10:14:40 PM | Valid: [23/50] Final Prec@1 84.8250%\n11/23 10:14:40 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 2)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('sep_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1107, 0.0995, 0.1102, 0.1209, 0.1029, 0.1144, 0.1086, 0.1158, 0.1171],\n        [0.1019, 0.0921, 0.1022, 0.1163, 0.1181, 0.1158, 0.1165, 0.1143, 0.1228]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1082, 0.0964, 0.1050, 0.1227, 0.1208, 0.1070, 0.1075, 0.1098, 0.1226],\n        [0.0988, 0.0912, 0.1007, 0.1156, 0.1177, 0.1167, 0.1144, 0.1199, 0.1251],\n        [0.0942, 0.0882, 0.1031, 0.1108, 0.1190, 0.1134, 0.1228, 0.1190, 0.1294]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1076, 0.0961, 0.1038, 0.1160, 0.1217, 0.1080, 0.1009, 0.1227, 0.1231],\n        [0.0950, 0.0875, 0.0928, 0.1265, 0.1171, 0.1198, 0.1159, 0.1106, 0.1348],\n        [0.0912, 0.0874, 0.0980, 0.1113, 0.1277, 0.1161, 0.1242, 0.1170, 0.1271],\n        [0.0901, 0.0848, 0.0940, 0.1160, 0.1148, 0.1252, 0.1213, 0.1121, 0.1417]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0954, 0.0885, 0.0936, 0.1048, 0.1291, 0.1187, 0.1115, 0.1284, 0.1301],\n        [0.0955, 0.0885, 0.0945, 0.1264, 0.1083, 0.1177, 0.1221, 0.1184, 0.1287],\n        [0.0866, 0.0833, 0.0940, 0.1204, 0.1140, 0.1225, 0.1218, 0.1175, 0.1399],\n        [0.0864, 0.0817, 0.0910, 0.1219, 0.1197, 0.1138, 0.1200, 0.1173, 0.1483],\n        [0.0840, 0.0812, 0.0865, 0.1135, 0.1230, 0.1202, 0.1237, 0.1295, 0.1385]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1224, 0.1128, 0.1111, 0.0995, 0.1142, 0.1164, 0.1049, 0.1095, 0.1092],\n        [0.1168, 0.1080, 0.1114, 0.1194, 0.1042, 0.1129, 0.1042, 0.1088, 0.1142]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1198, 0.1084, 0.1175, 0.1054, 0.1028, 0.1164, 0.1070, 0.1159, 0.1068],\n        [0.1218, 0.1176, 0.1112, 0.1071, 0.1150, 0.1083, 0.1032, 0.1100, 0.1057],\n        [0.1049, 0.0947, 0.1034, 0.1085, 0.1180, 0.1217, 0.1219, 0.1221, 0.1046]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1242, 0.1159, 0.1098, 0.1054, 0.1106, 0.1096, 0.1039, 0.1148, 0.1058],\n        [0.1219, 0.1155, 0.1117, 0.1119, 0.1117, 0.0990, 0.1105, 0.1159, 0.1019],\n        [0.1021, 0.0942, 0.1104, 0.1203, 0.1230, 0.1118, 0.1100, 0.1136, 0.1144],\n        [0.1113, 0.1072, 0.1276, 0.1035, 0.1079, 0.1099, 0.1057, 0.1086, 0.1183]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1203, 0.1129, 0.1074, 0.1074, 0.1103, 0.1155, 0.1058, 0.1043, 0.1161],\n        [0.1190, 0.1149, 0.1044, 0.1196, 0.1069, 0.1129, 0.1081, 0.1052, 0.1089],\n        [0.0990, 0.0917, 0.1085, 0.1175, 0.1162, 0.1075, 0.1195, 0.1219, 0.1183],\n        [0.1013, 0.0984, 0.1156, 0.1183, 0.1257, 0.1138, 0.1069, 0.1074, 0.1126],\n        [0.0982, 0.0961, 0.1147, 0.1144, 0.1109, 0.1068, 0.1087, 0.1201, 0.1300]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:14:47 PM | Train: [24/50] Step 000/062 Loss 0.355 Prec@(1,5) (85.9%, 100.0%)\n11/23 10:19:10 PM | Train: [24/50] Step 050/062 Loss 0.145 Prec@(1,5) (94.6%, 100.0%)\n11/23 10:20:11 PM | Train: [24/50] Step 062/062 Loss 0.150 Prec@(1,5) (94.5%, 100.0%)\n11/23 10:20:12 PM | Train: [24/50] Final Prec@1 94.4500%\n11/23 10:20:12 PM | Valid: [24/50] Step 000/062 Loss 0.645 Prec@(1,5) (84.4%, 98.4%)\n11/23 10:20:25 PM | Valid: [24/50] Step 050/062 Loss 0.531 Prec@(1,5) (83.3%, 99.1%)\n11/23 10:20:28 PM | Valid: [24/50] Step 062/062 Loss 0.535 Prec@(1,5) (83.5%, 99.2%)\n11/23 10:20:28 PM | Valid: [24/50] Final Prec@1 83.5000%\n11/23 10:20:28 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('sep_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1113, 0.0996, 0.1110, 0.1208, 0.1022, 0.1139, 0.1087, 0.1158, 0.1167],\n        [0.1018, 0.0916, 0.1022, 0.1174, 0.1175, 0.1154, 0.1164, 0.1134, 0.1243]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1081, 0.0960, 0.1051, 0.1229, 0.1214, 0.1063, 0.1077, 0.1096, 0.1228],\n        [0.0978, 0.0902, 0.1003, 0.1159, 0.1182, 0.1164, 0.1141, 0.1204, 0.1266],\n        [0.0932, 0.0870, 0.1026, 0.1104, 0.1188, 0.1130, 0.1239, 0.1198, 0.1313]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1074, 0.0956, 0.1036, 0.1167, 0.1221, 0.1077, 0.1003, 0.1230, 0.1237],\n        [0.0940, 0.0863, 0.0917, 0.1272, 0.1174, 0.1197, 0.1161, 0.1109, 0.1366],\n        [0.0901, 0.0861, 0.0971, 0.1107, 0.1289, 0.1163, 0.1251, 0.1172, 0.1286],\n        [0.0891, 0.0836, 0.0930, 0.1163, 0.1148, 0.1256, 0.1216, 0.1121, 0.1439]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0949, 0.0879, 0.0933, 0.1042, 0.1295, 0.1188, 0.1111, 0.1290, 0.1314],\n        [0.0948, 0.0877, 0.0940, 0.1274, 0.1075, 0.1175, 0.1228, 0.1181, 0.1303],\n        [0.0851, 0.0818, 0.0926, 0.1209, 0.1145, 0.1236, 0.1219, 0.1178, 0.1419],\n        [0.0854, 0.0804, 0.0901, 0.1220, 0.1202, 0.1127, 0.1202, 0.1179, 0.1511],\n        [0.0829, 0.0800, 0.0854, 0.1127, 0.1242, 0.1204, 0.1236, 0.1301, 0.1407]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1229, 0.1127, 0.1111, 0.0994, 0.1143, 0.1168, 0.1042, 0.1094, 0.1092],\n        [0.1181, 0.1087, 0.1119, 0.1193, 0.1031, 0.1124, 0.1037, 0.1087, 0.1141]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1201, 0.1080, 0.1175, 0.1049, 0.1022, 0.1172, 0.1071, 0.1162, 0.1067],\n        [0.1233, 0.1185, 0.1114, 0.1070, 0.1141, 0.1075, 0.1036, 0.1094, 0.1052],\n        [0.1050, 0.0941, 0.1031, 0.1080, 0.1189, 0.1217, 0.1218, 0.1230, 0.1044]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1246, 0.1157, 0.1094, 0.1056, 0.1100, 0.1100, 0.1040, 0.1150, 0.1056],\n        [0.1228, 0.1162, 0.1115, 0.1116, 0.1117, 0.0981, 0.1102, 0.1163, 0.1016],\n        [0.1013, 0.0933, 0.1103, 0.1213, 0.1241, 0.1113, 0.1100, 0.1139, 0.1144],\n        [0.1110, 0.1069, 0.1282, 0.1030, 0.1077, 0.1098, 0.1058, 0.1088, 0.1189]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1205, 0.1127, 0.1073, 0.1069, 0.1102, 0.1157, 0.1059, 0.1042, 0.1166],\n        [0.1205, 0.1160, 0.1038, 0.1195, 0.1069, 0.1127, 0.1078, 0.1045, 0.1083],\n        [0.0984, 0.0910, 0.1086, 0.1174, 0.1160, 0.1071, 0.1203, 0.1224, 0.1188],\n        [0.1008, 0.0981, 0.1158, 0.1186, 0.1265, 0.1134, 0.1067, 0.1076, 0.1126],\n        [0.0977, 0.0955, 0.1152, 0.1138, 0.1111, 0.1069, 0.1078, 0.1208, 0.1312]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:20:35 PM | Train: [25/50] Step 000/062 Loss 0.137 Prec@(1,5) (95.3%, 100.0%)\n11/23 10:24:59 PM | Train: [25/50] Step 050/062 Loss 0.120 Prec@(1,5) (95.5%, 100.0%)\n11/23 10:26:01 PM | Train: [25/50] Step 062/062 Loss 0.119 Prec@(1,5) (95.6%, 100.0%)\n11/23 10:26:01 PM | Train: [25/50] Final Prec@1 95.5750%\n11/23 10:26:02 PM | Valid: [25/50] Step 000/062 Loss 0.718 Prec@(1,5) (82.8%, 95.3%)\n11/23 10:26:14 PM | Valid: [25/50] Step 050/062 Loss 0.507 Prec@(1,5) (85.3%, 99.2%)\n11/23 10:26:17 PM | Valid: [25/50] Step 062/062 Loss 0.512 Prec@(1,5) (85.3%, 99.2%)\n11/23 10:26:17 PM | Valid: [25/50] Final Prec@1 85.3000%\n11/23 10:26:17 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('sep_conv_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1107, 0.0992, 0.1111, 0.1214, 0.1016, 0.1143, 0.1088, 0.1164, 0.1165],\n        [0.1012, 0.0910, 0.1021, 0.1179, 0.1184, 0.1148, 0.1159, 0.1131, 0.1255]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1076, 0.0954, 0.1050, 0.1237, 0.1218, 0.1060, 0.1072, 0.1098, 0.1234],\n        [0.0966, 0.0893, 0.0997, 0.1168, 0.1178, 0.1161, 0.1148, 0.1201, 0.1287],\n        [0.0920, 0.0860, 0.1022, 0.1097, 0.1185, 0.1132, 0.1251, 0.1205, 0.1328]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1071, 0.0951, 0.1036, 0.1179, 0.1224, 0.1080, 0.0997, 0.1224, 0.1238],\n        [0.0931, 0.0852, 0.0908, 0.1280, 0.1169, 0.1205, 0.1164, 0.1107, 0.1385],\n        [0.0889, 0.0850, 0.0963, 0.1100, 0.1301, 0.1167, 0.1265, 0.1172, 0.1292],\n        [0.0878, 0.0822, 0.0922, 0.1168, 0.1149, 0.1256, 0.1225, 0.1111, 0.1469]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0939, 0.0868, 0.0924, 0.1038, 0.1291, 0.1200, 0.1113, 0.1304, 0.1323],\n        [0.0936, 0.0865, 0.0929, 0.1284, 0.1074, 0.1173, 0.1231, 0.1189, 0.1319],\n        [0.0835, 0.0804, 0.0913, 0.1215, 0.1147, 0.1246, 0.1229, 0.1179, 0.1432],\n        [0.0839, 0.0789, 0.0890, 0.1222, 0.1208, 0.1124, 0.1204, 0.1184, 0.1540],\n        [0.0816, 0.0786, 0.0841, 0.1120, 0.1242, 0.1206, 0.1245, 0.1309, 0.1436]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1234, 0.1130, 0.1112, 0.0991, 0.1143, 0.1169, 0.1039, 0.1092, 0.1090],\n        [0.1191, 0.1091, 0.1119, 0.1193, 0.1022, 0.1128, 0.1030, 0.1081, 0.1144]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1203, 0.1079, 0.1175, 0.1047, 0.1010, 0.1183, 0.1072, 0.1167, 0.1064],\n        [0.1242, 0.1192, 0.1115, 0.1065, 0.1145, 0.1070, 0.1038, 0.1086, 0.1048],\n        [0.1043, 0.0933, 0.1027, 0.1082, 0.1190, 0.1225, 0.1220, 0.1240, 0.1041]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1251, 0.1161, 0.1092, 0.1053, 0.1099, 0.1098, 0.1037, 0.1150, 0.1058],\n        [0.1238, 0.1171, 0.1120, 0.1113, 0.1111, 0.0972, 0.1098, 0.1165, 0.1013],\n        [0.1007, 0.0928, 0.1106, 0.1214, 0.1236, 0.1115, 0.1102, 0.1147, 0.1146],\n        [0.1108, 0.1071, 0.1298, 0.1023, 0.1071, 0.1099, 0.1050, 0.1088, 0.1192]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1209, 0.1131, 0.1069, 0.1063, 0.1096, 0.1163, 0.1061, 0.1037, 0.1171],\n        [0.1214, 0.1166, 0.1036, 0.1191, 0.1071, 0.1130, 0.1076, 0.1041, 0.1075],\n        [0.0976, 0.0902, 0.1085, 0.1170, 0.1162, 0.1072, 0.1212, 0.1234, 0.1188],\n        [0.1006, 0.0983, 0.1166, 0.1186, 0.1271, 0.1132, 0.1061, 0.1074, 0.1121],\n        [0.0977, 0.0951, 0.1156, 0.1140, 0.1106, 0.1063, 0.1069, 0.1207, 0.1330]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:26:24 PM | Train: [26/50] Step 000/062 Loss 0.077 Prec@(1,5) (98.4%, 100.0%)\n11/23 10:30:48 PM | Train: [26/50] Step 050/062 Loss 0.093 Prec@(1,5) (96.7%, 100.0%)\n11/23 10:31:49 PM | Train: [26/50] Step 062/062 Loss 0.094 Prec@(1,5) (96.5%, 100.0%)\n11/23 10:31:49 PM | Train: [26/50] Final Prec@1 96.5500%\n11/23 10:31:50 PM | Valid: [26/50] Step 000/062 Loss 0.318 Prec@(1,5) (90.6%, 100.0%)\n11/23 10:32:03 PM | Valid: [26/50] Step 050/062 Loss 0.514 Prec@(1,5) (85.6%, 99.4%)\n11/23 10:32:06 PM | Valid: [26/50] Step 062/062 Loss 0.507 Prec@(1,5) (85.7%, 99.4%)\n11/23 10:32:06 PM | Valid: [26/50] Final Prec@1 85.7000%\n11/23 10:32:06 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1104, 0.0988, 0.1112, 0.1225, 0.1012, 0.1145, 0.1089, 0.1169, 0.1157],\n        [0.1006, 0.0901, 0.1019, 0.1188, 0.1185, 0.1141, 0.1162, 0.1128, 0.1272]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1072, 0.0947, 0.1048, 0.1239, 0.1226, 0.1055, 0.1073, 0.1105, 0.1236],\n        [0.0954, 0.0880, 0.0989, 0.1170, 0.1186, 0.1162, 0.1150, 0.1200, 0.1307],\n        [0.0909, 0.0850, 0.1020, 0.1089, 0.1183, 0.1135, 0.1260, 0.1209, 0.1346]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1073, 0.0948, 0.1039, 0.1174, 0.1227, 0.1077, 0.0993, 0.1228, 0.1242],\n        [0.0921, 0.0841, 0.0898, 0.1291, 0.1166, 0.1210, 0.1169, 0.1105, 0.1399],\n        [0.0879, 0.0840, 0.0959, 0.1097, 0.1310, 0.1173, 0.1275, 0.1160, 0.1307],\n        [0.0865, 0.0809, 0.0913, 0.1179, 0.1149, 0.1258, 0.1225, 0.1102, 0.1500]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0933, 0.0859, 0.0919, 0.1039, 0.1292, 0.1208, 0.1113, 0.1303, 0.1334],\n        [0.0925, 0.0853, 0.0918, 0.1296, 0.1073, 0.1176, 0.1233, 0.1195, 0.1331],\n        [0.0820, 0.0789, 0.0903, 0.1219, 0.1149, 0.1250, 0.1237, 0.1179, 0.1455],\n        [0.0825, 0.0776, 0.0879, 0.1223, 0.1212, 0.1116, 0.1199, 0.1195, 0.1574],\n        [0.0799, 0.0770, 0.0828, 0.1114, 0.1248, 0.1204, 0.1252, 0.1315, 0.1469]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1248, 0.1140, 0.1109, 0.0981, 0.1139, 0.1173, 0.1037, 0.1089, 0.1083],\n        [0.1203, 0.1097, 0.1128, 0.1189, 0.1011, 0.1120, 0.1025, 0.1075, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1212, 0.1083, 0.1184, 0.1038, 0.1006, 0.1190, 0.1069, 0.1167, 0.1052],\n        [0.1257, 0.1205, 0.1115, 0.1061, 0.1142, 0.1061, 0.1036, 0.1079, 0.1044],\n        [0.1037, 0.0925, 0.1024, 0.1083, 0.1192, 0.1223, 0.1223, 0.1250, 0.1042]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1267, 0.1174, 0.1084, 0.1044, 0.1093, 0.1103, 0.1034, 0.1148, 0.1050],\n        [0.1249, 0.1180, 0.1125, 0.1108, 0.1106, 0.0964, 0.1093, 0.1168, 0.1008],\n        [0.1000, 0.0921, 0.1111, 0.1215, 0.1237, 0.1112, 0.1097, 0.1151, 0.1156],\n        [0.1105, 0.1074, 0.1314, 0.1014, 0.1061, 0.1105, 0.1044, 0.1086, 0.1198]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1221, 0.1140, 0.1066, 0.1055, 0.1089, 0.1162, 0.1063, 0.1031, 0.1172],\n        [0.1229, 0.1180, 0.1028, 0.1189, 0.1070, 0.1127, 0.1074, 0.1034, 0.1069],\n        [0.0966, 0.0894, 0.1089, 0.1167, 0.1156, 0.1067, 0.1217, 0.1248, 0.1196],\n        [0.1000, 0.0983, 0.1173, 0.1195, 0.1272, 0.1130, 0.1061, 0.1067, 0.1120],\n        [0.0969, 0.0947, 0.1162, 0.1130, 0.1109, 0.1057, 0.1069, 0.1211, 0.1346]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:32:13 PM | Train: [27/50] Step 000/062 Loss 0.091 Prec@(1,5) (95.3%, 100.0%)\n11/23 10:36:36 PM | Train: [27/50] Step 050/062 Loss 0.111 Prec@(1,5) (95.8%, 99.9%)\n11/23 10:37:38 PM | Train: [27/50] Step 062/062 Loss 0.107 Prec@(1,5) (96.0%, 99.9%)\n11/23 10:37:38 PM | Train: [27/50] Final Prec@1 96.0000%\n11/23 10:37:39 PM | Valid: [27/50] Step 000/062 Loss 0.318 Prec@(1,5) (89.1%, 100.0%)\n11/23 10:37:51 PM | Valid: [27/50] Step 050/062 Loss 0.450 Prec@(1,5) (86.5%, 99.2%)\n11/23 10:37:54 PM | Valid: [27/50] Step 062/062 Loss 0.466 Prec@(1,5) (86.2%, 99.2%)\n11/23 10:37:54 PM | Valid: [27/50] Final Prec@1 86.1500%\n11/23 10:37:54 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('sep_conv_5x5', 3), ('dil_conv_5x5', 2)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1101, 0.0980, 0.1108, 0.1229, 0.1013, 0.1144, 0.1090, 0.1178, 0.1157],\n        [0.0999, 0.0893, 0.1017, 0.1191, 0.1185, 0.1137, 0.1166, 0.1128, 0.1284]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1067, 0.0938, 0.1044, 0.1238, 0.1234, 0.1053, 0.1077, 0.1108, 0.1241],\n        [0.0947, 0.0870, 0.0983, 0.1169, 0.1189, 0.1162, 0.1150, 0.1208, 0.1321],\n        [0.0899, 0.0839, 0.1017, 0.1083, 0.1181, 0.1137, 0.1268, 0.1207, 0.1369]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1066, 0.0937, 0.1031, 0.1177, 0.1237, 0.1076, 0.0987, 0.1235, 0.1253],\n        [0.0917, 0.0833, 0.0893, 0.1287, 0.1163, 0.1212, 0.1173, 0.1110, 0.1412],\n        [0.0870, 0.0829, 0.0951, 0.1091, 0.1320, 0.1180, 0.1285, 0.1157, 0.1318],\n        [0.0852, 0.0796, 0.0902, 0.1187, 0.1148, 0.1267, 0.1225, 0.1102, 0.1522]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0925, 0.0847, 0.0910, 0.1040, 0.1290, 0.1223, 0.1110, 0.1310, 0.1345],\n        [0.0919, 0.0844, 0.0911, 0.1308, 0.1068, 0.1179, 0.1233, 0.1200, 0.1338],\n        [0.0807, 0.0776, 0.0892, 0.1222, 0.1154, 0.1257, 0.1236, 0.1185, 0.1472],\n        [0.0814, 0.0764, 0.0873, 0.1229, 0.1214, 0.1112, 0.1188, 0.1201, 0.1606],\n        [0.0788, 0.0758, 0.0817, 0.1108, 0.1251, 0.1201, 0.1261, 0.1324, 0.1493]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1256, 0.1142, 0.1107, 0.0978, 0.1135, 0.1176, 0.1036, 0.1084, 0.1087],\n        [0.1209, 0.1100, 0.1131, 0.1191, 0.0999, 0.1116, 0.1024, 0.1081, 0.1149]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1217, 0.1083, 0.1186, 0.1030, 0.1002, 0.1199, 0.1067, 0.1169, 0.1047],\n        [0.1263, 0.1212, 0.1116, 0.1056, 0.1142, 0.1064, 0.1030, 0.1078, 0.1040],\n        [0.1035, 0.0921, 0.1023, 0.1076, 0.1204, 0.1229, 0.1224, 0.1250, 0.1038]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1275, 0.1180, 0.1083, 0.1039, 0.1089, 0.1102, 0.1034, 0.1151, 0.1046],\n        [0.1253, 0.1183, 0.1128, 0.1111, 0.1099, 0.0956, 0.1096, 0.1171, 0.1003],\n        [0.0995, 0.0916, 0.1117, 0.1214, 0.1238, 0.1112, 0.1092, 0.1154, 0.1161],\n        [0.1104, 0.1074, 0.1325, 0.1008, 0.1056, 0.1107, 0.1039, 0.1084, 0.1202]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1227, 0.1145, 0.1068, 0.1051, 0.1085, 0.1163, 0.1060, 0.1028, 0.1174],\n        [0.1234, 0.1188, 0.1019, 0.1193, 0.1066, 0.1124, 0.1079, 0.1032, 0.1065],\n        [0.0958, 0.0886, 0.1094, 0.1168, 0.1155, 0.1064, 0.1220, 0.1253, 0.1203],\n        [0.0995, 0.0981, 0.1183, 0.1199, 0.1264, 0.1128, 0.1059, 0.1067, 0.1123],\n        [0.0962, 0.0942, 0.1166, 0.1130, 0.1108, 0.1052, 0.1067, 0.1217, 0.1358]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:38:02 PM | Train: [28/50] Step 000/062 Loss 0.054 Prec@(1,5) (98.4%, 100.0%)\n11/23 10:42:26 PM | Train: [28/50] Step 050/062 Loss 0.085 Prec@(1,5) (96.6%, 100.0%)\n11/23 10:43:28 PM | Train: [28/50] Step 062/062 Loss 0.084 Prec@(1,5) (96.6%, 100.0%)\n11/23 10:43:28 PM | Train: [28/50] Final Prec@1 96.6250%\n11/23 10:43:29 PM | Valid: [28/50] Step 000/062 Loss 0.684 Prec@(1,5) (79.7%, 98.4%)\n11/23 10:43:41 PM | Valid: [28/50] Step 050/062 Loss 0.567 Prec@(1,5) (84.9%, 99.2%)\n11/23 10:43:44 PM | Valid: [28/50] Step 062/062 Loss 0.564 Prec@(1,5) (85.0%, 99.2%)\n11/23 10:43:44 PM | Valid: [28/50] Final Prec@1 84.9500%\n11/23 10:43:44 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1101, 0.0978, 0.1111, 0.1237, 0.1005, 0.1147, 0.1085, 0.1178, 0.1158],\n        [0.0990, 0.0882, 0.1012, 0.1199, 0.1193, 0.1137, 0.1166, 0.1129, 0.1293]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1061, 0.0930, 0.1039, 0.1241, 0.1241, 0.1050, 0.1077, 0.1111, 0.1249],\n        [0.0938, 0.0861, 0.0979, 0.1174, 0.1188, 0.1167, 0.1148, 0.1209, 0.1336],\n        [0.0889, 0.0827, 0.1011, 0.1069, 0.1178, 0.1146, 0.1281, 0.1217, 0.1382]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1063, 0.0929, 0.1027, 0.1183, 0.1235, 0.1068, 0.0988, 0.1240, 0.1267],\n        [0.0908, 0.0822, 0.0883, 0.1299, 0.1164, 0.1210, 0.1176, 0.1112, 0.1427],\n        [0.0861, 0.0817, 0.0944, 0.1096, 0.1319, 0.1180, 0.1298, 0.1156, 0.1328],\n        [0.0842, 0.0784, 0.0892, 0.1194, 0.1148, 0.1275, 0.1225, 0.1100, 0.1540]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0916, 0.0835, 0.0899, 0.1038, 0.1297, 0.1232, 0.1102, 0.1322, 0.1358],\n        [0.0906, 0.0832, 0.0900, 0.1314, 0.1067, 0.1183, 0.1240, 0.1209, 0.1350],\n        [0.0794, 0.0760, 0.0878, 0.1232, 0.1153, 0.1267, 0.1243, 0.1187, 0.1487],\n        [0.0801, 0.0750, 0.0860, 0.1238, 0.1214, 0.1104, 0.1187, 0.1208, 0.1637],\n        [0.0777, 0.0745, 0.0806, 0.1098, 0.1258, 0.1193, 0.1266, 0.1335, 0.1522]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1262, 0.1141, 0.1104, 0.0973, 0.1133, 0.1185, 0.1029, 0.1086, 0.1086],\n        [0.1220, 0.1107, 0.1134, 0.1187, 0.0991, 0.1115, 0.1018, 0.1077, 0.1150]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1219, 0.1082, 0.1189, 0.1024, 0.0991, 0.1209, 0.1070, 0.1175, 0.1040],\n        [0.1275, 0.1224, 0.1119, 0.1051, 0.1131, 0.1063, 0.1025, 0.1079, 0.1033],\n        [0.1030, 0.0914, 0.1020, 0.1073, 0.1206, 0.1236, 0.1223, 0.1259, 0.1038]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1277, 0.1179, 0.1086, 0.1032, 0.1084, 0.1108, 0.1035, 0.1161, 0.1038],\n        [0.1268, 0.1197, 0.1128, 0.1106, 0.1089, 0.0944, 0.1091, 0.1176, 0.1002],\n        [0.0989, 0.0909, 0.1119, 0.1215, 0.1243, 0.1116, 0.1088, 0.1153, 0.1169],\n        [0.1105, 0.1073, 0.1337, 0.0998, 0.1044, 0.1115, 0.1036, 0.1084, 0.1207]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1233, 0.1148, 0.1066, 0.1049, 0.1079, 0.1167, 0.1060, 0.1021, 0.1177],\n        [0.1243, 0.1198, 0.1014, 0.1198, 0.1062, 0.1128, 0.1072, 0.1024, 0.1060],\n        [0.0951, 0.0878, 0.1091, 0.1169, 0.1158, 0.1067, 0.1222, 0.1257, 0.1205],\n        [0.0993, 0.0978, 0.1189, 0.1194, 0.1260, 0.1132, 0.1062, 0.1063, 0.1129],\n        [0.0956, 0.0938, 0.1171, 0.1130, 0.1102, 0.1047, 0.1061, 0.1224, 0.1372]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:43:52 PM | Train: [29/50] Step 000/062 Loss 0.033 Prec@(1,5) (100.0%, 100.0%)\n11/23 10:48:15 PM | Train: [29/50] Step 050/062 Loss 0.060 Prec@(1,5) (97.9%, 100.0%)\n11/23 10:49:16 PM | Train: [29/50] Step 062/062 Loss 0.065 Prec@(1,5) (97.7%, 100.0%)\n11/23 10:49:16 PM | Train: [29/50] Final Prec@1 97.7250%\n11/23 10:49:17 PM | Valid: [29/50] Step 000/062 Loss 0.198 Prec@(1,5) (96.9%, 100.0%)\n11/23 10:49:29 PM | Valid: [29/50] Step 050/062 Loss 0.590 Prec@(1,5) (86.2%, 99.3%)\n11/23 10:49:32 PM | Valid: [29/50] Step 062/062 Loss 0.578 Prec@(1,5) (86.1%, 99.3%)\n11/23 10:49:32 PM | Valid: [29/50] Final Prec@1 86.0750%\n11/23 10:49:32 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1100, 0.0972, 0.1112, 0.1238, 0.1005, 0.1151, 0.1085, 0.1177, 0.1159],\n        [0.0981, 0.0871, 0.1005, 0.1203, 0.1201, 0.1135, 0.1167, 0.1132, 0.1305]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1058, 0.0921, 0.1034, 0.1241, 0.1248, 0.1050, 0.1078, 0.1116, 0.1254],\n        [0.0930, 0.0850, 0.0973, 0.1174, 0.1185, 0.1169, 0.1149, 0.1214, 0.1357],\n        [0.0876, 0.0813, 0.1002, 0.1064, 0.1182, 0.1152, 0.1295, 0.1226, 0.1390]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1061, 0.0921, 0.1024, 0.1190, 0.1238, 0.1062, 0.0984, 0.1241, 0.1279],\n        [0.0897, 0.0809, 0.0873, 0.1306, 0.1168, 0.1212, 0.1180, 0.1110, 0.1444],\n        [0.0849, 0.0804, 0.0935, 0.1095, 0.1323, 0.1186, 0.1313, 0.1156, 0.1338],\n        [0.0832, 0.0772, 0.0886, 0.1198, 0.1139, 0.1276, 0.1230, 0.1097, 0.1571]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0908, 0.0823, 0.0890, 0.1039, 0.1296, 0.1237, 0.1102, 0.1327, 0.1377],\n        [0.0895, 0.0819, 0.0891, 0.1321, 0.1062, 0.1186, 0.1246, 0.1214, 0.1365],\n        [0.0777, 0.0744, 0.0863, 0.1239, 0.1159, 0.1278, 0.1249, 0.1187, 0.1504],\n        [0.0786, 0.0736, 0.0849, 0.1247, 0.1221, 0.1103, 0.1181, 0.1208, 0.1669],\n        [0.0762, 0.0730, 0.0792, 0.1090, 0.1265, 0.1194, 0.1273, 0.1344, 0.1550]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1264, 0.1140, 0.1102, 0.0966, 0.1134, 0.1197, 0.1031, 0.1082, 0.1085],\n        [0.1221, 0.1103, 0.1132, 0.1195, 0.0985, 0.1115, 0.1019, 0.1076, 0.1154]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1219, 0.1077, 0.1196, 0.1021, 0.0983, 0.1220, 0.1074, 0.1181, 0.1029],\n        [0.1280, 0.1226, 0.1122, 0.1047, 0.1129, 0.1061, 0.1021, 0.1081, 0.1033],\n        [0.1021, 0.0907, 0.1018, 0.1074, 0.1207, 0.1234, 0.1230, 0.1267, 0.1042]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1284, 0.1182, 0.1082, 0.1032, 0.1080, 0.1113, 0.1032, 0.1164, 0.1032],\n        [0.1271, 0.1198, 0.1134, 0.1108, 0.1085, 0.0935, 0.1089, 0.1180, 0.1000],\n        [0.0976, 0.0898, 0.1118, 0.1217, 0.1251, 0.1119, 0.1088, 0.1158, 0.1174],\n        [0.1098, 0.1072, 0.1348, 0.0992, 0.1043, 0.1118, 0.1036, 0.1082, 0.1210]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1237, 0.1150, 0.1073, 0.1045, 0.1072, 0.1170, 0.1052, 0.1025, 0.1176],\n        [0.1245, 0.1200, 0.1008, 0.1205, 0.1061, 0.1136, 0.1067, 0.1024, 0.1054],\n        [0.0940, 0.0871, 0.1093, 0.1173, 0.1161, 0.1063, 0.1223, 0.1269, 0.1206],\n        [0.0985, 0.0977, 0.1197, 0.1199, 0.1261, 0.1130, 0.1061, 0.1060, 0.1130],\n        [0.0949, 0.0934, 0.1177, 0.1128, 0.1101, 0.1047, 0.1055, 0.1218, 0.1390]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:49:39 PM | Train: [30/50] Step 000/062 Loss 0.031 Prec@(1,5) (98.4%, 100.0%)\n11/23 10:54:04 PM | Train: [30/50] Step 050/062 Loss 0.064 Prec@(1,5) (97.7%, 100.0%)\n11/23 10:55:06 PM | Train: [30/50] Step 062/062 Loss 0.065 Prec@(1,5) (97.7%, 100.0%)\n11/23 10:55:06 PM | Train: [30/50] Final Prec@1 97.6500%\n11/23 10:55:06 PM | Valid: [30/50] Step 000/062 Loss 0.338 Prec@(1,5) (89.1%, 100.0%)\n11/23 10:55:19 PM | Valid: [30/50] Step 050/062 Loss 0.525 Prec@(1,5) (86.6%, 99.3%)\n11/23 10:55:22 PM | Valid: [30/50] Step 062/062 Loss 0.531 Prec@(1,5) (86.6%, 99.3%)\n11/23 10:55:22 PM | Valid: [30/50] Final Prec@1 86.5500%\n11/23 10:55:22 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1098, 0.0968, 0.1112, 0.1243, 0.1002, 0.1153, 0.1087, 0.1177, 0.1159],\n        [0.0976, 0.0862, 0.1001, 0.1201, 0.1200, 0.1142, 0.1173, 0.1133, 0.1312]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1049, 0.0910, 0.1027, 0.1247, 0.1257, 0.1050, 0.1076, 0.1118, 0.1264],\n        [0.0925, 0.0839, 0.0968, 0.1169, 0.1181, 0.1165, 0.1150, 0.1226, 0.1377],\n        [0.0866, 0.0802, 0.0993, 0.1058, 0.1186, 0.1159, 0.1311, 0.1229, 0.1395]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.0915, 0.1022, 0.1187, 0.1245, 0.1062, 0.0978, 0.1246, 0.1286],\n        [0.0892, 0.0800, 0.0868, 0.1310, 0.1170, 0.1208, 0.1181, 0.1109, 0.1462],\n        [0.0842, 0.0795, 0.0931, 0.1089, 0.1332, 0.1189, 0.1316, 0.1159, 0.1347],\n        [0.0825, 0.0763, 0.0883, 0.1192, 0.1139, 0.1276, 0.1228, 0.1091, 0.1601]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0900, 0.0813, 0.0882, 0.1033, 0.1296, 0.1241, 0.1104, 0.1337, 0.1395],\n        [0.0886, 0.0808, 0.0882, 0.1327, 0.1052, 0.1187, 0.1258, 0.1219, 0.1379],\n        [0.0763, 0.0730, 0.0850, 0.1244, 0.1158, 0.1288, 0.1257, 0.1185, 0.1526],\n        [0.0774, 0.0722, 0.0840, 0.1254, 0.1220, 0.1097, 0.1182, 0.1210, 0.1702],\n        [0.0747, 0.0713, 0.0775, 0.1088, 0.1270, 0.1195, 0.1279, 0.1355, 0.1579]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1268, 0.1143, 0.1100, 0.0956, 0.1132, 0.1204, 0.1030, 0.1085, 0.1082],\n        [0.1222, 0.1100, 0.1141, 0.1192, 0.0984, 0.1112, 0.1014, 0.1081, 0.1153]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1221, 0.1078, 0.1204, 0.1017, 0.0978, 0.1229, 0.1074, 0.1179, 0.1019],\n        [0.1289, 0.1235, 0.1123, 0.1042, 0.1123, 0.1059, 0.1012, 0.1084, 0.1033],\n        [0.1014, 0.0901, 0.1015, 0.1068, 0.1218, 0.1235, 0.1239, 0.1272, 0.1038]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1293, 0.1191, 0.1079, 0.1022, 0.1075, 0.1114, 0.1029, 0.1170, 0.1027],\n        [0.1276, 0.1204, 0.1141, 0.1102, 0.1080, 0.0927, 0.1093, 0.1182, 0.0996],\n        [0.0968, 0.0892, 0.1126, 0.1218, 0.1257, 0.1116, 0.1086, 0.1154, 0.1182],\n        [0.1094, 0.1072, 0.1360, 0.0991, 0.1039, 0.1114, 0.1033, 0.1087, 0.1212]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1243, 0.1156, 0.1074, 0.1043, 0.1067, 0.1164, 0.1048, 0.1022, 0.1183],\n        [0.1246, 0.1202, 0.1003, 0.1213, 0.1058, 0.1144, 0.1063, 0.1020, 0.1051],\n        [0.0932, 0.0864, 0.1093, 0.1178, 0.1165, 0.1062, 0.1226, 0.1273, 0.1207],\n        [0.0977, 0.0973, 0.1203, 0.1202, 0.1260, 0.1129, 0.1062, 0.1060, 0.1135],\n        [0.0941, 0.0926, 0.1179, 0.1130, 0.1096, 0.1042, 0.1053, 0.1220, 0.1413]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 10:55:29 PM | Train: [31/50] Step 000/062 Loss 0.032 Prec@(1,5) (100.0%, 100.0%)\n11/23 10:59:53 PM | Train: [31/50] Step 050/062 Loss 0.050 Prec@(1,5) (98.4%, 99.9%)\n11/23 11:00:54 PM | Train: [31/50] Step 062/062 Loss 0.053 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:00:54 PM | Train: [31/50] Final Prec@1 98.3750%\n11/23 11:00:55 PM | Valid: [31/50] Step 000/062 Loss 0.644 Prec@(1,5) (81.2%, 100.0%)\n11/23 11:01:07 PM | Valid: [31/50] Step 050/062 Loss 0.506 Prec@(1,5) (86.8%, 99.4%)\n11/23 11:01:11 PM | Valid: [31/50] Step 062/062 Loss 0.513 Prec@(1,5) (86.5%, 99.4%)\n11/23 11:01:11 PM | Valid: [31/50] Final Prec@1 86.5250%\n11/23 11:01:11 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1102, 0.0968, 0.1120, 0.1242, 0.0990, 0.1152, 0.1093, 0.1177, 0.1155],\n        [0.0969, 0.0851, 0.0995, 0.1203, 0.1206, 0.1141, 0.1173, 0.1132, 0.1331]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1050, 0.0907, 0.1029, 0.1252, 0.1265, 0.1043, 0.1072, 0.1119, 0.1262],\n        [0.0914, 0.0829, 0.0962, 0.1171, 0.1183, 0.1158, 0.1153, 0.1232, 0.1398],\n        [0.0856, 0.0793, 0.0990, 0.1058, 0.1186, 0.1158, 0.1324, 0.1226, 0.1409]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1065, 0.0915, 0.1026, 0.1191, 0.1249, 0.1051, 0.0974, 0.1238, 0.1291],\n        [0.0883, 0.0788, 0.0858, 0.1318, 0.1174, 0.1208, 0.1182, 0.1107, 0.1483],\n        [0.0836, 0.0787, 0.0928, 0.1085, 0.1341, 0.1189, 0.1322, 0.1154, 0.1357],\n        [0.0820, 0.0754, 0.0881, 0.1194, 0.1134, 0.1276, 0.1224, 0.1081, 0.1637]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0895, 0.0806, 0.0879, 0.1029, 0.1298, 0.1243, 0.1102, 0.1347, 0.1401],\n        [0.0878, 0.0796, 0.0872, 0.1335, 0.1041, 0.1188, 0.1269, 0.1224, 0.1396],\n        [0.0753, 0.0719, 0.0843, 0.1246, 0.1159, 0.1294, 0.1255, 0.1182, 0.1549],\n        [0.0761, 0.0709, 0.0830, 0.1259, 0.1221, 0.1093, 0.1177, 0.1210, 0.1739],\n        [0.0733, 0.0698, 0.0760, 0.1086, 0.1275, 0.1187, 0.1290, 0.1359, 0.1611]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1278, 0.1150, 0.1095, 0.0951, 0.1128, 0.1209, 0.1024, 0.1086, 0.1079],\n        [0.1227, 0.1102, 0.1141, 0.1197, 0.0979, 0.1113, 0.1010, 0.1076, 0.1155]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1227, 0.1081, 0.1208, 0.1006, 0.0968, 0.1234, 0.1080, 0.1182, 0.1014],\n        [0.1296, 0.1243, 0.1120, 0.1042, 0.1123, 0.1056, 0.1002, 0.1085, 0.1033],\n        [0.1008, 0.0893, 0.1008, 0.1061, 0.1222, 0.1243, 0.1250, 0.1280, 0.1034]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1310, 0.1206, 0.1070, 0.1010, 0.1069, 0.1112, 0.1028, 0.1172, 0.1022],\n        [0.1274, 0.1204, 0.1148, 0.1101, 0.1079, 0.0921, 0.1089, 0.1185, 0.0999],\n        [0.0961, 0.0884, 0.1122, 0.1221, 0.1264, 0.1122, 0.1089, 0.1153, 0.1183],\n        [0.1089, 0.1074, 0.1373, 0.0984, 0.1034, 0.1116, 0.1029, 0.1086, 0.1215]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1253, 0.1167, 0.1072, 0.1039, 0.1056, 0.1161, 0.1044, 0.1023, 0.1185],\n        [0.1248, 0.1207, 0.1006, 0.1218, 0.1059, 0.1143, 0.1053, 0.1015, 0.1050],\n        [0.0927, 0.0861, 0.1097, 0.1180, 0.1166, 0.1057, 0.1218, 0.1284, 0.1210],\n        [0.0972, 0.0976, 0.1209, 0.1200, 0.1262, 0.1131, 0.1060, 0.1055, 0.1134],\n        [0.0935, 0.0924, 0.1185, 0.1116, 0.1099, 0.1035, 0.1052, 0.1228, 0.1425]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:01:18 PM | Train: [32/50] Step 000/062 Loss 0.045 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:05:42 PM | Train: [32/50] Step 050/062 Loss 0.033 Prec@(1,5) (98.9%, 100.0%)\n11/23 11:06:43 PM | Train: [32/50] Step 062/062 Loss 0.035 Prec@(1,5) (98.7%, 100.0%)\n11/23 11:06:43 PM | Train: [32/50] Final Prec@1 98.7250%\n11/23 11:06:44 PM | Valid: [32/50] Step 000/062 Loss 0.671 Prec@(1,5) (85.9%, 100.0%)\n11/23 11:06:56 PM | Valid: [32/50] Step 050/062 Loss 0.552 Prec@(1,5) (86.5%, 99.4%)\n11/23 11:06:59 PM | Valid: [32/50] Step 062/062 Loss 0.540 Prec@(1,5) (87.0%, 99.3%)\n11/23 11:06:59 PM | Valid: [32/50] Final Prec@1 86.9750%\n11/23 11:06:59 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1104, 0.0966, 0.1123, 0.1239, 0.0982, 0.1151, 0.1101, 0.1185, 0.1149],\n        [0.0965, 0.0845, 0.0994, 0.1206, 0.1204, 0.1134, 0.1174, 0.1132, 0.1346]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1051, 0.0903, 0.1028, 0.1255, 0.1272, 0.1039, 0.1072, 0.1118, 0.1263],\n        [0.0907, 0.0821, 0.0957, 0.1174, 0.1188, 0.1153, 0.1154, 0.1236, 0.1409],\n        [0.0849, 0.0785, 0.0989, 0.1057, 0.1182, 0.1155, 0.1326, 0.1226, 0.1430]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1068, 0.0911, 0.1029, 0.1195, 0.1249, 0.1043, 0.0965, 0.1239, 0.1300],\n        [0.0881, 0.0781, 0.0854, 0.1318, 0.1173, 0.1209, 0.1187, 0.1105, 0.1493],\n        [0.0828, 0.0780, 0.0927, 0.1085, 0.1346, 0.1191, 0.1320, 0.1150, 0.1372],\n        [0.0814, 0.0746, 0.0878, 0.1195, 0.1132, 0.1272, 0.1221, 0.1079, 0.1664]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0888, 0.0798, 0.0873, 0.1034, 0.1299, 0.1242, 0.1101, 0.1356, 0.1409],\n        [0.0873, 0.0789, 0.0868, 0.1341, 0.1029, 0.1192, 0.1270, 0.1228, 0.1410],\n        [0.0743, 0.0708, 0.0835, 0.1243, 0.1161, 0.1299, 0.1258, 0.1183, 0.1569],\n        [0.0752, 0.0699, 0.0822, 0.1265, 0.1226, 0.1090, 0.1167, 0.1212, 0.1768],\n        [0.0722, 0.0687, 0.0750, 0.1074, 0.1280, 0.1170, 0.1296, 0.1375, 0.1646]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1285, 0.1151, 0.1097, 0.0943, 0.1122, 0.1215, 0.1022, 0.1083, 0.1081],\n        [0.1236, 0.1105, 0.1145, 0.1198, 0.0973, 0.1110, 0.1006, 0.1075, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1229, 0.1077, 0.1216, 0.1002, 0.0959, 0.1241, 0.1084, 0.1185, 0.1008],\n        [0.1308, 0.1252, 0.1125, 0.1040, 0.1118, 0.1052, 0.0994, 0.1087, 0.1024],\n        [0.1005, 0.0886, 0.1006, 0.1057, 0.1223, 0.1245, 0.1260, 0.1285, 0.1034]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1319, 0.1210, 0.1062, 0.1006, 0.1067, 0.1116, 0.1029, 0.1173, 0.1019],\n        [0.1282, 0.1210, 0.1148, 0.1098, 0.1077, 0.0913, 0.1086, 0.1187, 0.0999],\n        [0.0953, 0.0874, 0.1123, 0.1218, 0.1264, 0.1123, 0.1098, 0.1155, 0.1192],\n        [0.1085, 0.1072, 0.1383, 0.0976, 0.1028, 0.1117, 0.1028, 0.1091, 0.1220]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1257, 0.1170, 0.1070, 0.1036, 0.1053, 0.1163, 0.1044, 0.1020, 0.1186],\n        [0.1255, 0.1213, 0.0997, 0.1220, 0.1062, 0.1139, 0.1054, 0.1014, 0.1046],\n        [0.0920, 0.0855, 0.1101, 0.1180, 0.1171, 0.1054, 0.1215, 0.1285, 0.1218],\n        [0.0969, 0.0975, 0.1213, 0.1198, 0.1266, 0.1129, 0.1063, 0.1054, 0.1134],\n        [0.0929, 0.0919, 0.1186, 0.1111, 0.1099, 0.1026, 0.1051, 0.1239, 0.1440]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:07:06 PM | Train: [33/50] Step 000/062 Loss 0.018 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:11:30 PM | Train: [33/50] Step 050/062 Loss 0.051 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:12:32 PM | Train: [33/50] Step 062/062 Loss 0.049 Prec@(1,5) (98.5%, 100.0%)\n11/23 11:12:32 PM | Train: [33/50] Final Prec@1 98.5000%\n11/23 11:12:32 PM | Valid: [33/50] Step 000/062 Loss 0.621 Prec@(1,5) (85.9%, 100.0%)\n11/23 11:12:45 PM | Valid: [33/50] Step 050/062 Loss 0.524 Prec@(1,5) (87.5%, 99.2%)\n11/23 11:12:48 PM | Valid: [33/50] Step 062/062 Loss 0.551 Prec@(1,5) (87.0%, 99.1%)\n11/23 11:12:48 PM | Valid: [33/50] Final Prec@1 86.9500%\n11/23 11:12:48 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1105, 0.0963, 0.1126, 0.1241, 0.0978, 0.1150, 0.1104, 0.1184, 0.1149],\n        [0.0962, 0.0841, 0.0995, 0.1212, 0.1205, 0.1128, 0.1175, 0.1128, 0.1354]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1045, 0.0894, 0.1022, 0.1256, 0.1278, 0.1035, 0.1076, 0.1123, 0.1272],\n        [0.0902, 0.0814, 0.0956, 0.1176, 0.1189, 0.1147, 0.1149, 0.1244, 0.1423],\n        [0.0839, 0.0774, 0.0982, 0.1050, 0.1177, 0.1164, 0.1337, 0.1228, 0.1448]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1067, 0.0906, 0.1027, 0.1198, 0.1251, 0.1041, 0.0960, 0.1242, 0.1309],\n        [0.0876, 0.0773, 0.0850, 0.1316, 0.1179, 0.1199, 0.1189, 0.1106, 0.1513],\n        [0.0819, 0.0770, 0.0921, 0.1080, 0.1351, 0.1198, 0.1327, 0.1149, 0.1385],\n        [0.0805, 0.0736, 0.0870, 0.1197, 0.1133, 0.1275, 0.1225, 0.1070, 0.1688]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0881, 0.0791, 0.0868, 0.1031, 0.1306, 0.1232, 0.1103, 0.1368, 0.1422],\n        [0.0868, 0.0783, 0.0864, 0.1351, 0.1026, 0.1185, 0.1270, 0.1225, 0.1428],\n        [0.0731, 0.0696, 0.0823, 0.1244, 0.1164, 0.1304, 0.1269, 0.1183, 0.1587],\n        [0.0742, 0.0689, 0.0813, 0.1273, 0.1223, 0.1083, 0.1162, 0.1216, 0.1799],\n        [0.0710, 0.0675, 0.0739, 0.1069, 0.1279, 0.1164, 0.1305, 0.1377, 0.1683]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1287, 0.1149, 0.1091, 0.0940, 0.1127, 0.1220, 0.1019, 0.1081, 0.1084],\n        [0.1240, 0.1104, 0.1148, 0.1202, 0.0970, 0.1117, 0.0995, 0.1076, 0.1148]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1228, 0.1071, 0.1220, 0.0997, 0.0951, 0.1249, 0.1090, 0.1192, 0.1002],\n        [0.1320, 0.1261, 0.1127, 0.1036, 0.1112, 0.1046, 0.0989, 0.1084, 0.1025],\n        [0.1003, 0.0878, 0.0999, 0.1054, 0.1228, 0.1248, 0.1263, 0.1299, 0.1027]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1324, 0.1211, 0.1057, 0.1002, 0.1067, 0.1113, 0.1029, 0.1177, 0.1021],\n        [0.1289, 0.1213, 0.1148, 0.1097, 0.1076, 0.0903, 0.1088, 0.1192, 0.0995],\n        [0.0948, 0.0866, 0.1123, 0.1213, 0.1268, 0.1127, 0.1098, 0.1162, 0.1195],\n        [0.1086, 0.1076, 0.1396, 0.0971, 0.1021, 0.1116, 0.1025, 0.1088, 0.1222]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1261, 0.1171, 0.1066, 0.1034, 0.1052, 0.1166, 0.1039, 0.1015, 0.1196],\n        [0.1258, 0.1216, 0.0996, 0.1223, 0.1063, 0.1137, 0.1057, 0.1010, 0.1039],\n        [0.0919, 0.0852, 0.1106, 0.1183, 0.1171, 0.1041, 0.1215, 0.1292, 0.1220],\n        [0.0968, 0.0978, 0.1220, 0.1194, 0.1267, 0.1134, 0.1063, 0.1049, 0.1128],\n        [0.0927, 0.0919, 0.1194, 0.1107, 0.1097, 0.1025, 0.1046, 0.1232, 0.1454]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:12:55 PM | Train: [34/50] Step 000/062 Loss 0.136 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:17:18 PM | Train: [34/50] Step 050/062 Loss 0.048 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:18:20 PM | Train: [34/50] Step 062/062 Loss 0.047 Prec@(1,5) (98.5%, 100.0%)\n11/23 11:18:20 PM | Train: [34/50] Final Prec@1 98.4500%\n11/23 11:18:21 PM | Valid: [34/50] Step 000/062 Loss 0.176 Prec@(1,5) (96.9%, 100.0%)\n11/23 11:18:33 PM | Valid: [34/50] Step 050/062 Loss 0.542 Prec@(1,5) (86.6%, 99.2%)\n11/23 11:18:36 PM | Valid: [34/50] Step 062/062 Loss 0.525 Prec@(1,5) (86.7%, 99.2%)\n11/23 11:18:36 PM | Valid: [34/50] Final Prec@1 86.7250%\n11/23 11:18:36 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1106, 0.0961, 0.1131, 0.1248, 0.0971, 0.1152, 0.1103, 0.1180, 0.1148],\n        [0.0958, 0.0836, 0.0997, 0.1213, 0.1206, 0.1126, 0.1176, 0.1126, 0.1363]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1042, 0.0887, 0.1020, 0.1256, 0.1282, 0.1031, 0.1074, 0.1129, 0.1278],\n        [0.0899, 0.0810, 0.0958, 0.1178, 0.1186, 0.1146, 0.1149, 0.1241, 0.1432],\n        [0.0833, 0.0768, 0.0982, 0.1040, 0.1177, 0.1160, 0.1344, 0.1234, 0.1462]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1064, 0.0899, 0.1026, 0.1206, 0.1247, 0.1037, 0.0957, 0.1245, 0.1318],\n        [0.0870, 0.0765, 0.0846, 0.1326, 0.1177, 0.1191, 0.1199, 0.1104, 0.1522],\n        [0.0812, 0.0761, 0.0916, 0.1072, 0.1354, 0.1203, 0.1335, 0.1152, 0.1395],\n        [0.0796, 0.0726, 0.0864, 0.1202, 0.1135, 0.1275, 0.1223, 0.1066, 0.1714]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0873, 0.0782, 0.0863, 0.1032, 0.1303, 0.1230, 0.1100, 0.1382, 0.1435],\n        [0.0862, 0.0775, 0.0860, 0.1356, 0.1024, 0.1182, 0.1275, 0.1223, 0.1443],\n        [0.0720, 0.0685, 0.0814, 0.1245, 0.1167, 0.1303, 0.1274, 0.1184, 0.1608],\n        [0.0728, 0.0676, 0.0801, 0.1282, 0.1218, 0.1078, 0.1163, 0.1225, 0.1829],\n        [0.0697, 0.0662, 0.0728, 0.1063, 0.1279, 0.1156, 0.1311, 0.1386, 0.1717]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1302, 0.1158, 0.1080, 0.0932, 0.1123, 0.1223, 0.1018, 0.1081, 0.1082],\n        [0.1248, 0.1109, 0.1155, 0.1198, 0.0963, 0.1112, 0.0989, 0.1074, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1232, 0.1071, 0.1225, 0.0992, 0.0943, 0.1260, 0.1091, 0.1190, 0.0997],\n        [0.1330, 0.1272, 0.1131, 0.1034, 0.1107, 0.1039, 0.0983, 0.1082, 0.1021],\n        [0.0996, 0.0869, 0.0993, 0.1051, 0.1234, 0.1249, 0.1267, 0.1309, 0.1031]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1336, 0.1219, 0.1048, 0.0999, 0.1062, 0.1115, 0.1027, 0.1181, 0.1015],\n        [0.1294, 0.1221, 0.1152, 0.1091, 0.1072, 0.0898, 0.1085, 0.1193, 0.0995],\n        [0.0940, 0.0857, 0.1118, 0.1214, 0.1275, 0.1127, 0.1102, 0.1167, 0.1201],\n        [0.1080, 0.1076, 0.1404, 0.0969, 0.1019, 0.1116, 0.1025, 0.1088, 0.1224]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1270, 0.1179, 0.1065, 0.1032, 0.1050, 0.1167, 0.1031, 0.1014, 0.1192],\n        [0.1261, 0.1220, 0.0998, 0.1219, 0.1065, 0.1135, 0.1055, 0.1009, 0.1037],\n        [0.0910, 0.0842, 0.1104, 0.1181, 0.1177, 0.1040, 0.1221, 0.1298, 0.1227],\n        [0.0959, 0.0974, 0.1221, 0.1194, 0.1271, 0.1139, 0.1062, 0.1051, 0.1131],\n        [0.0919, 0.0914, 0.1195, 0.1110, 0.1096, 0.1027, 0.1042, 0.1233, 0.1464]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:18:43 PM | Train: [35/50] Step 000/062 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:23:06 PM | Train: [35/50] Step 050/062 Loss 0.029 Prec@(1,5) (99.1%, 100.0%)\n11/23 11:24:08 PM | Train: [35/50] Step 062/062 Loss 0.028 Prec@(1,5) (99.2%, 100.0%)\n11/23 11:24:08 PM | Train: [35/50] Final Prec@1 99.2250%\n11/23 11:24:09 PM | Valid: [35/50] Step 000/062 Loss 0.528 Prec@(1,5) (81.2%, 100.0%)\n11/23 11:24:22 PM | Valid: [35/50] Step 050/062 Loss 0.499 Prec@(1,5) (86.9%, 99.3%)\n11/23 11:24:24 PM | Valid: [35/50] Step 062/062 Loss 0.522 Prec@(1,5) (86.7%, 99.4%)\n11/23 11:24:24 PM | Valid: [35/50] Final Prec@1 86.7000%\n11/23 11:24:24 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_5x5', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1107, 0.0959, 0.1135, 0.1252, 0.0964, 0.1148, 0.1104, 0.1188, 0.1144],\n        [0.0950, 0.0826, 0.0989, 0.1216, 0.1212, 0.1123, 0.1176, 0.1130, 0.1379]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1037, 0.0879, 0.1016, 0.1257, 0.1293, 0.1029, 0.1074, 0.1138, 0.1278],\n        [0.0892, 0.0800, 0.0953, 0.1180, 0.1182, 0.1144, 0.1153, 0.1246, 0.1450],\n        [0.0827, 0.0759, 0.0978, 0.1032, 0.1179, 0.1158, 0.1353, 0.1241, 0.1473]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1062, 0.0892, 0.1025, 0.1201, 0.1251, 0.1032, 0.0955, 0.1250, 0.1331],\n        [0.0861, 0.0753, 0.0836, 0.1327, 0.1173, 0.1198, 0.1201, 0.1109, 0.1543],\n        [0.0803, 0.0749, 0.0907, 0.1068, 0.1358, 0.1207, 0.1341, 0.1155, 0.1412],\n        [0.0787, 0.0715, 0.0858, 0.1198, 0.1137, 0.1281, 0.1222, 0.1062, 0.1739]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0865, 0.0771, 0.0855, 0.1028, 0.1312, 0.1230, 0.1100, 0.1393, 0.1445],\n        [0.0852, 0.0762, 0.0849, 0.1368, 0.1016, 0.1186, 0.1281, 0.1228, 0.1458],\n        [0.0708, 0.0671, 0.0800, 0.1252, 0.1168, 0.1313, 0.1282, 0.1185, 0.1621],\n        [0.0718, 0.0664, 0.0793, 0.1290, 0.1224, 0.1068, 0.1160, 0.1224, 0.1860],\n        [0.0684, 0.0649, 0.0715, 0.1060, 0.1280, 0.1154, 0.1329, 0.1391, 0.1740]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1307, 0.1159, 0.1075, 0.0930, 0.1121, 0.1228, 0.1012, 0.1083, 0.1085],\n        [0.1250, 0.1109, 0.1159, 0.1196, 0.0962, 0.1118, 0.0983, 0.1074, 0.1150]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1235, 0.1069, 0.1225, 0.0988, 0.0936, 0.1273, 0.1094, 0.1190, 0.0989],\n        [0.1337, 0.1278, 0.1129, 0.1029, 0.1108, 0.1037, 0.0978, 0.1084, 0.1019],\n        [0.0996, 0.0864, 0.0990, 0.1048, 0.1235, 0.1253, 0.1274, 0.1315, 0.1025]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1342, 0.1223, 0.1045, 0.0995, 0.1053, 0.1120, 0.1023, 0.1185, 0.1013],\n        [0.1296, 0.1223, 0.1154, 0.1093, 0.1066, 0.0888, 0.1086, 0.1198, 0.0995],\n        [0.0933, 0.0848, 0.1116, 0.1213, 0.1279, 0.1131, 0.1108, 0.1166, 0.1207],\n        [0.1075, 0.1072, 0.1410, 0.0967, 0.1017, 0.1122, 0.1028, 0.1090, 0.1219]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1273, 0.1182, 0.1063, 0.1032, 0.1048, 0.1173, 0.1026, 0.1007, 0.1197],\n        [0.1261, 0.1222, 0.0997, 0.1221, 0.1066, 0.1144, 0.1050, 0.1006, 0.1032],\n        [0.0901, 0.0833, 0.1103, 0.1187, 0.1175, 0.1040, 0.1220, 0.1308, 0.1233],\n        [0.0951, 0.0970, 0.1226, 0.1193, 0.1273, 0.1146, 0.1064, 0.1046, 0.1132],\n        [0.0911, 0.0907, 0.1199, 0.1109, 0.1090, 0.1025, 0.1045, 0.1231, 0.1482]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:24:32 PM | Train: [36/50] Step 000/062 Loss 0.009 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:28:56 PM | Train: [36/50] Step 050/062 Loss 0.018 Prec@(1,5) (99.5%, 100.0%)\n11/23 11:29:57 PM | Train: [36/50] Step 062/062 Loss 0.019 Prec@(1,5) (99.5%, 100.0%)\n11/23 11:29:57 PM | Train: [36/50] Final Prec@1 99.4500%\n11/23 11:29:58 PM | Valid: [36/50] Step 000/062 Loss 0.329 Prec@(1,5) (87.5%, 100.0%)\n11/23 11:30:10 PM | Valid: [36/50] Step 050/062 Loss 0.545 Prec@(1,5) (87.1%, 99.3%)\n11/23 11:30:14 PM | Valid: [36/50] Step 062/062 Loss 0.556 Prec@(1,5) (87.1%, 99.4%)\n11/23 11:30:14 PM | Valid: [36/50] Final Prec@1 87.1000%\n11/23 11:30:14 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1108, 0.0958, 0.1138, 0.1255, 0.0960, 0.1148, 0.1096, 0.1192, 0.1145],\n        [0.0942, 0.0819, 0.0987, 0.1214, 0.1216, 0.1124, 0.1178, 0.1129, 0.1391]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1030, 0.0872, 0.1011, 0.1265, 0.1301, 0.1023, 0.1072, 0.1140, 0.1284],\n        [0.0886, 0.0794, 0.0950, 0.1183, 0.1183, 0.1144, 0.1153, 0.1247, 0.1462],\n        [0.0816, 0.0749, 0.0972, 0.1027, 0.1179, 0.1163, 0.1361, 0.1243, 0.1492]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1062, 0.0889, 0.1026, 0.1207, 0.1252, 0.1025, 0.0955, 0.1248, 0.1336],\n        [0.0855, 0.0744, 0.0829, 0.1337, 0.1173, 0.1198, 0.1204, 0.1104, 0.1556],\n        [0.0793, 0.0740, 0.0901, 0.1065, 0.1365, 0.1210, 0.1344, 0.1159, 0.1423],\n        [0.0781, 0.0708, 0.0853, 0.1202, 0.1133, 0.1279, 0.1223, 0.1055, 0.1764]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0858, 0.0762, 0.0847, 0.1021, 0.1319, 0.1232, 0.1095, 0.1407, 0.1458],\n        [0.0845, 0.0754, 0.0841, 0.1383, 0.1014, 0.1185, 0.1281, 0.1230, 0.1468],\n        [0.0697, 0.0660, 0.0789, 0.1254, 0.1169, 0.1320, 0.1286, 0.1187, 0.1638],\n        [0.0711, 0.0656, 0.0785, 0.1292, 0.1225, 0.1064, 0.1153, 0.1225, 0.1889],\n        [0.0674, 0.0639, 0.0705, 0.1056, 0.1281, 0.1145, 0.1339, 0.1398, 0.1764]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1308, 0.1156, 0.1078, 0.0927, 0.1121, 0.1234, 0.1008, 0.1083, 0.1084],\n        [0.1256, 0.1111, 0.1159, 0.1201, 0.0959, 0.1114, 0.0980, 0.1071, 0.1150]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1234, 0.1064, 0.1228, 0.0986, 0.0930, 0.1283, 0.1098, 0.1191, 0.0986],\n        [0.1344, 0.1283, 0.1132, 0.1028, 0.1107, 0.1034, 0.0971, 0.1083, 0.1018],\n        [0.0994, 0.0859, 0.0985, 0.1042, 0.1236, 0.1261, 0.1283, 0.1320, 0.1020]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1351, 0.1226, 0.1042, 0.0996, 0.1049, 0.1120, 0.1022, 0.1185, 0.1009],\n        [0.1299, 0.1224, 0.1158, 0.1088, 0.1063, 0.0884, 0.1084, 0.1202, 0.0997],\n        [0.0930, 0.0841, 0.1116, 0.1214, 0.1274, 0.1136, 0.1111, 0.1167, 0.1211],\n        [0.1074, 0.1076, 0.1420, 0.0962, 0.1010, 0.1124, 0.1025, 0.1089, 0.1220]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1274, 0.1182, 0.1064, 0.1030, 0.1045, 0.1177, 0.1022, 0.1006, 0.1201],\n        [0.1265, 0.1225, 0.0994, 0.1228, 0.1062, 0.1148, 0.1045, 0.1003, 0.1030],\n        [0.0896, 0.0826, 0.1102, 0.1186, 0.1177, 0.1041, 0.1224, 0.1317, 0.1230],\n        [0.0949, 0.0972, 0.1236, 0.1193, 0.1269, 0.1142, 0.1063, 0.1043, 0.1134],\n        [0.0907, 0.0905, 0.1204, 0.1099, 0.1086, 0.1024, 0.1044, 0.1235, 0.1495]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:30:21 PM | Train: [37/50] Step 000/062 Loss 0.004 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:34:44 PM | Train: [37/50] Step 050/062 Loss 0.022 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:35:45 PM | Train: [37/50] Step 062/062 Loss 0.021 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:35:46 PM | Train: [37/50] Final Prec@1 99.3750%\n11/23 11:35:46 PM | Valid: [37/50] Step 000/062 Loss 0.476 Prec@(1,5) (85.9%, 98.4%)\n11/23 11:35:59 PM | Valid: [37/50] Step 050/062 Loss 0.545 Prec@(1,5) (87.2%, 99.0%)\n11/23 11:36:01 PM | Valid: [37/50] Step 062/062 Loss 0.539 Prec@(1,5) (87.2%, 99.1%)\n11/23 11:36:01 PM | Valid: [37/50] Final Prec@1 87.2500%\n11/23 11:36:01 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1111, 0.0956, 0.1143, 0.1253, 0.0956, 0.1152, 0.1098, 0.1193, 0.1139],\n        [0.0938, 0.0813, 0.0984, 0.1216, 0.1225, 0.1119, 0.1177, 0.1126, 0.1403]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1023, 0.0866, 0.1009, 0.1266, 0.1309, 0.1020, 0.1072, 0.1147, 0.1288],\n        [0.0881, 0.0788, 0.0948, 0.1179, 0.1185, 0.1144, 0.1151, 0.1247, 0.1477],\n        [0.0809, 0.0741, 0.0970, 0.1020, 0.1172, 0.1162, 0.1370, 0.1247, 0.1509]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.0884, 0.1026, 0.1209, 0.1257, 0.1017, 0.0953, 0.1251, 0.1343],\n        [0.0847, 0.0736, 0.0823, 0.1343, 0.1171, 0.1198, 0.1211, 0.1102, 0.1570],\n        [0.0785, 0.0732, 0.0896, 0.1064, 0.1366, 0.1211, 0.1355, 0.1158, 0.1434],\n        [0.0772, 0.0700, 0.0849, 0.1208, 0.1123, 0.1279, 0.1227, 0.1051, 0.1791]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0849, 0.0753, 0.0841, 0.1012, 0.1321, 0.1240, 0.1091, 0.1419, 0.1476],\n        [0.0839, 0.0747, 0.0836, 0.1397, 0.1008, 0.1182, 0.1281, 0.1229, 0.1480],\n        [0.0687, 0.0649, 0.0780, 0.1250, 0.1172, 0.1331, 0.1290, 0.1187, 0.1653],\n        [0.0702, 0.0647, 0.0780, 0.1293, 0.1220, 0.1056, 0.1157, 0.1224, 0.1921],\n        [0.0664, 0.0630, 0.0696, 0.1047, 0.1285, 0.1139, 0.1342, 0.1404, 0.1792]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1308, 0.1153, 0.1083, 0.0922, 0.1122, 0.1245, 0.1002, 0.1081, 0.1085],\n        [0.1260, 0.1111, 0.1163, 0.1203, 0.0956, 0.1113, 0.0974, 0.1070, 0.1150]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1232, 0.1059, 0.1232, 0.0983, 0.0922, 0.1294, 0.1100, 0.1195, 0.0982],\n        [0.1353, 0.1291, 0.1139, 0.1023, 0.1103, 0.1029, 0.0965, 0.1081, 0.1015],\n        [0.0988, 0.0851, 0.0980, 0.1040, 0.1237, 0.1272, 0.1285, 0.1328, 0.1018]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1355, 0.1228, 0.1043, 0.0990, 0.1046, 0.1126, 0.1019, 0.1189, 0.1004],\n        [0.1304, 0.1224, 0.1167, 0.1084, 0.1060, 0.0872, 0.1085, 0.1205, 0.0998],\n        [0.0925, 0.0834, 0.1115, 0.1215, 0.1274, 0.1144, 0.1115, 0.1166, 0.1212],\n        [0.1072, 0.1076, 0.1435, 0.0954, 0.1001, 0.1124, 0.1019, 0.1092, 0.1227]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1274, 0.1182, 0.1063, 0.1030, 0.1042, 0.1179, 0.1022, 0.1008, 0.1199],\n        [0.1270, 0.1226, 0.0991, 0.1223, 0.1060, 0.1157, 0.1041, 0.1003, 0.1029],\n        [0.0889, 0.0818, 0.1097, 0.1186, 0.1183, 0.1044, 0.1230, 0.1327, 0.1225],\n        [0.0944, 0.0970, 0.1242, 0.1197, 0.1262, 0.1139, 0.1065, 0.1044, 0.1137],\n        [0.0902, 0.0900, 0.1208, 0.1099, 0.1085, 0.1016, 0.1039, 0.1239, 0.1512]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:36:09 PM | Train: [38/50] Step 000/062 Loss 0.021 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:40:33 PM | Train: [38/50] Step 050/062 Loss 0.021 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:41:34 PM | Train: [38/50] Step 062/062 Loss 0.025 Prec@(1,5) (99.2%, 100.0%)\n11/23 11:41:34 PM | Train: [38/50] Final Prec@1 99.2000%\n11/23 11:41:35 PM | Valid: [38/50] Step 000/062 Loss 0.504 Prec@(1,5) (90.6%, 100.0%)\n11/23 11:41:48 PM | Valid: [38/50] Step 050/062 Loss 0.537 Prec@(1,5) (87.4%, 99.1%)\n11/23 11:41:50 PM | Valid: [38/50] Step 062/062 Loss 0.530 Prec@(1,5) (87.4%, 99.2%)\n11/23 11:41:50 PM | Valid: [38/50] Final Prec@1 87.3500%\n11/23 11:41:50 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1108, 0.0952, 0.1143, 0.1257, 0.0957, 0.1147, 0.1099, 0.1202, 0.1135],\n        [0.0935, 0.0807, 0.0983, 0.1217, 0.1222, 0.1117, 0.1171, 0.1131, 0.1417]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1021, 0.0860, 0.1008, 0.1263, 0.1314, 0.1017, 0.1070, 0.1153, 0.1294],\n        [0.0879, 0.0783, 0.0947, 0.1178, 0.1176, 0.1136, 0.1153, 0.1248, 0.1500],\n        [0.0804, 0.0733, 0.0966, 0.1014, 0.1172, 0.1162, 0.1381, 0.1246, 0.1521]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1058, 0.0877, 0.1023, 0.1216, 0.1263, 0.1013, 0.0948, 0.1254, 0.1347],\n        [0.0840, 0.0726, 0.0815, 0.1345, 0.1169, 0.1195, 0.1220, 0.1099, 0.1591],\n        [0.0776, 0.0721, 0.0888, 0.1059, 0.1373, 0.1209, 0.1365, 0.1164, 0.1446],\n        [0.0762, 0.0689, 0.0839, 0.1211, 0.1120, 0.1280, 0.1228, 0.1051, 0.1820]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0843, 0.0746, 0.0835, 0.1011, 0.1325, 0.1235, 0.1091, 0.1423, 0.1491],\n        [0.0834, 0.0739, 0.0830, 0.1404, 0.1002, 0.1180, 0.1285, 0.1226, 0.1499],\n        [0.0678, 0.0639, 0.0770, 0.1257, 0.1173, 0.1331, 0.1292, 0.1188, 0.1671],\n        [0.0693, 0.0637, 0.0771, 0.1294, 0.1225, 0.1048, 0.1154, 0.1226, 0.1951],\n        [0.0656, 0.0621, 0.0687, 0.1037, 0.1287, 0.1135, 0.1348, 0.1409, 0.1821]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1307, 0.1150, 0.1081, 0.0917, 0.1124, 0.1251, 0.1004, 0.1081, 0.1085],\n        [0.1263, 0.1112, 0.1168, 0.1199, 0.0951, 0.1116, 0.0973, 0.1068, 0.1150]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1227, 0.1051, 0.1243, 0.0976, 0.0916, 0.1303, 0.1107, 0.1196, 0.0980],\n        [0.1360, 0.1299, 0.1146, 0.1018, 0.1101, 0.1026, 0.0958, 0.1086, 0.1007],\n        [0.0987, 0.0846, 0.0976, 0.1041, 0.1244, 0.1272, 0.1286, 0.1334, 0.1014]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1357, 0.1230, 0.1038, 0.0981, 0.1045, 0.1130, 0.1018, 0.1196, 0.1005],\n        [0.1305, 0.1226, 0.1166, 0.1079, 0.1057, 0.0866, 0.1084, 0.1215, 0.1003],\n        [0.0921, 0.0830, 0.1116, 0.1212, 0.1270, 0.1147, 0.1115, 0.1172, 0.1217],\n        [0.1070, 0.1080, 0.1444, 0.0952, 0.0998, 0.1124, 0.1015, 0.1090, 0.1227]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1272, 0.1182, 0.1066, 0.1031, 0.1042, 0.1178, 0.1024, 0.1008, 0.1196],\n        [0.1268, 0.1227, 0.0990, 0.1230, 0.1057, 0.1156, 0.1043, 0.1004, 0.1025],\n        [0.0883, 0.0813, 0.1096, 0.1192, 0.1185, 0.1045, 0.1233, 0.1328, 0.1226],\n        [0.0939, 0.0971, 0.1249, 0.1204, 0.1260, 0.1139, 0.1064, 0.1040, 0.1134],\n        [0.0894, 0.0895, 0.1210, 0.1095, 0.1084, 0.1010, 0.1034, 0.1244, 0.1535]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:41:58 PM | Train: [39/50] Step 000/062 Loss 0.010 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:46:21 PM | Train: [39/50] Step 050/062 Loss 0.021 Prec@(1,5) (99.5%, 100.0%)\n11/23 11:47:23 PM | Train: [39/50] Step 062/062 Loss 0.021 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:47:23 PM | Train: [39/50] Final Prec@1 99.4250%\n11/23 11:47:23 PM | Valid: [39/50] Step 000/062 Loss 0.529 Prec@(1,5) (84.4%, 100.0%)\n11/23 11:47:35 PM | Valid: [39/50] Step 050/062 Loss 0.538 Prec@(1,5) (86.3%, 99.2%)\n11/23 11:47:38 PM | Valid: [39/50] Step 062/062 Loss 0.533 Prec@(1,5) (86.6%, 99.2%)\n11/23 11:47:38 PM | Valid: [39/50] Final Prec@1 86.6000%\n11/23 11:47:38 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1103, 0.0950, 0.1147, 0.1262, 0.0952, 0.1140, 0.1103, 0.1211, 0.1131],\n        [0.0931, 0.0802, 0.0980, 0.1218, 0.1220, 0.1117, 0.1170, 0.1132, 0.1431]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1012, 0.0854, 0.1004, 0.1267, 0.1325, 0.1012, 0.1072, 0.1157, 0.1298],\n        [0.0877, 0.0779, 0.0947, 0.1177, 0.1171, 0.1135, 0.1150, 0.1247, 0.1516],\n        [0.0796, 0.0726, 0.0964, 0.1007, 0.1175, 0.1156, 0.1387, 0.1250, 0.1539]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1055, 0.0873, 0.1023, 0.1218, 0.1267, 0.1010, 0.0943, 0.1255, 0.1356],\n        [0.0838, 0.0720, 0.0810, 0.1351, 0.1163, 0.1191, 0.1222, 0.1093, 0.1611],\n        [0.0769, 0.0715, 0.0885, 0.1053, 0.1378, 0.1213, 0.1363, 0.1165, 0.1459],\n        [0.0753, 0.0682, 0.0836, 0.1211, 0.1110, 0.1281, 0.1227, 0.1049, 0.1851]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0834, 0.0739, 0.0830, 0.1007, 0.1335, 0.1239, 0.1080, 0.1435, 0.1500],\n        [0.0829, 0.0733, 0.0825, 0.1421, 0.0997, 0.1174, 0.1285, 0.1223, 0.1512],\n        [0.0667, 0.0630, 0.0762, 0.1255, 0.1173, 0.1340, 0.1296, 0.1192, 0.1685],\n        [0.0684, 0.0629, 0.0764, 0.1300, 0.1222, 0.1041, 0.1146, 0.1232, 0.1982],\n        [0.0644, 0.0610, 0.0677, 0.1031, 0.1292, 0.1128, 0.1358, 0.1414, 0.1847]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1312, 0.1151, 0.1080, 0.0912, 0.1121, 0.1255, 0.1004, 0.1081, 0.1085],\n        [0.1268, 0.1113, 0.1171, 0.1196, 0.0946, 0.1122, 0.0962, 0.1074, 0.1149]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1228, 0.1049, 0.1250, 0.0975, 0.0908, 0.1308, 0.1111, 0.1195, 0.0976],\n        [0.1370, 0.1308, 0.1152, 0.1014, 0.1093, 0.1023, 0.0951, 0.1084, 0.1005],\n        [0.0985, 0.0841, 0.0973, 0.1040, 0.1246, 0.1276, 0.1287, 0.1342, 0.1010]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1362, 0.1235, 0.1037, 0.0979, 0.1045, 0.1129, 0.1016, 0.1196, 0.1002],\n        [0.1308, 0.1229, 0.1172, 0.1074, 0.1052, 0.0858, 0.1085, 0.1218, 0.1004],\n        [0.0916, 0.0824, 0.1118, 0.1211, 0.1270, 0.1149, 0.1119, 0.1173, 0.1219],\n        [0.1064, 0.1078, 0.1453, 0.0950, 0.0993, 0.1124, 0.1011, 0.1094, 0.1233]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1275, 0.1186, 0.1067, 0.1032, 0.1039, 0.1177, 0.1022, 0.1004, 0.1198],\n        [0.1273, 0.1231, 0.0988, 0.1226, 0.1054, 0.1157, 0.1045, 0.1002, 0.1023],\n        [0.0877, 0.0807, 0.1099, 0.1195, 0.1187, 0.1048, 0.1231, 0.1330, 0.1225],\n        [0.0933, 0.0969, 0.1253, 0.1204, 0.1263, 0.1142, 0.1065, 0.1037, 0.1134],\n        [0.0888, 0.0891, 0.1216, 0.1097, 0.1081, 0.1005, 0.1028, 0.1245, 0.1550]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:47:46 PM | Train: [40/50] Step 000/062 Loss 0.060 Prec@(1,5) (98.4%, 100.0%)\n11/23 11:52:08 PM | Train: [40/50] Step 050/062 Loss 0.019 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:53:10 PM | Train: [40/50] Step 062/062 Loss 0.018 Prec@(1,5) (99.5%, 100.0%)\n11/23 11:53:10 PM | Train: [40/50] Final Prec@1 99.4750%\n11/23 11:53:11 PM | Valid: [40/50] Step 000/062 Loss 0.380 Prec@(1,5) (90.6%, 100.0%)\n11/23 11:53:23 PM | Valid: [40/50] Step 050/062 Loss 0.522 Prec@(1,5) (87.2%, 99.3%)\n11/23 11:53:26 PM | Valid: [40/50] Step 062/062 Loss 0.515 Prec@(1,5) (87.3%, 99.3%)\n11/23 11:53:26 PM | Valid: [40/50] Final Prec@1 87.2750%\n11/23 11:53:26 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('max_pool_3x3', 0)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1109, 0.0954, 0.1157, 0.1259, 0.0950, 0.1132, 0.1103, 0.1210, 0.1126],\n        [0.0925, 0.0796, 0.0977, 0.1217, 0.1220, 0.1116, 0.1170, 0.1133, 0.1448]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1011, 0.0851, 0.1005, 0.1268, 0.1331, 0.1008, 0.1068, 0.1160, 0.1297],\n        [0.0871, 0.0772, 0.0944, 0.1178, 0.1172, 0.1134, 0.1149, 0.1249, 0.1531],\n        [0.0788, 0.0718, 0.0960, 0.1002, 0.1174, 0.1156, 0.1391, 0.1251, 0.1560]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1058, 0.0872, 0.1026, 0.1219, 0.1270, 0.1005, 0.0940, 0.1251, 0.1360],\n        [0.0832, 0.0712, 0.0803, 0.1360, 0.1157, 0.1187, 0.1225, 0.1092, 0.1631],\n        [0.0763, 0.0709, 0.0881, 0.1049, 0.1380, 0.1220, 0.1369, 0.1165, 0.1462],\n        [0.0749, 0.0675, 0.0833, 0.1209, 0.1108, 0.1275, 0.1226, 0.1045, 0.1879]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0830, 0.0734, 0.0827, 0.1003, 0.1341, 0.1238, 0.1077, 0.1439, 0.1510],\n        [0.0824, 0.0727, 0.0820, 0.1433, 0.0992, 0.1169, 0.1285, 0.1221, 0.1530],\n        [0.0659, 0.0621, 0.0753, 0.1255, 0.1171, 0.1350, 0.1296, 0.1195, 0.1700],\n        [0.0676, 0.0620, 0.0756, 0.1298, 0.1228, 0.1038, 0.1145, 0.1232, 0.2008],\n        [0.0636, 0.0602, 0.0670, 0.1025, 0.1292, 0.1119, 0.1362, 0.1416, 0.1877]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1319, 0.1154, 0.1078, 0.0906, 0.1119, 0.1260, 0.1003, 0.1081, 0.1080],\n        [0.1275, 0.1116, 0.1173, 0.1194, 0.0939, 0.1120, 0.0961, 0.1070, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1231, 0.1048, 0.1251, 0.0970, 0.0900, 0.1314, 0.1113, 0.1199, 0.0974],\n        [0.1377, 0.1315, 0.1157, 0.1012, 0.1087, 0.1020, 0.0947, 0.1084, 0.1001],\n        [0.0985, 0.0835, 0.0972, 0.1038, 0.1250, 0.1276, 0.1287, 0.1349, 0.1009]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1372, 0.1243, 0.1031, 0.0975, 0.1043, 0.1129, 0.1012, 0.1199, 0.0996],\n        [0.1308, 0.1228, 0.1176, 0.1070, 0.1050, 0.0854, 0.1085, 0.1224, 0.1006],\n        [0.0914, 0.0821, 0.1120, 0.1214, 0.1270, 0.1149, 0.1121, 0.1171, 0.1220],\n        [0.1064, 0.1078, 0.1461, 0.0949, 0.0984, 0.1121, 0.1011, 0.1097, 0.1234]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1281, 0.1193, 0.1063, 0.1028, 0.1037, 0.1179, 0.1023, 0.0995, 0.1201],\n        [0.1273, 0.1232, 0.0986, 0.1233, 0.1055, 0.1155, 0.1042, 0.1000, 0.1023],\n        [0.0873, 0.0803, 0.1097, 0.1195, 0.1188, 0.1048, 0.1239, 0.1334, 0.1224],\n        [0.0933, 0.0969, 0.1257, 0.1200, 0.1258, 0.1144, 0.1063, 0.1038, 0.1136],\n        [0.0885, 0.0891, 0.1223, 0.1093, 0.1080, 0.0998, 0.1024, 0.1242, 0.1563]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:53:33 PM | Train: [41/50] Step 000/062 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)\n11/23 11:57:57 PM | Train: [41/50] Step 050/062 Loss 0.017 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:58:58 PM | Train: [41/50] Step 062/062 Loss 0.017 Prec@(1,5) (99.4%, 100.0%)\n11/23 11:58:58 PM | Train: [41/50] Final Prec@1 99.3500%\n11/23 11:58:59 PM | Valid: [41/50] Step 000/062 Loss 0.642 Prec@(1,5) (89.1%, 98.4%)\n11/23 11:59:12 PM | Valid: [41/50] Step 050/062 Loss 0.534 Prec@(1,5) (86.8%, 99.3%)\n11/23 11:59:14 PM | Valid: [41/50] Step 062/062 Loss 0.541 Prec@(1,5) (86.9%, 99.2%)\n11/23 11:59:14 PM | Valid: [41/50] Final Prec@1 86.9250%\n11/23 11:59:14 PM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1108, 0.0950, 0.1157, 0.1263, 0.0945, 0.1129, 0.1105, 0.1215, 0.1128],\n        [0.0920, 0.0789, 0.0973, 0.1217, 0.1222, 0.1119, 0.1168, 0.1135, 0.1458]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1005, 0.0843, 0.0997, 0.1271, 0.1340, 0.1008, 0.1067, 0.1167, 0.1303],\n        [0.0864, 0.0764, 0.0937, 0.1184, 0.1168, 0.1134, 0.1146, 0.1256, 0.1548],\n        [0.0779, 0.0710, 0.0953, 0.0998, 0.1176, 0.1157, 0.1397, 0.1258, 0.1573]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1060, 0.0867, 0.1023, 0.1220, 0.1270, 0.0999, 0.0938, 0.1254, 0.1370],\n        [0.0828, 0.0704, 0.0796, 0.1365, 0.1155, 0.1187, 0.1225, 0.1087, 0.1653],\n        [0.0758, 0.0701, 0.0877, 0.1047, 0.1383, 0.1223, 0.1372, 0.1163, 0.1475],\n        [0.0746, 0.0670, 0.0830, 0.1208, 0.1110, 0.1275, 0.1224, 0.1039, 0.1899]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0823, 0.0726, 0.0818, 0.1000, 0.1344, 0.1241, 0.1072, 0.1456, 0.1521],\n        [0.0819, 0.0719, 0.0814, 0.1441, 0.0987, 0.1167, 0.1284, 0.1225, 0.1545],\n        [0.0649, 0.0611, 0.0743, 0.1261, 0.1170, 0.1357, 0.1298, 0.1193, 0.1717],\n        [0.0668, 0.0611, 0.0747, 0.1303, 0.1229, 0.1032, 0.1143, 0.1233, 0.2034],\n        [0.0627, 0.0592, 0.0660, 0.1019, 0.1287, 0.1113, 0.1370, 0.1425, 0.1906]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1316, 0.1147, 0.1078, 0.0903, 0.1115, 0.1269, 0.1002, 0.1086, 0.1082],\n        [0.1277, 0.1115, 0.1174, 0.1199, 0.0934, 0.1118, 0.0961, 0.1075, 0.1147]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1232, 0.1044, 0.1254, 0.0965, 0.0896, 0.1319, 0.1116, 0.1200, 0.0974],\n        [0.1386, 0.1323, 0.1158, 0.1005, 0.1085, 0.1015, 0.0945, 0.1084, 0.0998],\n        [0.0985, 0.0832, 0.0969, 0.1030, 0.1248, 0.1280, 0.1293, 0.1361, 0.1002]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1375, 0.1244, 0.1028, 0.0973, 0.1036, 0.1132, 0.1017, 0.1199, 0.0997],\n        [0.1310, 0.1230, 0.1180, 0.1068, 0.1055, 0.0847, 0.1085, 0.1225, 0.1001],\n        [0.0910, 0.0815, 0.1119, 0.1217, 0.1269, 0.1157, 0.1121, 0.1172, 0.1220],\n        [0.1055, 0.1074, 0.1466, 0.0946, 0.0979, 0.1122, 0.1012, 0.1104, 0.1240]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1277, 0.1188, 0.1060, 0.1032, 0.1033, 0.1186, 0.1026, 0.0991, 0.1205],\n        [0.1273, 0.1233, 0.0984, 0.1232, 0.1055, 0.1161, 0.1041, 0.0999, 0.1022],\n        [0.0869, 0.0798, 0.1096, 0.1198, 0.1191, 0.1048, 0.1237, 0.1343, 0.1221],\n        [0.0924, 0.0964, 0.1258, 0.1204, 0.1261, 0.1150, 0.1062, 0.1039, 0.1139],\n        [0.0881, 0.0885, 0.1225, 0.1091, 0.1076, 0.0995, 0.1023, 0.1242, 0.1580]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/23 11:59:22 PM | Train: [42/50] Step 000/062 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:03:47 AM | Train: [42/50] Step 050/062 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:04:49 AM | Train: [42/50] Step 062/062 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:04:49 AM | Train: [42/50] Final Prec@1 99.7500%\n11/24 12:04:50 AM | Valid: [42/50] Step 000/062 Loss 0.716 Prec@(1,5) (87.5%, 100.0%)\n11/24 12:05:02 AM | Valid: [42/50] Step 050/062 Loss 0.513 Prec@(1,5) (88.1%, 99.3%)\n11/24 12:05:05 AM | Valid: [42/50] Step 062/062 Loss 0.516 Prec@(1,5) (87.9%, 99.2%)\n11/24 12:05:05 AM | Valid: [42/50] Final Prec@1 87.8750%\n11/24 12:05:05 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_5x5', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1112, 0.0951, 0.1164, 0.1266, 0.0943, 0.1127, 0.1104, 0.1208, 0.1126],\n        [0.0916, 0.0785, 0.0973, 0.1216, 0.1224, 0.1120, 0.1165, 0.1134, 0.1466]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1000, 0.0838, 0.0995, 0.1277, 0.1348, 0.1000, 0.1068, 0.1168, 0.1307],\n        [0.0860, 0.0760, 0.0938, 0.1182, 0.1168, 0.1129, 0.1142, 0.1262, 0.1559],\n        [0.0774, 0.0704, 0.0951, 0.0992, 0.1174, 0.1154, 0.1403, 0.1259, 0.1589]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.0864, 0.1024, 0.1220, 0.1275, 0.0997, 0.0938, 0.1249, 0.1374],\n        [0.0823, 0.0698, 0.0793, 0.1361, 0.1153, 0.1185, 0.1226, 0.1090, 0.1671],\n        [0.0752, 0.0696, 0.0876, 0.1047, 0.1380, 0.1225, 0.1375, 0.1160, 0.1489],\n        [0.0741, 0.0663, 0.0827, 0.1209, 0.1108, 0.1270, 0.1222, 0.1033, 0.1927]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0816, 0.0720, 0.0814, 0.1001, 0.1349, 0.1241, 0.1068, 0.1464, 0.1527],\n        [0.0815, 0.0715, 0.0812, 0.1445, 0.0985, 0.1163, 0.1282, 0.1228, 0.1554],\n        [0.0641, 0.0602, 0.0736, 0.1261, 0.1171, 0.1361, 0.1304, 0.1195, 0.1728],\n        [0.0660, 0.0602, 0.0740, 0.1304, 0.1227, 0.1031, 0.1141, 0.1234, 0.2061],\n        [0.0618, 0.0584, 0.0653, 0.1015, 0.1283, 0.1104, 0.1379, 0.1430, 0.1935]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1319, 0.1147, 0.1075, 0.0902, 0.1115, 0.1277, 0.1001, 0.1082, 0.1082],\n        [0.1277, 0.1112, 0.1177, 0.1201, 0.0932, 0.1117, 0.0961, 0.1077, 0.1148]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1233, 0.1041, 0.1255, 0.0963, 0.0890, 0.1329, 0.1117, 0.1200, 0.0972],\n        [0.1393, 0.1329, 0.1160, 0.0999, 0.1082, 0.1010, 0.0946, 0.1085, 0.0995],\n        [0.0980, 0.0825, 0.0964, 0.1031, 0.1250, 0.1288, 0.1296, 0.1368, 0.0999]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1381, 0.1247, 0.1029, 0.0967, 0.1034, 0.1131, 0.1014, 0.1200, 0.0997],\n        [0.1315, 0.1234, 0.1181, 0.1064, 0.1053, 0.0841, 0.1083, 0.1230, 0.1000],\n        [0.0906, 0.0809, 0.1119, 0.1220, 0.1266, 0.1159, 0.1126, 0.1173, 0.1222],\n        [0.1052, 0.1075, 0.1473, 0.0946, 0.0976, 0.1120, 0.1011, 0.1107, 0.1239]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1276, 0.1188, 0.1062, 0.1032, 0.1035, 0.1187, 0.1023, 0.0989, 0.1209],\n        [0.1269, 0.1232, 0.0983, 0.1238, 0.1057, 0.1167, 0.1042, 0.0995, 0.1017],\n        [0.0861, 0.0790, 0.1093, 0.1201, 0.1197, 0.1053, 0.1233, 0.1344, 0.1225],\n        [0.0918, 0.0961, 0.1259, 0.1204, 0.1264, 0.1153, 0.1062, 0.1037, 0.1140],\n        [0.0875, 0.0882, 0.1224, 0.1092, 0.1073, 0.0996, 0.1021, 0.1240, 0.1596]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:05:13 AM | Train: [43/50] Step 000/062 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:09:38 AM | Train: [43/50] Step 050/062 Loss 0.013 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:10:40 AM | Train: [43/50] Step 062/062 Loss 0.014 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:10:40 AM | Train: [43/50] Final Prec@1 99.7000%\n11/24 12:10:41 AM | Valid: [43/50] Step 000/062 Loss 0.640 Prec@(1,5) (89.1%, 98.4%)\n11/24 12:10:54 AM | Valid: [43/50] Step 050/062 Loss 0.564 Prec@(1,5) (87.3%, 99.1%)\n11/24 12:10:57 AM | Valid: [43/50] Step 062/062 Loss 0.555 Prec@(1,5) (87.5%, 99.1%)\n11/24 12:10:57 AM | Valid: [43/50] Final Prec@1 87.4500%\n11/24 12:10:57 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1116, 0.0949, 0.1168, 0.1267, 0.0940, 0.1123, 0.1102, 0.1214, 0.1122],\n        [0.0911, 0.0778, 0.0970, 0.1217, 0.1223, 0.1117, 0.1165, 0.1140, 0.1478]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0997, 0.0832, 0.0993, 0.1280, 0.1355, 0.0998, 0.1065, 0.1169, 0.1310],\n        [0.0851, 0.0751, 0.0930, 0.1184, 0.1164, 0.1129, 0.1147, 0.1267, 0.1577],\n        [0.0766, 0.0694, 0.0944, 0.0987, 0.1171, 0.1160, 0.1416, 0.1260, 0.1603]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1060, 0.0858, 0.1022, 0.1224, 0.1277, 0.0994, 0.0938, 0.1249, 0.1379],\n        [0.0817, 0.0688, 0.0783, 0.1369, 0.1152, 0.1184, 0.1235, 0.1089, 0.1684],\n        [0.0746, 0.0686, 0.0871, 0.1048, 0.1379, 0.1229, 0.1376, 0.1157, 0.1508],\n        [0.0735, 0.0655, 0.0821, 0.1211, 0.1103, 0.1270, 0.1221, 0.1032, 0.1954]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0813, 0.0713, 0.0810, 0.0996, 0.1350, 0.1244, 0.1066, 0.1471, 0.1538],\n        [0.0810, 0.0708, 0.0806, 0.1448, 0.0984, 0.1163, 0.1281, 0.1230, 0.1570],\n        [0.0633, 0.0593, 0.0727, 0.1265, 0.1171, 0.1368, 0.1308, 0.1193, 0.1741],\n        [0.0654, 0.0594, 0.0732, 0.1307, 0.1223, 0.1022, 0.1142, 0.1232, 0.2093],\n        [0.0612, 0.0576, 0.0645, 0.1006, 0.1285, 0.1095, 0.1386, 0.1438, 0.1957]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1316, 0.1141, 0.1072, 0.0899, 0.1118, 0.1289, 0.0998, 0.1082, 0.1085],\n        [0.1288, 0.1116, 0.1177, 0.1200, 0.0923, 0.1119, 0.0958, 0.1074, 0.1145]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1228, 0.1032, 0.1260, 0.0959, 0.0887, 0.1336, 0.1120, 0.1202, 0.0976],\n        [0.1408, 0.1339, 0.1167, 0.0995, 0.1077, 0.1010, 0.0939, 0.1080, 0.0985],\n        [0.0980, 0.0821, 0.0962, 0.1028, 0.1251, 0.1291, 0.1296, 0.1374, 0.0996]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1382, 0.1242, 0.1024, 0.0965, 0.1027, 0.1137, 0.1016, 0.1205, 0.1001],\n        [0.1324, 0.1238, 0.1186, 0.1062, 0.1048, 0.0833, 0.1085, 0.1227, 0.0997],\n        [0.0903, 0.0804, 0.1118, 0.1218, 0.1269, 0.1165, 0.1126, 0.1173, 0.1223],\n        [0.1051, 0.1073, 0.1477, 0.0946, 0.0973, 0.1119, 0.1006, 0.1113, 0.1241]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1273, 0.1181, 0.1062, 0.1032, 0.1036, 0.1194, 0.1024, 0.0988, 0.1211],\n        [0.1276, 0.1235, 0.0979, 0.1242, 0.1052, 0.1173, 0.1040, 0.0993, 0.1009],\n        [0.0858, 0.0786, 0.1093, 0.1201, 0.1200, 0.1058, 0.1233, 0.1344, 0.1226],\n        [0.0916, 0.0959, 0.1261, 0.1203, 0.1266, 0.1155, 0.1062, 0.1035, 0.1143],\n        [0.0873, 0.0879, 0.1225, 0.1092, 0.1072, 0.0991, 0.1017, 0.1243, 0.1607]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:11:04 AM | Train: [44/50] Step 000/062 Loss 0.017 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:15:30 AM | Train: [44/50] Step 050/062 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:16:32 AM | Train: [44/50] Step 062/062 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:16:32 AM | Train: [44/50] Final Prec@1 99.8000%\n11/24 12:16:33 AM | Valid: [44/50] Step 000/062 Loss 0.353 Prec@(1,5) (84.4%, 100.0%)\n11/24 12:16:46 AM | Valid: [44/50] Step 050/062 Loss 0.506 Prec@(1,5) (88.4%, 99.4%)\n11/24 12:16:49 AM | Valid: [44/50] Step 062/062 Loss 0.530 Prec@(1,5) (88.0%, 99.3%)\n11/24 12:16:49 AM | Valid: [44/50] Final Prec@1 88.0500%\n11/24 12:16:49 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1119, 0.0953, 0.1176, 0.1268, 0.0934, 0.1121, 0.1099, 0.1216, 0.1114],\n        [0.0902, 0.0770, 0.0964, 0.1222, 0.1222, 0.1117, 0.1163, 0.1140, 0.1499]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0995, 0.0830, 0.0994, 0.1277, 0.1363, 0.0991, 0.1064, 0.1177, 0.1310],\n        [0.0843, 0.0744, 0.0925, 0.1185, 0.1164, 0.1124, 0.1145, 0.1274, 0.1597],\n        [0.0758, 0.0687, 0.0940, 0.0982, 0.1164, 0.1160, 0.1428, 0.1264, 0.1617]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.0857, 0.1025, 0.1223, 0.1281, 0.0989, 0.0936, 0.1246, 0.1385],\n        [0.0810, 0.0681, 0.0777, 0.1374, 0.1149, 0.1185, 0.1240, 0.1085, 0.1700],\n        [0.0739, 0.0680, 0.0867, 0.1045, 0.1383, 0.1234, 0.1379, 0.1154, 0.1521],\n        [0.0728, 0.0649, 0.0818, 0.1214, 0.1102, 0.1265, 0.1212, 0.1028, 0.1983]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0807, 0.0709, 0.0806, 0.0989, 0.1358, 0.1241, 0.1064, 0.1486, 0.1541],\n        [0.0802, 0.0699, 0.0797, 0.1462, 0.0978, 0.1159, 0.1286, 0.1232, 0.1586],\n        [0.0624, 0.0585, 0.0720, 0.1263, 0.1167, 0.1377, 0.1313, 0.1192, 0.1759],\n        [0.0645, 0.0586, 0.0724, 0.1312, 0.1224, 0.1018, 0.1133, 0.1232, 0.2127],\n        [0.0602, 0.0567, 0.0636, 0.1001, 0.1284, 0.1084, 0.1392, 0.1446, 0.1989]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1320, 0.1141, 0.1072, 0.0892, 0.1121, 0.1294, 0.0994, 0.1084, 0.1082],\n        [0.1292, 0.1116, 0.1179, 0.1201, 0.0920, 0.1116, 0.0955, 0.1072, 0.1149]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1228, 0.1030, 0.1262, 0.0953, 0.0881, 0.1341, 0.1125, 0.1208, 0.0973],\n        [0.1417, 0.1348, 0.1171, 0.0989, 0.1071, 0.1006, 0.0938, 0.1080, 0.0981],\n        [0.0976, 0.0817, 0.0959, 0.1028, 0.1252, 0.1293, 0.1299, 0.1383, 0.0994]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1386, 0.1246, 0.1019, 0.0960, 0.1024, 0.1140, 0.1019, 0.1208, 0.0997],\n        [0.1326, 0.1237, 0.1192, 0.1059, 0.1048, 0.0826, 0.1082, 0.1235, 0.0995],\n        [0.0893, 0.0799, 0.1117, 0.1217, 0.1273, 0.1170, 0.1129, 0.1175, 0.1227],\n        [0.1042, 0.1070, 0.1483, 0.0944, 0.0971, 0.1119, 0.1010, 0.1114, 0.1247]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1275, 0.1185, 0.1059, 0.1028, 0.1034, 0.1199, 0.1020, 0.0986, 0.1214],\n        [0.1279, 0.1236, 0.0976, 0.1244, 0.1051, 0.1177, 0.1040, 0.0990, 0.1008],\n        [0.0852, 0.0781, 0.1091, 0.1201, 0.1201, 0.1058, 0.1233, 0.1354, 0.1227],\n        [0.0908, 0.0955, 0.1260, 0.1205, 0.1265, 0.1155, 0.1068, 0.1038, 0.1145],\n        [0.0869, 0.0877, 0.1230, 0.1084, 0.1072, 0.0987, 0.1016, 0.1242, 0.1623]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:16:57 AM | Train: [45/50] Step 000/062 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:21:23 AM | Train: [45/50] Step 050/062 Loss 0.009 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:22:25 AM | Train: [45/50] Step 062/062 Loss 0.010 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:22:25 AM | Train: [45/50] Final Prec@1 99.7750%\n11/24 12:22:26 AM | Valid: [45/50] Step 000/062 Loss 0.264 Prec@(1,5) (92.2%, 98.4%)\n11/24 12:22:39 AM | Valid: [45/50] Step 050/062 Loss 0.527 Prec@(1,5) (87.4%, 99.3%)\n11/24 12:22:42 AM | Valid: [45/50] Step 062/062 Loss 0.515 Prec@(1,5) (87.8%, 99.4%)\n11/24 12:22:42 AM | Valid: [45/50] Final Prec@1 87.8250%\n11/24 12:22:42 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1121, 0.0956, 0.1184, 0.1271, 0.0929, 0.1113, 0.1102, 0.1212, 0.1111],\n        [0.0897, 0.0764, 0.0962, 0.1224, 0.1225, 0.1109, 0.1165, 0.1141, 0.1514]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0991, 0.0824, 0.0990, 0.1282, 0.1374, 0.0985, 0.1063, 0.1178, 0.1313],\n        [0.0837, 0.0737, 0.0921, 0.1186, 0.1161, 0.1121, 0.1146, 0.1277, 0.1614],\n        [0.0751, 0.0681, 0.0937, 0.0972, 0.1166, 0.1162, 0.1435, 0.1267, 0.1631]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1059, 0.0855, 0.1027, 0.1222, 0.1283, 0.0985, 0.0931, 0.1245, 0.1392],\n        [0.0803, 0.0673, 0.0771, 0.1380, 0.1146, 0.1186, 0.1247, 0.1082, 0.1711],\n        [0.0732, 0.0673, 0.0864, 0.1045, 0.1386, 0.1232, 0.1380, 0.1155, 0.1534],\n        [0.0722, 0.0643, 0.0815, 0.1218, 0.1096, 0.1261, 0.1210, 0.1022, 0.2014]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0801, 0.0703, 0.0801, 0.0985, 0.1360, 0.1246, 0.1065, 0.1493, 0.1546],\n        [0.0795, 0.0692, 0.0793, 0.1468, 0.0971, 0.1156, 0.1288, 0.1237, 0.1602],\n        [0.0615, 0.0576, 0.0712, 0.1268, 0.1159, 0.1385, 0.1316, 0.1192, 0.1775],\n        [0.0636, 0.0578, 0.0718, 0.1317, 0.1229, 0.1013, 0.1124, 0.1231, 0.2154],\n        [0.0593, 0.0558, 0.0628, 0.0997, 0.1284, 0.1077, 0.1397, 0.1451, 0.2015]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1321, 0.1141, 0.1067, 0.0887, 0.1125, 0.1299, 0.0993, 0.1087, 0.1078],\n        [0.1299, 0.1119, 0.1185, 0.1202, 0.0912, 0.1113, 0.0949, 0.1069, 0.1152]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1229, 0.1028, 0.1264, 0.0944, 0.0876, 0.1346, 0.1133, 0.1210, 0.0969],\n        [0.1429, 0.1358, 0.1174, 0.0982, 0.1065, 0.1003, 0.0933, 0.1077, 0.0978],\n        [0.0976, 0.0813, 0.0958, 0.1022, 0.1249, 0.1293, 0.1304, 0.1395, 0.0990]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1389, 0.1248, 0.1018, 0.0958, 0.1018, 0.1143, 0.1021, 0.1209, 0.0995],\n        [0.1328, 0.1239, 0.1198, 0.1056, 0.1047, 0.0819, 0.1081, 0.1237, 0.0994],\n        [0.0888, 0.0794, 0.1118, 0.1216, 0.1272, 0.1171, 0.1129, 0.1180, 0.1231],\n        [0.1036, 0.1067, 0.1489, 0.0941, 0.0968, 0.1120, 0.1010, 0.1113, 0.1255]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1278, 0.1189, 0.1060, 0.1028, 0.1030, 0.1200, 0.1016, 0.0984, 0.1214],\n        [0.1283, 0.1240, 0.0975, 0.1243, 0.1051, 0.1181, 0.1034, 0.0987, 0.1005],\n        [0.0850, 0.0780, 0.1094, 0.1198, 0.1205, 0.1058, 0.1231, 0.1359, 0.1225],\n        [0.0903, 0.0957, 0.1266, 0.1207, 0.1267, 0.1154, 0.1066, 0.1034, 0.1146],\n        [0.0864, 0.0875, 0.1237, 0.1078, 0.1069, 0.0981, 0.1014, 0.1246, 0.1637]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:22:49 AM | Train: [46/50] Step 000/062 Loss 0.035 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:27:13 AM | Train: [46/50] Step 050/062 Loss 0.013 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:28:15 AM | Train: [46/50] Step 062/062 Loss 0.012 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:28:15 AM | Train: [46/50] Final Prec@1 99.7500%\n11/24 12:28:15 AM | Valid: [46/50] Step 000/062 Loss 0.498 Prec@(1,5) (89.1%, 98.4%)\n11/24 12:28:27 AM | Valid: [46/50] Step 050/062 Loss 0.521 Prec@(1,5) (87.5%, 99.4%)\n11/24 12:28:31 AM | Valid: [46/50] Step 062/062 Loss 0.531 Prec@(1,5) (87.2%, 99.4%)\n11/24 12:28:31 AM | Valid: [46/50] Final Prec@1 87.2000%\n11/24 12:28:31 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1118, 0.0953, 0.1185, 0.1277, 0.0922, 0.1117, 0.1102, 0.1215, 0.1112],\n        [0.0890, 0.0759, 0.0960, 0.1221, 0.1228, 0.1112, 0.1163, 0.1145, 0.1521]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0985, 0.0817, 0.0984, 0.1281, 0.1387, 0.0982, 0.1065, 0.1184, 0.1316],\n        [0.0829, 0.0731, 0.0916, 0.1183, 0.1160, 0.1119, 0.1149, 0.1286, 0.1627],\n        [0.0743, 0.0674, 0.0933, 0.0967, 0.1159, 0.1163, 0.1442, 0.1268, 0.1652]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1057, 0.0851, 0.1025, 0.1222, 0.1286, 0.0985, 0.0928, 0.1246, 0.1400],\n        [0.0798, 0.0667, 0.0766, 0.1379, 0.1147, 0.1192, 0.1247, 0.1080, 0.1725],\n        [0.0725, 0.0666, 0.0859, 0.1047, 0.1388, 0.1231, 0.1385, 0.1156, 0.1543],\n        [0.0715, 0.0635, 0.0809, 0.1221, 0.1093, 0.1261, 0.1212, 0.1016, 0.2039]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0793, 0.0695, 0.0795, 0.0985, 0.1361, 0.1250, 0.1060, 0.1501, 0.1559],\n        [0.0789, 0.0686, 0.0788, 0.1471, 0.0963, 0.1156, 0.1286, 0.1242, 0.1619],\n        [0.0606, 0.0568, 0.0704, 0.1270, 0.1164, 0.1387, 0.1319, 0.1193, 0.1789],\n        [0.0626, 0.0569, 0.0710, 0.1322, 0.1230, 0.1010, 0.1118, 0.1232, 0.2183],\n        [0.0585, 0.0550, 0.0619, 0.0997, 0.1283, 0.1070, 0.1402, 0.1454, 0.2041]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1323, 0.1139, 0.1066, 0.0884, 0.1120, 0.1310, 0.0992, 0.1089, 0.1076],\n        [0.1299, 0.1117, 0.1186, 0.1203, 0.0911, 0.1114, 0.0947, 0.1070, 0.1154]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1227, 0.1021, 0.1265, 0.0942, 0.0868, 0.1360, 0.1140, 0.1211, 0.0967],\n        [0.1434, 0.1363, 0.1178, 0.0981, 0.1067, 0.0998, 0.0933, 0.1069, 0.0978],\n        [0.0972, 0.0808, 0.0954, 0.1019, 0.1250, 0.1302, 0.1309, 0.1400, 0.0986]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1394, 0.1249, 0.1018, 0.0955, 0.1012, 0.1144, 0.1024, 0.1211, 0.0994],\n        [0.1332, 0.1239, 0.1205, 0.1052, 0.1043, 0.0814, 0.1079, 0.1239, 0.0996],\n        [0.0882, 0.0788, 0.1117, 0.1215, 0.1273, 0.1174, 0.1133, 0.1184, 0.1233],\n        [0.1032, 0.1070, 0.1498, 0.0938, 0.0965, 0.1120, 0.1010, 0.1111, 0.1256]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1279, 0.1188, 0.1060, 0.1025, 0.1028, 0.1205, 0.1014, 0.0983, 0.1218],\n        [0.1284, 0.1240, 0.0973, 0.1250, 0.1049, 0.1180, 0.1036, 0.0985, 0.1003],\n        [0.0843, 0.0775, 0.1092, 0.1204, 0.1203, 0.1062, 0.1236, 0.1361, 0.1224],\n        [0.0898, 0.0957, 0.1269, 0.1205, 0.1270, 0.1156, 0.1067, 0.1034, 0.1144],\n        [0.0859, 0.0871, 0.1238, 0.1081, 0.1069, 0.0979, 0.1013, 0.1241, 0.1648]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:28:38 AM | Train: [47/50] Step 000/062 Loss 0.013 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:33:04 AM | Train: [47/50] Step 050/062 Loss 0.011 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:34:05 AM | Train: [47/50] Step 062/062 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:34:06 AM | Train: [47/50] Final Prec@1 99.7750%\n11/24 12:34:06 AM | Valid: [47/50] Step 000/062 Loss 0.930 Prec@(1,5) (82.8%, 100.0%)\n11/24 12:34:19 AM | Valid: [47/50] Step 050/062 Loss 0.560 Prec@(1,5) (86.5%, 99.3%)\n11/24 12:34:21 AM | Valid: [47/50] Step 062/062 Loss 0.558 Prec@(1,5) (86.9%, 99.3%)\n11/24 12:34:22 AM | Valid: [47/50] Final Prec@1 86.8750%\n11/24 12:34:22 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('dil_conv_5x5', 4)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1119, 0.0952, 0.1190, 0.1278, 0.0914, 0.1119, 0.1103, 0.1214, 0.1110],\n        [0.0888, 0.0756, 0.0961, 0.1218, 0.1231, 0.1112, 0.1158, 0.1145, 0.1531]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0981, 0.0812, 0.0981, 0.1281, 0.1401, 0.0977, 0.1061, 0.1185, 0.1319],\n        [0.0825, 0.0725, 0.0913, 0.1180, 0.1159, 0.1114, 0.1150, 0.1291, 0.1643],\n        [0.0737, 0.0667, 0.0930, 0.0961, 0.1152, 0.1166, 0.1448, 0.1274, 0.1665]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1058, 0.0849, 0.1026, 0.1219, 0.1286, 0.0981, 0.0927, 0.1248, 0.1406],\n        [0.0792, 0.0659, 0.0759, 0.1383, 0.1143, 0.1189, 0.1252, 0.1083, 0.1739],\n        [0.0719, 0.0660, 0.0855, 0.1047, 0.1392, 0.1231, 0.1391, 0.1151, 0.1555],\n        [0.0708, 0.0628, 0.0804, 0.1222, 0.1087, 0.1260, 0.1213, 0.1014, 0.2063]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0789, 0.0691, 0.0792, 0.0982, 0.1365, 0.1251, 0.1053, 0.1507, 0.1570],\n        [0.0785, 0.0680, 0.0785, 0.1479, 0.0956, 0.1153, 0.1287, 0.1241, 0.1635],\n        [0.0599, 0.0561, 0.0699, 0.1273, 0.1162, 0.1382, 0.1320, 0.1196, 0.1808],\n        [0.0620, 0.0562, 0.0704, 0.1326, 0.1226, 0.1006, 0.1114, 0.1232, 0.2211],\n        [0.0578, 0.0543, 0.0612, 0.0996, 0.1283, 0.1064, 0.1406, 0.1454, 0.2064]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1327, 0.1140, 0.1066, 0.0882, 0.1119, 0.1316, 0.0988, 0.1087, 0.1075],\n        [0.1304, 0.1120, 0.1189, 0.1201, 0.0906, 0.1109, 0.0944, 0.1071, 0.1156]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1225, 0.1016, 0.1268, 0.0938, 0.0866, 0.1366, 0.1144, 0.1215, 0.0962],\n        [0.1441, 0.1369, 0.1182, 0.0977, 0.1063, 0.0995, 0.0930, 0.1068, 0.0976],\n        [0.0965, 0.0803, 0.0950, 0.1014, 0.1256, 0.1303, 0.1307, 0.1416, 0.0985]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1401, 0.1250, 0.1015, 0.0950, 0.1012, 0.1145, 0.1022, 0.1215, 0.0991],\n        [0.1338, 0.1242, 0.1213, 0.1048, 0.1040, 0.0806, 0.1076, 0.1242, 0.0996],\n        [0.0875, 0.0783, 0.1117, 0.1215, 0.1273, 0.1179, 0.1133, 0.1186, 0.1238],\n        [0.1028, 0.1071, 0.1507, 0.0932, 0.0958, 0.1118, 0.1014, 0.1116, 0.1256]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1282, 0.1188, 0.1057, 0.1025, 0.1027, 0.1206, 0.1012, 0.0984, 0.1218],\n        [0.1285, 0.1241, 0.0970, 0.1254, 0.1046, 0.1180, 0.1038, 0.0987, 0.0999],\n        [0.0837, 0.0771, 0.1092, 0.1206, 0.1205, 0.1064, 0.1237, 0.1362, 0.1226],\n        [0.0894, 0.0958, 0.1275, 0.1206, 0.1266, 0.1159, 0.1064, 0.1033, 0.1145],\n        [0.0853, 0.0867, 0.1239, 0.1080, 0.1066, 0.0974, 0.1010, 0.1245, 0.1665]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:34:29 AM | Train: [48/50] Step 000/062 Loss 0.007 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:38:55 AM | Train: [48/50] Step 050/062 Loss 0.011 Prec@(1,5) (99.8%, 100.0%)\n11/24 12:39:57 AM | Train: [48/50] Step 062/062 Loss 0.011 Prec@(1,5) (99.9%, 100.0%)\n11/24 12:39:57 AM | Train: [48/50] Final Prec@1 99.8500%\n11/24 12:39:58 AM | Valid: [48/50] Step 000/062 Loss 0.374 Prec@(1,5) (87.5%, 98.4%)\n11/24 12:40:11 AM | Valid: [48/50] Step 050/062 Loss 0.510 Prec@(1,5) (87.9%, 99.4%)\n11/24 12:40:13 AM | Valid: [48/50] Step 062/062 Loss 0.489 Prec@(1,5) (88.1%, 99.4%)\n11/24 12:40:14 AM | Valid: [48/50] Final Prec@1 88.1500%\n11/24 12:40:14 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('dil_conv_5x5', 4), ('dil_conv_5x5', 0)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1118, 0.0949, 0.1190, 0.1282, 0.0909, 0.1118, 0.1106, 0.1217, 0.1111],\n        [0.0886, 0.0754, 0.0963, 0.1221, 0.1233, 0.1111, 0.1152, 0.1142, 0.1537]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0977, 0.0806, 0.0978, 0.1283, 0.1408, 0.0977, 0.1061, 0.1186, 0.1324],\n        [0.0822, 0.0722, 0.0913, 0.1179, 0.1155, 0.1109, 0.1151, 0.1290, 0.1660],\n        [0.0730, 0.0661, 0.0927, 0.0954, 0.1150, 0.1163, 0.1453, 0.1280, 0.1681]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1057, 0.0845, 0.1026, 0.1220, 0.1289, 0.0978, 0.0922, 0.1249, 0.1414],\n        [0.0786, 0.0654, 0.0755, 0.1380, 0.1143, 0.1193, 0.1256, 0.1081, 0.1751],\n        [0.0713, 0.0653, 0.0851, 0.1042, 0.1396, 0.1227, 0.1400, 0.1149, 0.1569],\n        [0.0702, 0.0622, 0.0798, 0.1225, 0.1085, 0.1261, 0.1213, 0.1011, 0.2084]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0784, 0.0685, 0.0787, 0.0978, 0.1368, 0.1250, 0.1056, 0.1512, 0.1580],\n        [0.0781, 0.0676, 0.0782, 0.1483, 0.0955, 0.1156, 0.1284, 0.1239, 0.1643],\n        [0.0592, 0.0554, 0.0692, 0.1271, 0.1160, 0.1388, 0.1324, 0.1195, 0.1823],\n        [0.0614, 0.0556, 0.0699, 0.1327, 0.1225, 0.1006, 0.1109, 0.1226, 0.2239],\n        [0.0571, 0.0536, 0.0606, 0.0991, 0.1287, 0.1057, 0.1407, 0.1458, 0.2087]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1326, 0.1138, 0.1066, 0.0878, 0.1120, 0.1322, 0.0987, 0.1087, 0.1075],\n        [0.1311, 0.1121, 0.1189, 0.1198, 0.0903, 0.1111, 0.0939, 0.1071, 0.1157]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1221, 0.1010, 0.1271, 0.0936, 0.0861, 0.1377, 0.1147, 0.1217, 0.0961],\n        [0.1453, 0.1379, 0.1182, 0.0974, 0.1062, 0.0989, 0.0925, 0.1065, 0.0971],\n        [0.0963, 0.0799, 0.0946, 0.1013, 0.1257, 0.1305, 0.1310, 0.1427, 0.0981]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1402, 0.1249, 0.1013, 0.0947, 0.1006, 0.1147, 0.1023, 0.1223, 0.0991],\n        [0.1344, 0.1246, 0.1215, 0.1045, 0.1039, 0.0799, 0.1076, 0.1245, 0.0992],\n        [0.0871, 0.0778, 0.1119, 0.1217, 0.1272, 0.1182, 0.1129, 0.1187, 0.1246],\n        [0.1028, 0.1074, 0.1520, 0.0928, 0.0952, 0.1116, 0.1010, 0.1116, 0.1258]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1283, 0.1190, 0.1055, 0.1027, 0.1024, 0.1210, 0.1009, 0.0982, 0.1219],\n        [0.1291, 0.1245, 0.0968, 0.1253, 0.1044, 0.1183, 0.1036, 0.0982, 0.0998],\n        [0.0833, 0.0768, 0.1093, 0.1207, 0.1206, 0.1064, 0.1238, 0.1365, 0.1226],\n        [0.0892, 0.0959, 0.1283, 0.1206, 0.1260, 0.1160, 0.1061, 0.1037, 0.1141],\n        [0.0851, 0.0864, 0.1240, 0.1082, 0.1065, 0.0972, 0.1008, 0.1241, 0.1676]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:40:21 AM | Train: [49/50] Step 000/062 Loss 0.003 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:44:45 AM | Train: [49/50] Step 050/062 Loss 0.012 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:45:47 AM | Train: [49/50] Step 062/062 Loss 0.011 Prec@(1,5) (99.7%, 100.0%)\n11/24 12:45:47 AM | Train: [49/50] Final Prec@1 99.6750%\n11/24 12:45:48 AM | Valid: [49/50] Step 000/062 Loss 0.460 Prec@(1,5) (81.2%, 100.0%)\n11/24 12:46:00 AM | Valid: [49/50] Step 050/062 Loss 0.481 Prec@(1,5) (87.7%, 99.4%)\n11/24 12:46:03 AM | Valid: [49/50] Step 062/062 Loss 0.492 Prec@(1,5) (87.7%, 99.3%)\n11/24 12:46:03 AM | Valid: [49/50] Final Prec@1 87.7250%\n11/24 12:46:03 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('sep_conv_7x7', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))\n/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  \"please use `get_last_lr()`.\", UserWarning)\n####### ALPHA #######\n# Alpha - normal\ntensor([[0.1119, 0.0949, 0.1196, 0.1281, 0.0904, 0.1116, 0.1108, 0.1218, 0.1108],\n        [0.0881, 0.0749, 0.0961, 0.1226, 0.1235, 0.1110, 0.1151, 0.1138, 0.1549]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0972, 0.0801, 0.0975, 0.1281, 0.1419, 0.0975, 0.1059, 0.1193, 0.1325],\n        [0.0814, 0.0713, 0.0905, 0.1177, 0.1155, 0.1106, 0.1154, 0.1294, 0.1683],\n        [0.0724, 0.0655, 0.0922, 0.0950, 0.1148, 0.1165, 0.1454, 0.1288, 0.1694]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1054, 0.0842, 0.1026, 0.1218, 0.1297, 0.0972, 0.0918, 0.1255, 0.1418],\n        [0.0779, 0.0647, 0.0750, 0.1385, 0.1142, 0.1191, 0.1260, 0.1080, 0.1766],\n        [0.0707, 0.0647, 0.0848, 0.1036, 0.1401, 0.1228, 0.1407, 0.1144, 0.1582],\n        [0.0695, 0.0615, 0.0795, 0.1222, 0.1075, 0.1261, 0.1213, 0.1005, 0.2120]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.0779, 0.0680, 0.0783, 0.0975, 0.1374, 0.1250, 0.1050, 0.1525, 0.1584],\n        [0.0776, 0.0671, 0.0779, 0.1490, 0.0951, 0.1148, 0.1288, 0.1238, 0.1660],\n        [0.0586, 0.0548, 0.0687, 0.1270, 0.1164, 0.1394, 0.1329, 0.1191, 0.1833],\n        [0.0607, 0.0548, 0.0692, 0.1328, 0.1225, 0.1002, 0.1102, 0.1227, 0.2269],\n        [0.0563, 0.0528, 0.0599, 0.0988, 0.1289, 0.1047, 0.1413, 0.1462, 0.2112]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n\n# Alpha - reduce\ntensor([[0.1324, 0.1133, 0.1068, 0.0874, 0.1122, 0.1330, 0.0986, 0.1089, 0.1075],\n        [0.1313, 0.1121, 0.1191, 0.1201, 0.0900, 0.1113, 0.0935, 0.1071, 0.1156]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1218, 0.1005, 0.1275, 0.0931, 0.0856, 0.1380, 0.1150, 0.1226, 0.0958],\n        [0.1459, 0.1386, 0.1186, 0.0973, 0.1058, 0.0987, 0.0921, 0.1062, 0.0968],\n        [0.0960, 0.0795, 0.0945, 0.1009, 0.1259, 0.1305, 0.1310, 0.1437, 0.0979]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1405, 0.1248, 0.1010, 0.0941, 0.1000, 0.1152, 0.1025, 0.1229, 0.0991],\n        [0.1349, 0.1250, 0.1215, 0.1041, 0.1038, 0.0791, 0.1075, 0.1250, 0.0990],\n        [0.0866, 0.0773, 0.1119, 0.1218, 0.1271, 0.1189, 0.1129, 0.1187, 0.1247],\n        [0.1026, 0.1074, 0.1529, 0.0922, 0.0945, 0.1119, 0.1009, 0.1116, 0.1259]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\ntensor([[0.1283, 0.1190, 0.1051, 0.1030, 0.1023, 0.1215, 0.1009, 0.0979, 0.1219],\n        [0.1294, 0.1248, 0.0965, 0.1256, 0.1041, 0.1191, 0.1032, 0.0979, 0.0995],\n        [0.0828, 0.0764, 0.1097, 0.1206, 0.1209, 0.1065, 0.1240, 0.1364, 0.1228],\n        [0.0889, 0.0959, 0.1289, 0.1207, 0.1255, 0.1159, 0.1061, 0.1037, 0.1143],\n        [0.0846, 0.0859, 0.1241, 0.1082, 0.1066, 0.0969, 0.1007, 0.1241, 0.1689]],\n       device='cuda:0', grad_fn=<SoftmaxBackward>)\n#####################\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n11/24 12:46:10 AM | Train: [50/50] Step 000/062 Loss 0.011 Prec@(1,5) (100.0%, 100.0%)\n11/24 12:50:33 AM | Train: [50/50] Step 050/062 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)\n11/24 12:51:34 AM | Train: [50/50] Step 062/062 Loss 0.009 Prec@(1,5) (99.9%, 100.0%)\n11/24 12:51:35 AM | Train: [50/50] Final Prec@1 99.8750%\n11/24 12:51:35 AM | Valid: [50/50] Step 000/062 Loss 0.520 Prec@(1,5) (90.6%, 100.0%)\n11/24 12:51:48 AM | Valid: [50/50] Step 050/062 Loss 0.499 Prec@(1,5) (87.7%, 99.4%)\n11/24 12:51:51 AM | Valid: [50/50] Step 062/062 Loss 0.505 Prec@(1,5) (87.5%, 99.3%)\n11/24 12:51:51 AM | Valid: [50/50] Final Prec@1 87.4500%\n11/24 12:51:51 AM | genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('sep_conv_7x7', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))\n11/24 12:51:52 AM | Final best Prec@1 = 88.1500%\n11/24 12:51:52 AM | Best Genotype = Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('sep_conv_7x7', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))\n","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Genotype(normal=[[('sep_conv_3x3', 0), ('sep_conv_5x5', 1)], [('dil_conv_3x3', 2), ('sep_conv_5x5', 0)], [('dil_conv_3x3', 2), ('sep_conv_3x3', 1)], [('dil_conv_5x5', 0), ('dil_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('max_pool_3x3', 1), ('sep_conv_7x7', 0)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('skip_connect', 3), ('max_pool_3x3', 0)], [('dil_conv_5x5', 2), ('skip_connect', 3)]], reduce_concat=range(2, 6))"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}