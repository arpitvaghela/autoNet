{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Weighted_Random_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VztqR5dlMdsr"
      },
      "source": [
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers import Dropout\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRk4ZJhM2Ls"
      },
      "source": [
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train / 255.0\n",
        "\ttest_norm = test / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewH8hGHuKrsi"
      },
      "source": [
        "# def load_dataset():\n",
        "#     imgs_path = \"/content/drive/MyDrive/CIFAR10/\"\n",
        "#     classes = sorted(os.listdir(imgs_path))[1:]\n",
        "\n",
        "#     n_classes = 4\n",
        "#     y_onehot = []\n",
        "\n",
        "#     for i in range(n_classes):\n",
        "#         alist = []\n",
        "#         for j in range(n_classes):\n",
        "#             if i==j:\n",
        "#                 alist.append(1)\n",
        "#             else:\n",
        "#                 alist.append(0)\n",
        "#         y_onehot.append(alist)\n",
        "\n",
        "\n",
        "#     X_train = []\n",
        "#     y_train = []\n",
        "#     X_test = []\n",
        "#     y_test = []\n",
        "\n",
        "\n",
        "#     for i in range(n_classes):\n",
        "#         class1 = classes[i]\n",
        "#         imgs = sorted(os.listdir(imgs_path + class1))\n",
        "#         train_imgs = imgs[:80]\n",
        "#         test_imgs = imgs[80:]\n",
        "#         print(\"train: \" + str(len(train_imgs)) + \"test: \" + str(len(test_imgs)))\n",
        "\n",
        "#         for j in range(len(train_imgs)):\n",
        "#             X_train.append(cv2.imread(imgs_path + class1 + '/' + train_imgs[j]))\n",
        "#             y_train.append(y_onehot[i])\n",
        "\n",
        "#         for j in range(len(test_imgs)):\n",
        "#             X_test.append(cv2.imread(imgs_path + class1 + '/' + test_imgs[j]))\n",
        "#             y_test.append(y_onehot[i])\n",
        "\n",
        "#     X_train = np.array(X_train).astype('float32')\n",
        "#     X_test = np.array(X_test).astype('float32')\n",
        "#     y_train = np.array(y_train).astype('float32')\n",
        "#     y_test = np.array(y_test).astype('float32')\n",
        "\n",
        "    \n",
        "#     X_train, X_test = prep_pixels(X_train, X_test)\n",
        "\n",
        "#     return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7M9bvYWMulU"
      },
      "source": [
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        " \n",
        "\ttrainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "\treturn trainX, trainY, testX, testY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FSrT9sANjot"
      },
      "source": [
        "# def summarize_diagnostics(history):\n",
        "# \t# plot loss\n",
        "# \tpyplot.subplot(211)\n",
        "# \tpyplot.title('Cross Entropy Loss')\n",
        "# \tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "# \tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "# \t# plot accuracy\n",
        "# \tpyplot.subplot(212)\n",
        "# \tpyplot.title('Classification Accuracy')\n",
        "# \tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "# \tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "# \t# save plot to file\n",
        "# \tfilename = sys.argv[0].split('/')[-1]\n",
        "# \tpyplot.savefig(filename + '_plot.png')\n",
        "# \tpyplot.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YlNp5qI6JCD"
      },
      "source": [
        "def return_choice_init(val1,val2,log):\n",
        "    if log:\n",
        "        val1 = math.log(val1,3)\n",
        "        val2 = math.log(val2,3)\n",
        "\n",
        "        val1 = (val2-val1)*0.33 + val1\n",
        "        val2 = (val2-val1)*0.67 + val1\n",
        "\n",
        "        return 3**val1, 3**val2\n",
        "\n",
        "    return (val2-val1)*0.33 + val1, (val2-val1)*0.67 + val1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD9mGzz36G-t"
      },
      "source": [
        "def initialise_search(record):\n",
        "\n",
        "    init_vals_1 = {}\n",
        "\n",
        "    init_vals_1[\"dr\"] = [] \n",
        "    init_vals_1[\"kr\"] = [] \n",
        "\n",
        "    init_vals_2 = {}\n",
        "\n",
        "    init_vals_2[\"dr\"] = [] \n",
        "    init_vals_2[\"kr\"] = [] \n",
        "\n",
        "    for i in range(4):\n",
        "        v1, v2 = return_choice_init(dr_range_init[0],dr_range_init[1],log=True)\n",
        "        init_vals_1[\"dr\"].append(v1)\n",
        "        init_vals_2[\"dr\"].append(v2)\n",
        "\n",
        "    for i in range(7):\n",
        "        v1, v2 = return_choice_init(kr_range_init[0],kr_range_init[1],log=True)\n",
        "        init_vals_1[\"kr\"].append(v1)\n",
        "        init_vals_2[\"kr\"].append(v2)\n",
        "\n",
        "    init_vals_1[\"learn\"], init_vals_2[\"learn\"] = return_choice_init(learn_range_init[0],learn_range_init[1],log=True)\n",
        "\n",
        "    init_vals_1[\"momentum\"], init_vals_2[\"momentum\"] = return_choice_init(momentum_range_init[0],momentum_range_init[1],log=True)\n",
        "\n",
        "    x, y = return_choice_init(epochs_range_init[0],epochs_range_init[1],log=False)\n",
        "    init_vals_1[\"epochs\"], init_vals_2[\"epochs\"] = int(x), int(y)\n",
        "\n",
        "    print(init_vals_1[\"learn\"])\n",
        "    model1, history = build_train_model(init_vals_1)\n",
        "    init_vals_1[\"loss\"], acc1 = model1.evaluate(testX, testY, verbose=0)\n",
        "    print(\"Validation loss of first initialisation: \" + str(init_vals_1[\"loss\"]))\n",
        "\n",
        "    model2, history = build_train_model(init_vals_2)\n",
        "    init_vals_2[\"loss\"], acc2 = model2.evaluate(testX, testY, verbose=0)\n",
        "    print(\"Validation loss of second initialisation: \" + str(init_vals_2[\"loss\"]))\n",
        "\n",
        "\n",
        "    if init_vals_1[\"loss\"] < init_vals_2[\"loss\"]:\n",
        "        record[\"loss\"].append(init_vals_1[\"loss\"])\n",
        "        record[\"acc\"].append(acc1)\n",
        "        return init_vals_1, init_vals_2, model1, record\n",
        "    \n",
        "    else:\n",
        "        record[\"loss\"].append(init_vals_2[\"loss\"])\n",
        "        record[\"acc\"].append(acc2)\n",
        "        return init_vals_2, init_vals_1, model2, record"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwEQ1nE2dr6B"
      },
      "source": [
        "def return_divisions(min,max,n_divs):\n",
        "    arr = []\n",
        "\n",
        "    diff = (max-min)/n_divs\n",
        "\n",
        "    curr_num = min+(diff/2)\n",
        "    arr.append(curr_num)\n",
        "    for i in range(n_divs-1):\n",
        "        curr_num = curr_num + diff\n",
        "        arr.append(curr_num)\n",
        "\n",
        "    return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRK2cmpyYo0w"
      },
      "source": [
        "def return_choice(val1,val2,loss1,loss2,log):\n",
        "\n",
        "    min = None\n",
        "    max = None\n",
        "    minloss = None\n",
        "    maxloss = None\n",
        "\n",
        "    if val1<val2:\n",
        "        min = val1\n",
        "        max = val2\n",
        "        minloss=loss1\n",
        "        maxloss=loss2\n",
        "    else:\n",
        "        min = val2\n",
        "        max = val1\n",
        "        minloss=loss2\n",
        "        maxloss=loss1\n",
        "\n",
        "    maxrat = int((minloss/(minloss+maxloss))*100)\n",
        "    minrat = int((maxloss/(minloss+maxloss))*100)\n",
        "\n",
        "    if minrat==0:\n",
        "        minrat = 1\n",
        "        maxrat = 99\n",
        "    \n",
        "    if maxrat==0:\n",
        "        minrat=99\n",
        "        maxrat=1\n",
        "\n",
        "    if log:\n",
        "        min = math.log(min,3)\n",
        "        max = math.log(max,3)\n",
        "\n",
        "\n",
        "    mid = (min+max)/2\n",
        "\n",
        "\n",
        "    min = min - (mid/2)\n",
        "    max = max + (mid/2)\n",
        "\n",
        "    arr1 = return_divisions(min,mid,minrat)\n",
        "    arr2 = return_divisions(mid,max,maxrat)\n",
        "\n",
        "    choice_range = arr1+arr2\n",
        "\n",
        "    choice = random.choice(choice_range)\n",
        "\n",
        "    if log:\n",
        "        choice = 3**choice\n",
        "\n",
        "    return choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huLpbpFFC-jp"
      },
      "source": [
        "def return_new_vals(best_1,best_2):\n",
        "    new = {}\n",
        "\n",
        "    new[\"dr\"] = [] \n",
        "    new[\"kr\"] = [] \n",
        "\n",
        "    for i in range(4):\n",
        "        new[\"dr\"].append(return_choice(best_1[\"dr\"][i],best_2[\"dr\"][i],best_1[\"loss\"],best_2[\"loss\"],log=True))\n",
        "\n",
        "    for i in range(7):\n",
        "        new[\"kr\"].append(return_choice(best_1[\"kr\"][i],best_2[\"kr\"][i],best_1[\"loss\"],best_2[\"loss\"],log=True))\n",
        "\n",
        "    new[\"learn\"] = return_choice(best_1[\"learn\"],best_2[\"learn\"],best_1[\"loss\"],best_2[\"loss\"],log=True)\n",
        "\n",
        "    new[\"momentum\"] = return_choice(best_1[\"momentum\"],best_2[\"momentum\"],best_1[\"loss\"],best_2[\"loss\"],log=True)\n",
        "\n",
        "    new[\"epochs\"] = int(return_choice(best_1[\"epochs\"],best_2[\"epochs\"],best_1[\"loss\"],best_2[\"loss\"],log=False))\n",
        "\n",
        "    return new;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkWu-FyVNhw-"
      },
      "source": [
        "def build_train_model(var_dict):\n",
        "    print(var_dict)\n",
        "    dr_i, kr_i, learn_i, momentum_i, epoch_i = var_dict[\"dr\"], var_dict[\"kr\"], var_dict[\"learn\"], var_dict[\"momentum\"], var_dict[\"epochs\"]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[0]), input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[1])))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dr_i[0]))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[2])))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[3])))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dr_i[1]))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[4])))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=l2(kr_i[5])))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(dr_i[2]))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(kr_i[6])))\n",
        "    model.add(Dropout(dr_i[3]))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    # compile model\n",
        "    opt = SGD(lr=learn_i, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(trainX, trainY, epochs=10, batch_size=64, validation_data=(testX, testY), verbose=1)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYQFRIi1Afsr"
      },
      "source": [
        "def update_best(best_1,best_2,new, model):\n",
        "    if new[\"loss\"] < best_1[\"loss\"]:\n",
        "        model.save('/content/drive/MyDrive/Weighted_Random')\n",
        "        return new, best_1\n",
        "\n",
        "    elif new[\"loss\"] < best_2[\"loss\"]:\n",
        "        return best_1, new\n",
        "\n",
        "    else:\n",
        "        return best_1, best_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Za1yGhFEYpIK"
      },
      "source": [
        "# load dataset\n",
        "# prepare pixel data\n",
        "# define model\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "dr_range_init = [0.01,0.5]\n",
        "kr_range_init = [0.01,0.1]\n",
        "learn_range_init = [0.001,0.01]\n",
        "momentum_range_init = [0.99,0.999]\n",
        "epochs_range_init = [5,25]\n",
        "\n",
        "total_iter = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d44xER2yb8u7"
      },
      "source": [
        "record = {}\n",
        "record[\"loss\"] = []\n",
        "record[\"acc\"] = []\n",
        "\n",
        "best_1, best_2, best_model, record = initialise_search(record)\n",
        "record_file = open('/content/drive/MyDrive/Weighted_Random/record.pkl', \"wb\")\n",
        "pickle.dump(record, record_file)\n",
        "best_model.save('/content/drive/MyDrive/Weighted_Random/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XsKyX3UbN0os"
      },
      "source": [
        "for i in range(total_iter):\n",
        "    new = return_new_vals(best_1,best_2)\n",
        "\n",
        "    model, history = build_train_model(new)\n",
        "\n",
        "    new[\"loss\"], acc = model.evaluate(testX, testY, verbose=0)\n",
        "    record[\"loss\"].append(new[\"loss\"])\n",
        "    record[\"acc\"].append(acc)\n",
        "\n",
        "    best_1, best_2 = update_best(best_1,best_2,new, model)\n",
        "\n",
        "    record_file = open('/content/drive/MyDrive/Weighted_Random/record.pkl', \"wb\")\n",
        "    pickle.dump(record, record_file)\n",
        "    best_model.save('/content/drive/MyDrive/Weighted_Random/')\n",
        "    print(\"Validation Loss of this Iteration: \" + str(new[\"loss\"]))\n",
        "    print(\"Validation Loss of Best Model: \" + str(best_1[\"loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw5kR00dSWjD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}